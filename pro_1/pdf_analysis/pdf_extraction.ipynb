{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF: https://arodes.hes-so.ch/record/6812/files/published%20version.pdf\n",
      "PDF: https://arxiv.org/pdf/2108.05542\n",
      "PDF: https://www.mdpi.com/2673-2688/4/1/4/pdf\n",
      "PDF: https://aclanthology.org/2020.emnlp-demos.6.pdf\n",
      "PDF: https://www.researchgate.net/profile/Saurav-Singla-4/publication/346565049_Comparative_Analysis_of_Transformer_Based_Pre-Trained_NLP_Models/links/5fc77e3ea6fdcc697bd360ec/Comparative-Analysis-of-Transformer-Based-Pre-Trained-NLP-Models.pdf\n",
      "PDF: https://www.mdpi.com/2078-2489/14/4/242/pdf\n",
      "PDF: https://www.researchgate.net/profile/Henry-Nunoo-Mensah/publication/348740926_Transformer_Models_for_Text-based_Emotion_Detection_A_Review_of_BERT-based_Approaches/links/600e5ef145851553a06b0ac4/Transformer-Models-for-Text-based-Emotion-Detection-A-Review-of-BERT-based-Approaches.pdf\n",
      "PDF: https://arxiv.org/pdf/1910.03771\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")  # Run in background\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "query = \"transformer models in NLP\"\n",
    "driver.get(f\"https://scholar.google.com/scholar?q={query}\")\n",
    "\n",
    "pdf_links=[]\n",
    "# Find PDF links (right-hand panel)\n",
    "pdfs = driver.find_elements(By.XPATH, \"//div[@class='gs_or_ggsm']/a\")\n",
    "for pdf in pdfs:\n",
    "    url = pdf.get_attribute(\"href\")\n",
    "    print(\"PDF:\", url)\n",
    "    pdf_links.append(url)\n",
    "\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import re\n",
    "\n",
    "SAVE_DIR = 'samples_pdf'\n",
    "for url in pdf_links:\n",
    "    filename = url.split(\"/\")[-1]\n",
    "    if not re.findall(\".pdf\", filename):\n",
    "        filename = filename + \".pdf\"\n",
    "    filename = os.path.join(SAVE_DIR, filename)\n",
    "    r = requests.get(url)\n",
    "    with open(filename, \"wb\") as f:\n",
    "        f.write(r.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import time\n",
    "\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")  # No browser window\n",
    "options.add_argument(\"--disable-gpu\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "driver = webdriver.Chrome(options=options)\n",
    "#url = \"https://onlinelibrary.wiley.com/doi/abs/10.1002/ijc.29408\"\n",
    "\n",
    "query = \"convolution neural network for WBC classification\"\n",
    "driver.get(f\"https://scholar.google.com/scholar?q={query}\")\n",
    "time.sleep(3)\n",
    "\n",
    "html = driver.page_source\n",
    "\n",
    "with open(\"samples_pdf/real_page.html\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(html)\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Trying article #1: White blood cells detection and classification based on regional convolutional neural networks (268 citations)\n",
      "⚠️ Skipped: Content invalid or blocked\n",
      "🔍 Trying article #2: Automatic detection and classification of leukocytes using convolutional neural networks (250 citations)\n",
      "✅ Valid content saved: samples_pdf/article_1.html\n",
      "🔍 Trying article #3: White blood cell classification and counting using convolutional neural network (152 citations)\n",
      "⚠️ Skipped: Content invalid or blocked\n",
      "🔍 Trying article #4: An efficient multi-level convolutional neural network approach for white blood cells classification (89 citations)\n",
      "⚠️ Skipped: Content invalid or blocked\n",
      "🔍 Trying article #5: Classification of white blood cell using convolution neural network (70 citations)\n",
      "⚠️ Skipped: Content invalid or blocked\n",
      "🔍 Trying article #6: White blood cells classification with deep convolutional neural networks (67 citations)\n",
      "⚠️ Skipped: Content invalid or blocked\n",
      "🔍 Trying article #7: WBCs-Net: Type identification of white blood cells using convolutional neural network (47 citations)\n",
      "✅ Valid content saved: samples_pdf/article_2.html\n",
      "🔍 Trying article #8: WBC image classification and generative models based on convolutional neural network (42 citations)\n",
      "✅ Valid content saved: samples_pdf/article_3.html\n",
      "🔍 Trying article #9: An approach to classify white blood cells using convolutional neural network optimized by particle swarm optimization algorithm (24 citations)\n",
      "✅ Valid content saved: samples_pdf/article_4.html\n",
      "🔍 Trying article #10: Deep features based convolutional neural network to detect and automatic classification of white blood cells (10 citations)\n",
      "✅ Valid content saved: samples_pdf/article_5.html\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "\n",
    "# Setup headless Chrome\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")\n",
    "options.add_argument(\"--disable-gpu\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "query = \"convolution neural network for WBC classification\"\n",
    "search_url = f\"https://scholar.google.com/scholar?q={query}\"\n",
    "driver.get(search_url)\n",
    "time.sleep(3)\n",
    "\n",
    "# Create output folder\n",
    "os.makedirs(\"samples_pdf\", exist_ok=True)\n",
    "\n",
    "results = driver.find_elements(By.CSS_SELECTOR, \".gs_r.gs_or.gs_scl\")\n",
    "\n",
    "def extract_citations_and_links(results):\n",
    "    extracted = []\n",
    "    for res in results:\n",
    "        try:\n",
    "            title = res.find_element(By.CSS_SELECTOR, \"h3\").text\n",
    "            link_el = res.find_element(By.CSS_SELECTOR, \"h3 a\")\n",
    "            article_link = link_el.get_attribute(\"href\")\n",
    "\n",
    "            cite_text = res.find_element(By.PARTIAL_LINK_TEXT, \"Cited by\").text\n",
    "            citations = int(re.search(r\"Cited by (\\d+)\", cite_text).group(1))\n",
    "\n",
    "            extracted.append({\n",
    "                \"title\": title,\n",
    "                \"citations\": citations,\n",
    "                \"link\": article_link\n",
    "            })\n",
    "        except Exception:\n",
    "            continue\n",
    "    return sorted(extracted, key=lambda x: -x['citations'])\n",
    "\n",
    "top_articles = extract_citations_and_links(results)\n",
    "\n",
    "# Utility: Check if content looks valid\n",
    "def is_valid_content(html):\n",
    "    invalid_markers = [\n",
    "        \"enable javascript\", \"verify you are human\",\n",
    "        \"access denied\", \"robot check\", \"just a moment\",\n",
    "        \"cloudflare\", \"404 not found\", \"site can’t be reached\"\n",
    "    ]\n",
    "    lower_html = html.lower()\n",
    "    return not any(marker in lower_html for marker in invalid_markers) and len(lower_html.strip()) > 5000\n",
    "\n",
    "# Visit and save up to 5 valid articles\n",
    "valid_count = 0\n",
    "i = 0\n",
    "while valid_count < 5 and i < len(top_articles):\n",
    "    article = top_articles[i]\n",
    "    print(f\"🔍 Trying article #{i+1}: {article['title']} ({article['citations']} citations)\")\n",
    "    try:\n",
    "        driver.get(article[\"link\"])\n",
    "        time.sleep(5)\n",
    "        html = driver.page_source\n",
    "\n",
    "        if is_valid_content(html):\n",
    "            file_path = f\"samples_pdf/article_{valid_count+1}.html\"\n",
    "            with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(html)\n",
    "            print(f\"✅ Valid content saved: {file_path}\")\n",
    "            valid_count += 1\n",
    "        else:\n",
    "            print(\"⚠️ Skipped: Content invalid or blocked\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading article: {e}\")\n",
    "    i += 1\n",
    "\n",
    "driver.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
