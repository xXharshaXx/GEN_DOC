<html lang="en" class="js"><head>
        <meta charset="UTF-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="applicable-device" content="pc,mobile">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        
        
        
            <meta name="robots" content="max-image-preview:large">
            <meta name="access" content="Yes">

        
        <meta name="360-site-verification" content="1268d79b5e96aecf3ff2a7dac04ad990">

        <title>WBC image classification and generative models based on convolutional neural network | BMC Medical Imaging
        </title>

        
            
            
    
    <meta name="citation_abstract" content="Computer-aided methods for analyzing white blood cells (WBC) are popular due to the complexity of the manual alternatives. Recent works have shown highly accurate segmentation and detection of white blood cells from microscopic blood images. However, the classification of the observed cells is still a challenge, in part due to the distribution of the five types that affect the condition of the immune system. (i) This work proposes W-Net, a CNN-based method for WBC classification. We evaluate W-Net on a real-world large-scale dataset that includes 6562 real images of the five WBC types. (ii) For further benefits, we generate synthetic WBC images using Generative Adversarial Network to be used for education and research purposes through sharing. (i) W-Net achieves an average accuracy of 97%. In comparison to state-of-the-art methods in the field of WBC classification, we show that W-Net outperforms other CNN- and RNN-based model architectures. Moreover, we show the benefits of using pre-trained W-Net in a transfer learning context when fine-tuned to specific task or accommodating another dataset. (ii) The synthetic WBC images are confirmed by experiments and a domain expert to have a high degree of similarity to the original images. The pre-trained W-Net and the generated WBC dataset are available for the community to facilitate reproducibility and follow up research work. This work proposed W-Net, a CNN-based architecture with a small number of layers, to accurately classify the five WBC types. We evaluated W-Net on a real-world large-scale dataset and addressed several challenges such as the transfer learning property and the class imbalance. W-Net achieved an average classification accuracy of 97%. We synthesized a dataset of new WBC image samples using DCGAN, which we released to the public for education and research purposes.">
    <meta name="journal_id" content="12880">
    <meta name="dc.title" content="WBC image classification and generative models based on convolutional neural network">
    <meta name="dc.source" content="BMC Medical Imaging 2022 22:1">
    <meta name="dc.format" content="text/html">
    <meta name="dc.publisher" content="BioMed Central">
    <meta name="dc.date" content="2022-05-20">
    <meta name="dc.type" content="OriginalPaper">
    <meta name="dc.language" content="En">
    <meta name="dc.copyright" content="2022 The Author(s)">
    <meta name="dc.rights" content="2022 The Author(s)">
    <meta name="dc.rightsAgent" content="reprints@biomedcentral.com">
    <meta name="dc.description" content="Computer-aided methods for analyzing white blood cells (WBC) are popular due to the complexity of the manual alternatives. Recent works have shown highly accurate segmentation and detection of white blood cells from microscopic blood images. However, the classification of the observed cells is still a challenge, in part due to the distribution of the five types that affect the condition of the immune system. (i) This work proposes W-Net, a CNN-based method for WBC classification. We evaluate W-Net on a real-world large-scale dataset that includes 6562 real images of the five WBC types. (ii) For further benefits, we generate synthetic WBC images using Generative Adversarial Network to be used for education and research purposes through sharing. (i) W-Net achieves an average accuracy of 97%. In comparison to state-of-the-art methods in the field of WBC classification, we show that W-Net outperforms other CNN- and RNN-based model architectures. Moreover, we show the benefits of using pre-trained W-Net in a transfer learning context when fine-tuned to specific task or accommodating another dataset. (ii) The synthetic WBC images are confirmed by experiments and a domain expert to have a high degree of similarity to the original images. The pre-trained W-Net and the generated WBC dataset are available for the community to facilitate reproducibility and follow up research work. This work proposed W-Net, a CNN-based architecture with a small number of layers, to accurately classify the five WBC types. We evaluated W-Net on a real-world large-scale dataset and addressed several challenges such as the transfer learning property and the class imbalance. W-Net achieved an average classification accuracy of 97%. We synthesized a dataset of new WBC image samples using DCGAN, which we released to the public for education and research purposes.">
    <meta name="prism.issn" content="1471-2342">
    <meta name="prism.publicationName" content="BMC Medical Imaging">
    <meta name="prism.publicationDate" content="2022-05-20">
    <meta name="prism.volume" content="22">
    <meta name="prism.number" content="1">
    <meta name="prism.section" content="OriginalPaper">
    <meta name="prism.startingPage" content="1">
    <meta name="prism.endingPage" content="16">
    <meta name="prism.copyright" content="2022 The Author(s)">
    <meta name="prism.rightsAgent" content="reprints@biomedcentral.com">
    <meta name="prism.url" content="https://link.springer.com/articles/10.1186/s12880-022-00818-1">
    <meta name="prism.doi" content="doi:10.1186/s12880-022-00818-1">
    <meta name="citation_pdf_url" content="https://bmcmedimaging.biomedcentral.com/counter/pdf/10.1186/s12880-022-00818-1">
    <meta name="citation_fulltext_html_url" content="https://link.springer.com/articles/10.1186/s12880-022-00818-1">
    <meta name="citation_journal_title" content="BMC Medical Imaging">
    <meta name="citation_journal_abbrev" content="BMC Med Imaging">
    <meta name="citation_publisher" content="BioMed Central">
    <meta name="citation_issn" content="1471-2342">
    <meta name="citation_title" content="WBC image classification and generative models based on convolutional neural network">
    <meta name="citation_volume" content="22">
    <meta name="citation_issue" content="1">
    <meta name="citation_publication_date" content="2022/12">
    <meta name="citation_online_date" content="2022/05/20">
    <meta name="citation_firstpage" content="1">
    <meta name="citation_lastpage" content="16">
    <meta name="citation_article_type" content="Research">
    <meta name="citation_fulltext_world_readable" content="">
    <meta name="citation_language" content="en">
    <meta name="dc.identifier" content="doi:10.1186/s12880-022-00818-1">
    <meta name="DOI" content="10.1186/s12880-022-00818-1">
    <meta name="size" content="379043">
    <meta name="citation_doi" content="10.1186/s12880-022-00818-1">
    <meta name="citation_springer_api_url" content="http://api.springer.com/xmldata/jats?q=doi:10.1186/s12880-022-00818-1&amp;api_key=">
    <meta name="description" content="Computer-aided methods for analyzing white blood cells (WBC) are popular due to the complexity of the manual alternatives. Recent works have shown highly accurate segmentation and detection of white blood cells from microscopic blood images. However, the classification of the observed cells is still a challenge, in part due to the distribution of the five types that affect the condition of the immune system. (i) This work proposes W-Net, a CNN-based method for WBC classification. We evaluate W-Net on a real-world large-scale dataset that includes 6562 real images of the five WBC types. (ii) For further benefits, we generate synthetic WBC images using Generative Adversarial Network to be used for education and research purposes through sharing. (i) W-Net achieves an average accuracy of 97%. In comparison to state-of-the-art methods in the field of WBC classification, we show that W-Net outperforms other CNN- and RNN-based model architectures. Moreover, we show the benefits of using pre-trained W-Net in a transfer learning context when fine-tuned to specific task or accommodating another dataset. (ii) The synthetic WBC images are confirmed by experiments and a domain expert to have a high degree of similarity to the original images. The pre-trained W-Net and the generated WBC dataset are available for the community to facilitate reproducibility and follow up research work. This work proposed W-Net, a CNN-based architecture with a small number of layers, to accurately classify the five WBC types. We evaluated W-Net on a real-world large-scale dataset and addressed several challenges such as the transfer learning property and the class imbalance. W-Net achieved an average classification accuracy of 97%. We synthesized a dataset of new WBC image samples using DCGAN, which we released to the public for education and research purposes.">
    <meta name="dc.creator" content="Jung, Changhun">
    <meta name="dc.creator" content="Abuhamad, Mohammed">
    <meta name="dc.creator" content="Mohaisen, David">
    <meta name="dc.creator" content="Han, Kyungja">
    <meta name="dc.creator" content="Nyang, DaeHun">
    <meta name="dc.subject" content="Imaging / Radiology">
    <meta name="citation_reference" content="Changhun J, Mohammed A, Jumabek A, Aziz M, Kyungja H, DaeHun N. W-Net: a CNN-based architecture for white blood cells image classification. In: AAAI 2019 fall symposium on AI for social good; 2019.">
    <meta name="citation_reference" content="citation_journal_title=Blood; citation_title=In vivo labeling with 2H2O reveals a human neutrophil lifespan of 5.4 days; citation_author=J Pillay; citation_volume=116; citation_issue=4; citation_publication_date=2010; citation_pages=625-627; citation_doi=10.1182/blood-2010-01-259028; citation_id=CR2">
    <meta name="citation_reference" content="citation_journal_title=Annu Rev Immunol; citation_title=The eosinophil; citation_author=ME Rothenberg, SP Hogan; citation_volume=24; citation_publication_date=2006; citation_pages=147-174; citation_doi=10.1146/annurev.immunol.24.021605.090720; citation_id=CR3">
    <meta name="citation_reference" content="citation_journal_title=Blood; citation_title=The human basophil: a new appreciation of its role in immune responses; citation_author=FH Falcone, H Haas, BF Gibbs; citation_volume=13; citation_publication_date=2000; citation_pages=4028-4038; citation_doi=10.1182/blood.V96.13.4028; citation_id=CR4">
    <meta name="citation_reference" content="citation_journal_title=Science; citation_title=Lymphocyte homing and homeostasis; citation_author=EC Butcher, LJ Picker; citation_volume=272; citation_issue=5258; citation_publication_date=1996; citation_pages=60-67; citation_doi=10.1126/science.272.5258.60; citation_id=CR5">
    <meta name="citation_reference" content="citation_journal_title=Nat Rev Immunol; citation_title=Monocyte and macrophage heterogeneity; citation_author=S Gordon, PR Taylor; citation_volume=5; citation_publication_date=2005; citation_pages=953-964; citation_doi=10.1038/nri1733; citation_id=CR6">
    <meta name="citation_reference" content="WBC (White Blood Cell) Count. 
                  https://bit.ly/3cDg58c
                  
                . Accessed: 2020-06-09.">
    <meta name="citation_reference" content="Statistics. 
                  https://bit.ly/30esTwt
                  
                . Accessed: 2019-06-27.">
    <meta name="citation_reference" content="Leukemia. 
                  https://bit.ly/32WFwOn
                  
                . Accessed: 2019-06-29.">
    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Med Imaging; citation_title=Deep convolutional neural networks for computer-aided detection: CNN architectures, dataset characteristics and transfer learning; citation_author=H Shin; citation_volume=35; citation_issue=5; citation_publication_date=2016; citation_pages=1285-1298; citation_doi=10.1109/TMI.2016.2528162; citation_id=CR10">
    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Med Imaging; citation_title=Convolutional neural networks for medical image analysis: full training or fine tuning?; citation_author=N Tajbakhsh; citation_volume=35; citation_issue=5; citation_publication_date=2016; citation_pages=1299-1312; citation_doi=10.1109/TMI.2016.2535302; citation_id=CR11">
    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Med Imaging; citation_title=CNN-based projected gradient descent for consistent CT image reconstruction; citation_author=H Gupta, KH Jin, HQ Nguyen, MT McCann, M Unser; citation_volume=37; citation_issue=6; citation_publication_date=2018; citation_pages=1440-1453; citation_doi=10.1109/TMI.2018.2832656; citation_id=CR12">
    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Med Imaging; citation_title=Generative adversarial networks for noise reduction in low-dose CT; citation_author=JM Wolterink, T Leiner, MA Viergever, I Isgum; citation_volume=36; citation_issue=12; citation_publication_date=2017; citation_pages=2536-2545; citation_doi=10.1109/TMI.2017.2708987; citation_id=CR13">
    <meta name="citation_reference" content="Krizhevsky A, Sutskever I, Hinton GE. Imagenet classification with deep convolutional neural networks. In: Advances in NIPS; 2012.">
    <meta name="citation_reference" content="Simonyan K, Zisserman A. Very deep convolutional networks for large-scale image recognition. arXiv preprint 
                  arXiv:1409.1556
                  
                 (2014).">
    <meta name="citation_reference" content="He K, Zhang X, Ren S, Sun J. Deep residual learning for image recognition. In: IEEE CVPR; 2016. p. 770–8.">
    <meta name="citation_reference" content="citation_journal_title=Comput Med Imaging Graph; citation_title=Automatic recognition of five types of white blood cells in peripheral blood; citation_author=SH Rezatofighi, H Soltanian-Zadeh; citation_volume=35; citation_issue=4; citation_publication_date=2011; citation_pages=333-343; citation_doi=10.1016/j.compmedimag.2011.01.003; citation_id=CR17">
    <meta name="citation_reference" content="Goodfellow I, et al. Generative adversarial nets. In: Advances in NIPS; 2014. p. 2672–80.">
    <meta name="citation_reference" content="citation_journal_title=Opt Laser Technol; citation_title=A spectral and morphologic method for white blood cell classification; citation_author=Q Wang, L Chang, M Zhou, Q Li, H Liu, F Guo; citation_volume=84; citation_publication_date=2016; citation_pages=144-148; citation_doi=10.1016/j.optlastec.2016.05.013; citation_id=CR19">
    <meta name="citation_reference" content="citation_journal_title=IEEE J Biomed Health Inform; citation_title=Semiautomatic white blood cell segmentation based on multiscale analysis; citation_author=LB Dorini, R Minetto, NJ Leite; citation_volume=17; citation_issue=1; citation_publication_date=2012; citation_pages=250-256; citation_doi=10.1109/TITB.2012.2207398; citation_id=CR20">
    <meta name="citation_reference" content="citation_journal_title=Biomed Eng Online; citation_title=Segmentation of white blood cells and comparison of cell morphology by linear and naïve bayes classifiers; citation_author=J Prinyakupt, C Pluempitiwiriyawej; citation_volume=14; citation_issue=1; citation_publication_date=2015; citation_pages=63; citation_doi=10.1186/s12938-015-0037-1; citation_id=CR21">
    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Inf Technol Biomed Publ Inf; citation_title=A new detection algorithm (NDA) based on fuzzy cellular neural networks for white blood cell detection; citation_author=W Shitong, W Min; citation_volume=10; citation_issue=1; citation_publication_date=2006; citation_pages=5-10; citation_doi=10.1109/TITB.2005.855545; citation_id=CR22">
    <meta name="citation_reference" content="citation_journal_title=Comput Methods Prog Biomed; citation_title=Recent computational methods for white blood cell nuclei segmentation: a comparative study; citation_author=AR Andrade, LH Vogado, MS Veras, RRV Silva, FH Araujo, FN Medeiros; citation_volume=173; citation_publication_date=2019; citation_pages=1-14; citation_doi=10.1016/j.cmpb.2019.03.001; citation_id=CR23">
    <meta name="citation_reference" content="citation_journal_title=Procedia Comput Sci; citation_title=Fuzzy c means detection of leukemia based on morphological contour segmentation; citation_author=P Viswanathan; citation_volume=58; citation_publication_date=2015; citation_pages=84-90; citation_doi=10.1016/j.procs.2015.08.017; citation_id=CR24">
    <meta name="citation_reference" content="Gautam A, Bhadauria H. White blood nucleus extraction using k-mean clustering and mathematical morphing. In: 2014 5th International conference-confluence the next generation information technology summit (confluence). IEEE; 2014. p. 549–554.">
    <meta name="citation_reference" content="Mohapatra S, Samanta SS, Patra D, Satpathi S. Fuzzy based blood image segmentation for automated leukemia detection. In: 2011 ICDeCom. IEEE; 2011. p. 1–5.">
    <meta name="citation_reference" content="citation_journal_title=Measurement; citation_title=Automatic segmentation, counting, size determination and classification of white blood cells; citation_author=S Nazlibilek, D Karacor, T Ercan, MH Sazli, O Kalender, Y Ege; citation_volume=55; citation_publication_date=2014; citation_pages=58-65; citation_doi=10.1016/j.measurement.2014.04.008; citation_id=CR27">
    <meta name="citation_reference" content="citation_title=Computer-aided acute lymphoblastic leukemia diagnosis system based on image analysis; citation_publication_date=2018; citation_id=CR28; citation_author=AM Abdeldaim; citation_author=AT Sahlol; citation_author=M Elhoseny; citation_author=AE Hassanien; citation_publisher=Springer">
    <meta name="citation_reference" content="Tosta TAA, De Abreu AF, Travençolo BAN, do Nascimento MZ, Neves LA. Unsupervised segmentation of leukocytes images using thresholding neighborhood valley-emphasis. In: 2015 IEEE CBMS. IEEE; 2015. p. 93–94.">
    <meta name="citation_reference" content="citation_journal_title=Biomed Signal Process Control; citation_title=A novel algorithm for segmentation of leukocytes in peripheral blood; citation_author=H Cao, H Liu, E Song; citation_volume=45; citation_publication_date=2018; citation_pages=10-21; citation_doi=10.1016/j.bspc.2018.05.010; citation_id=CR30">
    <meta name="citation_reference" content="Mohammed EA, Mohamed MM, Naugler C, Far BH. Chronic lymphocytic leukemia cell segmentation from microscopic blood images using watershed algorithm and optimal thresholding. In: 2013 26th IEEE CCECE. IEEE; 2013. p. 1–5.">
    <meta name="citation_reference" content="citation_journal_title=J Med Syst; citation_title=Development of a robust algorithm for detection of nuclei and classification of white blood cells in peripheral blood smear images; citation_author=RB Hegde, K Prasad, H Hebbar, BMK Singh; citation_volume=42; citation_issue=6; citation_publication_date=2018; citation_pages=110; citation_doi=10.1007/s10916-018-0962-1; citation_id=CR32">
    <meta name="citation_reference" content="citation_journal_title=BMC Bioinform; citation_title=Leukocyte nucleus segmentation and nucleus lobe counting; citation_author=Y-K Chan, M-H Tsai, D-C Huang, Z-H Zheng, K-D Hung; citation_volume=11; citation_issue=1; citation_publication_date=2010; citation_pages=558; citation_doi=10.1186/1471-2105-11-558; citation_id=CR33">
    <meta name="citation_reference" content="citation_journal_title=J Pathol Inform; citation_title=Scalable system for classification of white blood cells from Leishman stained blood stain images; citation_author=A Mathur, AS Tripathi, M Kuse; citation_volume=4; citation_issue=Suppl; citation_publication_date=2013; citation_pages=15; citation_doi=10.4103/2153-3539.109883; citation_id=CR34">
    <meta name="citation_reference" content="citation_journal_title=Sci World J; citation_title=A neural-network-based approach to WBC classification; citation_author=M-C Su, C-Y Cheng, P-C Wang; citation_volume=2014; citation_publication_date=2014; citation_pages=1-9; citation_id=CR35">
    <meta name="citation_reference" content="citation_journal_title=J Pathol Inform; citation_title=Isolation and two-step classification of normal white blood cells in peripheral blood smears; citation_author=N Ramesh, B Dangott, ME Salama, T Tasdizen; citation_volume=3; citation_publication_date=2012; citation_pages=179-191; citation_doi=10.4103/2153-3539.93895; citation_id=CR36">
    <meta name="citation_reference" content="citation_journal_title=Appl Soft Comput; citation_title=Blood smear analyzer for white blood cell counting: a hybrid microscopic image analyzing technique; citation_author=P Ghosh, D Bhattacharjee, M Nasipuri; citation_volume=46; citation_publication_date=2016; citation_pages=629-638; citation_doi=10.1016/j.asoc.2015.12.038; citation_id=CR37">
    <meta name="citation_reference" content="Habibzadeh M, Jannesari M, Rezaei Z, Baharvand H, Totonchi M. Automatic white blood cell classification using pre-trained deep learning models: Resnet and inception. In: Tenth ICMV 2017, vol 10696. International Society for Optics and Photonics; 2018. p. 1069612.">
    <meta name="citation_reference" content="citation_journal_title=Int J Comput Syst Eng; citation_title=Application of ensemble artificial neural network for the classification of white blood cells using microscopic blood images; citation_author=J Rawat, A Singh, H Bhadauria, J Virmani, JS Devgun; citation_volume=4; citation_issue=2–3; citation_publication_date=2018; citation_pages=202-216; citation_doi=10.1504/IJCSYSE.2018.091407; citation_id=CR39">
    <meta name="citation_reference" content="citation_journal_title=IRBM; citation_title=White blood cells image classification using deep learning with canonical correlation analysis; citation_author=AM Patil, MD Patil, GK Birajdar; citation_volume=42; citation_issue=5; citation_publication_date=2021; citation_pages=378-389; citation_doi=10.1016/j.irbm.2020.08.005; citation_id=CR40">
    <meta name="citation_reference" content="citation_journal_title=Appl Soft Comput; citation_title=Classification of white blood cells using deep features obtained from convolutional neural network models based on the combination of feature selection methods; citation_author=M Toğaçar, B Ergen, Z Cömert; citation_volume=97; citation_publication_date=2020; citation_pages=106810; citation_doi=10.1016/j.asoc.2020.106810; citation_id=CR41">
    <meta name="citation_reference" content="citation_journal_title=IEEE Access; citation_title=A review on traditional machine learning and deep learning models for WBCs classification in blood smear images; citation_author=S Khan, M Sajjad, T Hussain, A Ullah, AS Imran; citation_volume=9; citation_publication_date=2020; citation_pages=10657-10673; citation_doi=10.1109/ACCESS.2020.3048172; citation_id=CR42">
    <meta name="citation_reference" content="citation_journal_title=J Commun Softw Syst; citation_title=Improved white blood cells classification based on pre-trained deep learning models; citation_author=EH Mohamed, WH El-Behaidy, G Khoriba, J Li; citation_volume=16; citation_issue=1; citation_publication_date=2020; citation_pages=37-45; citation_doi=10.24138/jcomss.v16i1.818; citation_id=CR43">
    <meta name="citation_reference" content="citation_journal_title=Expert Syst Appl; citation_title=An automatic nucleus segmentation and CNN model based classification method of white blood cell; citation_author=PP Banik, R Saha, KD Kim; citation_volume=149; citation_publication_date=2020; citation_pages=113211; citation_doi=10.1016/j.eswa.2020.113211; citation_id=CR44">
    <meta name="citation_reference" content="citation_journal_title=Soft Comput; citation_title=Interpolative Leishman-stained transformation invariant deep pattern classification for white blood cells; citation_author=MP Karthikeyan, R Venkatesan; citation_volume=24; citation_issue=16; citation_publication_date=2020; citation_pages=12215-12225; citation_doi=10.1007/s00500-019-04662-4; citation_id=CR45">
    <meta name="citation_reference" content="citation_journal_title=Med Hypotheses; citation_title=White blood cells detection and classification based on regional convolutional neural networks; citation_author=H Kutlu, E Avci, F Özyurt; citation_volume=135; citation_publication_date=2020; citation_pages=109472; citation_doi=10.1016/j.mehy.2019.109472; citation_id=CR46">
    <meta name="citation_reference" content="Rakhlin A, Shvets A, Iglovikov V, Kalinin AA. Deep convolutional neural networks for breast cancer histology image analysis. In: ICIAR. Springer; 2018. p. 737–44.">
    <meta name="citation_reference" content="citation_journal_title=Cell; citation_title=Identifying medical diagnoses and treatable diseases by image-based deep learning; citation_author=DS Kermany; citation_volume=172; citation_issue=5; citation_publication_date=2018; citation_pages=1122-1131; citation_doi=10.1016/j.cell.2018.02.010; citation_id=CR48">
    <meta name="citation_reference" content="citation_journal_title=Futur Gener Comput Syst; citation_title=Automated identification of shockable and non-shockable life-threatening ventricular arrhythmias using convolutional neural network; citation_author=UR Acharya; citation_volume=79; citation_publication_date=2018; citation_pages=952-959; citation_doi=10.1016/j.future.2017.08.039; citation_id=CR49">
    <meta name="citation_reference" content="Moran MB, et al. Identification of thyroid nodules in infrared images by convolutional neural networks. In: 2018 IJCNN. IEEE; 2018. p. 1–7.">
    <meta name="citation_reference" content="citation_journal_title=Med Image Anal; citation_title=Mitotic nuclei analysis in breast cancer histopathology images using deep ensemble classifier; citation_author=A Sohail, A Khan, H Nisar, S Tabassum, A Zameer; citation_volume=72; citation_publication_date=2021; citation_pages=102121; citation_doi=10.1016/j.media.2021.102121; citation_id=CR51">
    <meta name="citation_reference" content="citation_journal_title=Neuroimage; citation_title=DeepNAT: deep convolutional neural network for segmenting neuroanatomy; citation_author=C Wachinger, M Reuter, T Klein; citation_volume=170; citation_publication_date=2018; citation_pages=434-445; citation_doi=10.1016/j.neuroimage.2017.02.035; citation_id=CR52">
    <meta name="citation_reference" content="citation_journal_title=Artif Intell Med; citation_title=Multi-feature representation for burn depth classification via burn images; citation_author=B Zhang, J Zhou; citation_volume=118; citation_publication_date=2021; citation_pages=102128; citation_doi=10.1016/j.artmed.2021.102128; citation_id=CR53">
    <meta name="citation_reference" content="citation_journal_title=Artif Intell Med; citation_title=Transfer learning in medical image segmentation: new insights from analysis of the dynamics of model parameters and learned representations; citation_author=D Karimi, SK Warfield, A Gholipour; citation_volume=116; citation_publication_date=2021; citation_pages=102078; citation_doi=10.1016/j.artmed.2021.102078; citation_id=CR54">
    <meta name="citation_reference" content="citation_journal_title=Artif Intell Med; citation_title=A deep learning approach for mitosis detection: application in tumor proliferation prediction from whole slide images; citation_author=R Nateghi, H Danyali, MS Helfroush; citation_volume=114; citation_publication_date=2021; citation_pages=102048; citation_doi=10.1016/j.artmed.2021.102048; citation_id=CR55">
    <meta name="citation_reference" content="citation_journal_title=BMC Med Imaging; citation_title=Image quality assessment of pediatric chest and abdomen ct by deep learning reconstruction; citation_author=H Yoon, J Kim, HJ Lim, M-J Lee; citation_volume=21; citation_publication_date=2021; citation_pages=1-11; citation_doi=10.1186/s12880-021-00677-2; citation_id=CR56">
    <meta name="citation_reference" content="citation_journal_title=BMC Med Imaging; citation_title=Efficiency of a deep learning-based artificial intelligence diagnostic system in spontaneous intracerebral hemorrhage volume measurement; citation_author=T Wang, N Song, L Liu, Z Zhu, B Chen, W Yang, Z Chen; citation_volume=21; citation_issue=1; citation_publication_date=2021; citation_pages=1-9; citation_doi=10.1186/s12880-021-00657-6; citation_id=CR57">
    <meta name="citation_reference" content="citation_journal_title=BMC Med Imaging; citation_title=Hahn-PCNN-CNN: an end-to-end multi-modal brain medical image fusion framework useful for clinical diagnosis; citation_author=K Guo, X Li, X Hu, J Liu, T Fan; citation_volume=21; citation_issue=1; citation_publication_date=2021; citation_pages=1-22; citation_doi=10.1186/s12880-021-00642-z; citation_id=CR58">
    <meta name="citation_reference" content="citation_journal_title=BMC Med Imaging; citation_title=Application of a deep learning image reconstruction (DLIR) algorithm in head CT imaging for children to improve image quality and lesion detection; citation_author=J Sun, H Li, B Wang, J Li, M Li, Z Zhou, Y Peng; citation_volume=21; citation_issue=1; citation_publication_date=2021; citation_pages=1-9; citation_doi=10.1186/s12880-021-00637-w; citation_id=CR59">
    <meta name="citation_reference" content="The Catholic University of Korea Institutional Review Board. 
                  https://bit.ly/2YrlQPl
                  
                . Accessed: 2019-07-17.">
    <meta name="citation_reference" content="Sysmex DI-60. 
                  https://bit.ly/313v6L3
                  
                . Accessed: 2019-07-17.">
    <meta name="citation_reference" content="citation_journal_title=J Mach Learn Res; citation_title=Dropout: a simple way to prevent neural networks from overfitting; citation_author=N Srivastava, G Hinton, A Krizhevsky, I Sutskever, R Salakhutdinov; citation_volume=15; citation_issue=1; citation_publication_date=2014; citation_pages=1929-1958; citation_id=CR62">
    <meta name="citation_reference" content="Kingma DP, Ba J. Adam: a method for stochastic optimization. arXiv preprint 
                  arXiv:1412.6980
                  
                 (2014).">
    <meta name="citation_reference" content="citation_journal_title=Stat Surv; citation_title=A survey of cross-validation procedures for model selection; citation_author=S Arlot, A Celisse; citation_volume=4; citation_publication_date=2010; citation_pages=40-79; citation_doi=10.1214/09-SS054; citation_id=CR64">
    <meta name="citation_reference" content="citation_journal_title=Neural Comput; citation_title=Are loss functions all the same?; citation_author=L Rosasco, ED Vito, A Caponnetto, M Piana, A Verri; citation_volume=16; citation_issue=5; citation_publication_date=2004; citation_pages=1063-1076; citation_doi=10.1162/089976604773135104; citation_id=CR65">
    <meta name="citation_reference" content="Sutskever I, Martens J, Dahl G, Hinton G. On the importance of initialization and momentum in deep learning. In: International conference on ML; 2013. p. 1139–47.">
    <meta name="citation_reference" content="Jung, C. W-Net model and generated WBC images. 2022. 
                  https://bit.ly/2KAffwM
                  
                . Accessed: 2022-3-24.">
    <meta name="citation_reference" content="Tiny ImageNet. 
                  https://bit.ly/36Qxvfp
                  
                . Accessed: 2020-01-13.">
    <meta name="citation_reference" content="Gregor K, Danihelka I, Graves A, Rezende D, Wierstra D. Draw: a recurrent neural network for image generation. In: International conference on machine learning. PMLR (2015, June). p. 1462–71.">
    <meta name="citation_reference" content="Radford A, Metz L, Chintala S. Unsupervised representation learning with deep convolutional generative adversarial networks. arXiv preprint 
                  arXiv:1511.06434
                  
                 (2015).">
    <meta name="citation_reference" content="Maas AL, Hannun AY, Ng AY. Rectifier nonlinearities improve neural network acoustic models. In: Proceedings of ICML, vol 30; 2013. p. 3.">
    <meta name="citation_reference" content="Ioffe S, Szegedy C. Batch normalization: accelerating deep network training by reducing internal covariate shift. arXiv preprint 
                  arXiv:1502.03167
                  
                 (2).">
    <meta name="citation_reference" content="Kanbilim Dataset. 
                  http://kanbilim.com/
                  
                . Accessed: 2019-06-15.">
    <meta name="citation_reference" content="Ghosh A, Singh S, Sheet D. Simultaneous localization and classification of acute lymphoblastic leukemic cells in peripheral blood smears using a deep convolutional network with average pooling layer. In: 2017 IEEE ICIIS. IEEE; 2017. p. 1–6.">
    <meta name="citation_reference" content="WBC-classification. 
                  https://bit.ly/2zbz8oA
                  
                . Accessed: 2019-06-15.">
    <meta name="citation_reference" content="citation_journal_title=IEEE Access; citation_title=Combining convolutional neural network with recursive neural network for blood cell image classification; citation_author=G Liang, H Hong, W Xie, L Zheng; citation_volume=6; citation_publication_date=2018; citation_pages=36188-36197; citation_doi=10.1109/ACCESS.2018.2846685; citation_id=CR76">
    <meta name="citation_reference" content="BCCD dataset. 
                  https://bit.ly/2X5vQOl
                  
                . Accessed: 2019-06-15.">
    <meta name="citation_reference" content="An efficient technique for white blood cells nuclei automatic segmentation. 
                  https://bit.ly/2XN064Z
                  
                . Accessed: 2019-06-15.">
    <meta name="citation_reference" content="citation_journal_title=Artif Intell Med; citation_title=Leucocyte classification for leukaemia detection using image processing techniques; citation_author=L Putzu, G Caocci, C Ruberto; citation_volume=62; citation_issue=3; citation_publication_date=2014; citation_pages=179-191; citation_doi=10.1016/j.artmed.2014.09.002; citation_id=CR79">
    <meta name="citation_reference" content="CellaVision. 
                  https://www.cellavision.com/
                  
                . Accessed: 2019-06-15.">
    <meta name="citation_author" content="Jung, Changhun">
    <meta name="citation_author_institution" content="Department of Cyber Security, Ewha Womans University, Seoul, Republic of Korea">
    <meta name="citation_author" content="Abuhamad, Mohammed">
    <meta name="citation_author_institution" content="Department of Computer Science, Loyola University Chicago, Chicago, USA">
    <meta name="citation_author" content="Mohaisen, David">
    <meta name="citation_author_institution" content="Department of Computer Science, University of Central Florida, Orlando, USA">
    <meta name="citation_author" content="Han, Kyungja">
    <meta name="citation_author_institution" content="Department of Laboratory Medicine and College of Medicine, The Catholic University of Korea Seoul St. Mary’s Hospital, Seoul, Republic of Korea">
    <meta name="citation_author" content="Nyang, DaeHun">
    <meta name="citation_author_institution" content="Department of Cyber Security, Ewha Womans University, Seoul, Republic of Korea">
    

            
    
    <meta property="og:url" content="https://link.springer.com/article/10.1186/s12880-022-00818-1">
    <meta property="og:type" content="article">
    <meta property="og:site_name" content="SpringerLink">
    <meta property="og:title" content="WBC image classification and generative models based on convolutional neural network - BMC Medical Imaging">
    <meta property="og:description" content="Background Computer-aided methods for analyzing white blood cells (WBC) are popular due to the complexity of the manual alternatives. Recent works have shown highly accurate segmentation and detection of white blood cells from microscopic blood images. However, the classification of the observed cells is still a challenge, in part due to the distribution of the five types that affect the condition of the immune system. Methods (i) This work proposes W-Net, a CNN-based method for WBC classification. We evaluate W-Net on a real-world large-scale dataset that includes 6562 real images of the five WBC types. (ii) For further benefits, we generate synthetic WBC images using Generative Adversarial Network to be used for education and research purposes through sharing. Results (i) W-Net achieves an average accuracy of 97%. In comparison to state-of-the-art methods in the field of WBC classification, we show that W-Net outperforms other CNN- and RNN-based model architectures. Moreover, we show the benefits of using pre-trained W-Net in a transfer learning context when fine-tuned to specific task or accommodating another dataset. (ii) The synthetic WBC images are confirmed by experiments and a domain expert to have a high degree of similarity to the original images. The pre-trained W-Net and the generated WBC dataset are available for the community to facilitate reproducibility and follow up research work. Conclusion This work proposed W-Net, a CNN-based architecture with a small number of layers, to accurately classify the five WBC types. We evaluated W-Net on a real-world large-scale dataset and addressed several challenges such as the transfer learning property and the class imbalance. W-Net achieved an average classification accuracy of 97%. We synthesized a dataset of new WBC image samples using DCGAN, which we released to the public for education and research purposes.">
    <meta property="og:image" content="https://static-content.springer.com/image/art%3A10.1186%2Fs12880-022-00818-1/MediaObjects/12880_2022_818_Fig1_HTML.png">
    

            
        

        <meta name="format-detection" content="telephone=no">

        
    
        
    
    
    

    


        <link rel="apple-touch-icon" sizes="180x180" href="/oscar-static/img/favicons/darwin/apple-touch-icon-92e819bf8a.png">
<link rel="icon" type="image/png" sizes="192x192" href="/oscar-static/img/favicons/darwin/android-chrome-192x192-6f081ca7e5.png">
<link rel="icon" type="image/png" sizes="32x32" href="/oscar-static/img/favicons/darwin/favicon-32x32-1435da3e82.png">
<link rel="icon" type="image/png" sizes="16x16" href="/oscar-static/img/favicons/darwin/favicon-16x16-ed57f42bd2.png">
<link rel="shortcut icon" data-test="shortcut-icon" href="/oscar-static/img/favicons/darwin/favicon-c6d59aafac.ico">

<meta name="theme-color" content="#e6e6e6">


        <!-- Please see discussion: https://github.com/springernature/frontend-open-space/issues/316-->
<!--TODO: Implement alternative to CTM in here if the discussion concludes we do not continue with CTM as a practice-->


<link rel="stylesheet" media="print" href="/oscar-static/app-springerlink/css/print-b8af42253b.css">



    
        
            
    <style> html{text-size-adjust:100%;line-height:1.15}body{font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;line-height:1.8;margin:0}details,main{display:block}h1{font-size:2em;margin:.67em 0}a{background-color:transparent;color:#025e8d}sub{bottom:-.25em;font-size:75%;line-height:0;position:relative;vertical-align:baseline}img{border:0;height:auto;max-width:100%;vertical-align:middle}button,input{font-family:inherit;font-size:100%;line-height:1.15;margin:0;overflow:visible}button{text-transform:none}[type=button],[type=submit],button{-webkit-appearance:button}[type=search]{-webkit-appearance:textfield;outline-offset:-2px}summary{display:list-item}[hidden]{display:none}button{cursor:pointer}svg{height:1rem;width:1rem} </style>
    <style>@media only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark) {  body{background:#fff;color:#222;font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;line-height:1.8;min-height:100%}a{color:#025e8d;text-decoration:underline;text-decoration-skip-ink:auto}button{cursor:pointer}img{border:0;height:auto;max-width:100%;vertical-align:middle}html{box-sizing:border-box;font-size:100%;height:100%;overflow-y:scroll}h1{font-size:2.25rem}h2{font-size:1.75rem}h1,h2,h4{font-weight:700;line-height:1.2}h4{font-size:1.25rem}body{font-size:1.125rem}*{box-sizing:inherit}p{margin-bottom:2rem;margin-top:0}p:last-of-type{margin-bottom:0}.c-ad{text-align:center}@media only screen and (min-width:480px){.c-ad{padding:8px}}.c-ad--728x90{display:none}.c-ad--728x90 .c-ad__inner{min-height:calc(1.5em + 94px)}@media only screen and (min-width:876px){.js .c-ad--728x90{display:none}}.c-ad__label{color:#333;font-size:.875rem;font-weight:400;line-height:1.5;margin-bottom:4px}.c-ad__label,.c-status-message{font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif}.c-status-message{align-items:center;box-sizing:border-box;display:flex;position:relative;width:100%}.c-status-message :last-child{margin-bottom:0}.c-status-message--boxed{background-color:#fff;border:1px solid #ccc;line-height:1.4;padding:16px}.c-status-message__heading{font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;font-size:.875rem;font-weight:700}.c-status-message__icon{fill:currentcolor;display:inline-block;flex:0 0 auto;height:1.5em;margin-right:8px;transform:translate(0);vertical-align:text-top;width:1.5em}.c-status-message__icon--top{align-self:flex-start}.c-status-message--info .c-status-message__icon{color:#003f8d}.c-status-message--boxed.c-status-message--info{border-bottom:4px solid #003f8d}.c-status-message--error .c-status-message__icon{color:#c40606}.c-status-message--boxed.c-status-message--error{border-bottom:4px solid #c40606}.c-status-message--success .c-status-message__icon{color:#00b8b0}.c-status-message--boxed.c-status-message--success{border-bottom:4px solid #00b8b0}.c-status-message--warning .c-status-message__icon{color:#edbc53}.c-status-message--boxed.c-status-message--warning{border-bottom:4px solid #edbc53}.eds-c-header{background-color:#fff;border-bottom:2px solid #01324b;font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;font-size:1rem;line-height:1.5;padding:8px 0 0}.eds-c-header__container{align-items:center;display:flex;flex-wrap:nowrap;gap:8px 16px;justify-content:space-between;margin:0 auto 8px;max-width:1280px;padding:0 8px;position:relative}.eds-c-header__nav{border-top:2px solid #c5e0f4;padding-top:4px;position:relative}.eds-c-header__nav-container{align-items:center;display:flex;flex-wrap:wrap;margin:0 auto 4px;max-width:1280px;padding:0 8px;position:relative}.eds-c-header__nav-container>:not(:last-child){margin-right:32px}.eds-c-header__link-container{align-items:center;display:flex;flex:1 0 auto;gap:8px 16px;justify-content:space-between}.eds-c-header__list{list-style:none;margin:0;padding:0}.eds-c-header__list-item{font-weight:700;margin:0 auto;max-width:1280px;padding:8px}.eds-c-header__list-item:not(:last-child){border-bottom:2px solid #c5e0f4}.eds-c-header__item{color:inherit}@media only screen and (min-width:768px){.eds-c-header__item--menu{display:none;visibility:hidden}.eds-c-header__item--menu:first-child+*{margin-block-start:0}}.eds-c-header__item--inline-links{display:none;visibility:hidden}@media only screen and (min-width:768px){.eds-c-header__item--inline-links{display:flex;gap:16px 16px;visibility:visible}}.eds-c-header__item--divider:before{border-left:2px solid #c5e0f4;content:"";height:calc(100% - 16px);margin-left:-15px;position:absolute;top:8px}.eds-c-header__brand{padding:16px 8px}.eds-c-header__brand a{display:block;line-height:1;text-decoration:none}.eds-c-header__brand img{height:1.5rem;width:auto}.eds-c-header__link{color:inherit;display:inline-block;font-weight:700;padding:16px 8px;position:relative;text-decoration-color:transparent;white-space:nowrap;word-break:normal}.eds-c-header__icon{fill:currentcolor;display:inline-block;font-size:1.5rem;height:1em;transform:translate(0);vertical-align:bottom;width:1em}.eds-c-header__icon+*{margin-left:8px}.eds-c-header__expander{background-color:#f0f7fc}.eds-c-header__search{display:block;padding:24px 0}@media only screen and (min-width:768px){.eds-c-header__search{max-width:70%}}.eds-c-header__search-container{position:relative}.eds-c-header__search-label{color:inherit;display:inline-block;font-weight:700;margin-bottom:8px}.eds-c-header__search-input{background-color:#fff;border:1px solid #000;padding:8px 48px 8px 8px;width:100%}.eds-c-header__search-button{background-color:transparent;border:0;color:inherit;height:100%;padding:0 8px;position:absolute;right:0}.has-tethered.eds-c-header__expander{border-bottom:2px solid #01324b;left:0;margin-top:-2px;top:100%;width:100%;z-index:10}@media only screen and (min-width:768px){.has-tethered.eds-c-header__expander--menu{display:none;visibility:hidden}}.has-tethered .eds-c-header__heading{display:none;visibility:hidden}.has-tethered .eds-c-header__heading:first-child+*{margin-block-start:0}.has-tethered .eds-c-header__search{margin:auto}.eds-c-header__heading{margin:0 auto;max-width:1280px;padding:16px 16px 0}.eds-c-pagination{align-items:center;display:flex;flex-wrap:wrap;font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;font-size:.875rem;gap:16px 0;justify-content:center;line-height:1.4;list-style:none;margin:0;padding:32px 0}@media only screen and (min-width:480px){.eds-c-pagination{padding:32px 16px}}.eds-c-pagination__item{margin-right:8px}.eds-c-pagination__item--prev{margin-right:16px}.eds-c-pagination__item--next .eds-c-pagination__link,.eds-c-pagination__item--prev .eds-c-pagination__link{padding:16px 8px}.eds-c-pagination__item--next{margin-left:8px}.eds-c-pagination__item:last-child{margin-right:0}.eds-c-pagination__link{align-items:center;color:#222;cursor:pointer;display:inline-block;font-size:1rem;margin:0;padding:16px 24px;position:relative;text-align:center;transition:all .2s ease 0s}.eds-c-pagination__link:visited{color:#222}.eds-c-pagination__link--disabled{border-color:#555;color:#555;cursor:default}.eds-c-pagination__link--active{background-color:#01324b;background-image:none;border-radius:8px;color:#fff}.eds-c-pagination__link--active:focus,.eds-c-pagination__link--active:hover,.eds-c-pagination__link--active:visited{color:#fff}.eds-c-pagination__link-container{align-items:center;display:flex}.eds-c-pagination__icon{fill:#222;height:1.5rem;width:1.5rem}.eds-c-pagination__icon--disabled{fill:#555}.eds-c-pagination__visually-hidden{clip:rect(0,0,0,0);border:0;clip-path:inset(50%);height:1px;overflow:hidden;padding:0;position:absolute!important;white-space:nowrap;width:1px}.c-breadcrumbs{color:#333;font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;font-size:1rem;list-style:none;margin:0;padding:0}.c-breadcrumbs>li{display:inline}svg.c-breadcrumbs__chevron{fill:#333;height:10px;margin:0 .25rem;width:10px}.c-breadcrumbs--contrast,.c-breadcrumbs--contrast .c-breadcrumbs__link{color:#fff}.c-breadcrumbs--contrast svg.c-breadcrumbs__chevron{fill:#fff}@media only screen and (max-width:479px){.c-breadcrumbs .c-breadcrumbs__item{display:none}.c-breadcrumbs .c-breadcrumbs__item:last-child,.c-breadcrumbs .c-breadcrumbs__item:nth-last-child(2){display:inline}}.c-skip-link{background:#01324b;bottom:auto;color:#fff;font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;font-size:1rem;padding:8px;position:absolute;text-align:center;transform:translateY(-100%);width:100%;z-index:9999}@media (prefers-reduced-motion:reduce){.c-skip-link{transition:top .3s ease-in-out 0s}}@media print{.c-skip-link{display:none}}.c-skip-link:active,.c-skip-link:hover,.c-skip-link:link,.c-skip-link:visited{color:#fff}.c-skip-link:focus{transform:translateY(0)}.l-with-sidebar{display:flex;flex-wrap:wrap}.l-with-sidebar>*{margin:0}.l-with-sidebar__sidebar{flex-basis:var(--with-sidebar--basis,400px);flex-grow:1}.l-with-sidebar>:not(.l-with-sidebar__sidebar){flex-basis:0px;flex-grow:999;min-width:var(--with-sidebar--min,53%)}.l-with-sidebar>:first-child{padding-right:4rem}@supports (gap:1em){.l-with-sidebar>:first-child{padding-right:0}.l-with-sidebar{gap:var(--with-sidebar--gap,4rem)}}.c-header__link{color:inherit;display:inline-block;font-weight:700;padding:16px 8px;position:relative;text-decoration-color:transparent;white-space:nowrap;word-break:normal}.app-masthead__colour-4{--background-color:#ff9500;--gradient-light:rgba(0,0,0,.5);--gradient-dark:rgba(0,0,0,.8)}.app-masthead{background:var(--background-color,#0070a8);position:relative}.app-masthead:after{background:radial-gradient(circle at top right,var(--gradient-light,rgba(0,0,0,.4)),var(--gradient-dark,rgba(0,0,0,.7)));bottom:0;content:"";left:0;position:absolute;right:0;top:0}@media only screen and (max-width:479px){.app-masthead:after{background:linear-gradient(225deg,var(--gradient-light,rgba(0,0,0,.4)),var(--gradient-dark,rgba(0,0,0,.7)))}}.app-masthead__container{color:var(--masthead-color,#fff);margin:0 auto;max-width:1280px;padding:0 16px;position:relative;z-index:1}.u-button{align-items:center;background-color:#01324b;background-image:none;border:4px solid transparent;border-radius:32px;cursor:pointer;display:inline-flex;font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;font-size:.875rem;font-weight:700;justify-content:center;line-height:1.3;margin:0;padding:16px 32px;position:relative;transition:all .2s ease 0s;width:auto}.u-button svg,.u-button--contrast svg,.u-button--primary svg,.u-button--secondary svg,.u-button--tertiary svg{fill:currentcolor}.u-button,.u-button:visited{color:#fff}.u-button,.u-button:hover{box-shadow:0 0 0 1px #01324b;text-decoration:none}.u-button:hover{border:4px solid #fff}.u-button:focus{border:4px solid #fc0;box-shadow:none;outline:0;text-decoration:none}.u-button:focus,.u-button:hover{background-color:#fff;background-image:none;color:#01324b}.app-masthead--pastel .c-pdf-download .u-button--primary:focus svg path,.app-masthead--pastel .c-pdf-download .u-button--primary:hover svg path,.c-context-bar--sticky .c-context-bar__container .c-pdf-download .u-button--primary:focus svg path,.c-context-bar--sticky .c-context-bar__container .c-pdf-download .u-button--primary:hover svg path,.u-button--primary:focus svg path,.u-button--primary:hover svg path,.u-button:focus svg path,.u-button:hover svg path{fill:#01324b}.u-button--primary{background-color:#01324b;background-image:none;border:4px solid transparent;box-shadow:0 0 0 1px #01324b;color:#fff;font-weight:700}.u-button--primary:visited{color:#fff}.u-button--primary:hover{border:4px solid #fff;box-shadow:0 0 0 1px #01324b;text-decoration:none}.u-button--primary:focus{border:4px solid #fc0;box-shadow:none;outline:0;text-decoration:none}.u-button--primary:focus,.u-button--primary:hover{background-color:#fff;background-image:none;color:#01324b}.u-button--secondary{background-color:#fff;border:4px solid #fff;color:#01324b;font-weight:700}.u-button--secondary:visited{color:#01324b}.u-button--secondary:hover{border:4px solid #01324b;box-shadow:none}.u-button--secondary:focus,.u-button--secondary:hover{background-color:#01324b;color:#fff}.app-masthead--pastel .c-pdf-download .u-button--secondary:focus svg path,.app-masthead--pastel .c-pdf-download .u-button--secondary:hover svg path,.c-context-bar--sticky .c-context-bar__container .c-pdf-download .u-button--secondary:focus svg path,.c-context-bar--sticky .c-context-bar__container .c-pdf-download .u-button--secondary:hover svg path,.u-button--secondary:focus svg path,.u-button--secondary:hover svg path,.u-button--tertiary:focus svg path,.u-button--tertiary:hover svg path{fill:#fff}.u-button--tertiary{background-color:#ebf1f5;border:4px solid transparent;box-shadow:none;color:#666;font-weight:700}.u-button--tertiary:visited{color:#666}.u-button--tertiary:hover{border:4px solid #01324b;box-shadow:none}.u-button--tertiary:focus,.u-button--tertiary:hover{background-color:#01324b;color:#fff}.u-button--contrast{background-color:transparent;background-image:none;color:#fff;font-weight:400}.u-button--contrast:visited{color:#fff}.u-button--contrast,.u-button--contrast:focus,.u-button--contrast:hover{border:4px solid #fff}.u-button--contrast:focus,.u-button--contrast:hover{background-color:#fff;background-image:none;color:#000}.u-button--contrast:focus svg path,.u-button--contrast:hover svg path{fill:#000}.u-button--disabled,.u-button:disabled{background-color:transparent;background-image:none;border:4px solid #ccc;color:#000;cursor:default;font-weight:400;opacity:.7}.u-button--disabled svg,.u-button:disabled svg{fill:currentcolor}.u-button--disabled:visited,.u-button:disabled:visited{color:#000}.u-button--disabled:focus,.u-button--disabled:hover,.u-button:disabled:focus,.u-button:disabled:hover{border:4px solid #ccc;text-decoration:none}.u-button--disabled:focus,.u-button--disabled:hover,.u-button:disabled:focus,.u-button:disabled:hover{background-color:transparent;background-image:none;color:#000}.u-button--disabled:focus svg path,.u-button--disabled:hover svg path,.u-button:disabled:focus svg path,.u-button:disabled:hover svg path{fill:#000}.u-button--small,.u-button--xsmall{font-size:.875rem;padding:2px 8px}.u-button--small{padding:8px 16px}.u-button--large{font-size:1.125rem;padding:10px 35px}.u-button--full-width{display:flex;width:100%}.u-button--icon-left svg{margin-right:8px}.u-button--icon-right svg{margin-left:8px}.u-clear-both{clear:both}.u-container{margin:0 auto;max-width:1280px;padding:0 16px}.u-justify-content-space-between{justify-content:space-between}.u-display-none{display:none}.js .u-js-hide,.u-hide{display:none;visibility:hidden}.u-visually-hidden{clip:rect(0,0,0,0);border:0;clip-path:inset(50%);height:1px;overflow:hidden;padding:0;position:absolute!important;white-space:nowrap;width:1px}.u-icon{fill:currentcolor;display:inline-block;height:1em;transform:translate(0);vertical-align:text-top;width:1em}.u-list-reset{list-style:none;margin:0;padding:0}.u-ma-16{margin:16px}.u-mt-0{margin-top:0}.u-mt-24{margin-top:24px}.u-mt-32{margin-top:32px}.u-mb-8{margin-bottom:8px}.u-mb-32{margin-bottom:32px}.u-button-reset{background-color:transparent;border:0;padding:0}.u-sans-serif{font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif}.u-serif{font-family:Merriweather,serif}h1,h2,h4{-webkit-font-smoothing:antialiased}p{overflow-wrap:break-word;word-break:break-word}.u-h4{font-size:1.25rem;font-weight:700;line-height:1.2}.u-mbs-0{margin-block-start:0!important}.c-article-header{font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif}.c-article-identifiers{color:#6f6f6f;display:flex;flex-wrap:wrap;font-size:1rem;line-height:1.3;list-style:none;margin:0 0 8px;padding:0}.c-article-identifiers__item{border-right:1px solid #6f6f6f;list-style:none;margin-right:8px;padding-right:8px}.c-article-identifiers__item:last-child{border-right:0;margin-right:0;padding-right:0}@media only screen and (min-width:876px){.c-article-title{font-size:1.875rem;line-height:1.2}}.c-article-author-list{display:inline;font-size:1rem;list-style:none;margin:0 8px 0 0;padding:0;width:100%}.c-article-author-list__item{display:inline;padding-right:0}.c-article-author-list__show-more{display:none;margin-right:4px}.c-article-author-list__button,.js .c-article-author-list__item--hide,.js .c-article-author-list__show-more{display:none}.js .c-article-author-list--long .c-article-author-list__show-more,.js .c-article-author-list--long+.c-article-author-list__button{display:inline}@media only screen and (max-width:767px){.js .c-article-author-list__item--hide-small-screen{display:none}.js .c-article-author-list--short .c-article-author-list__show-more,.js .c-article-author-list--short+.c-article-author-list__button{display:inline}}#uptodate-client,.js .c-article-author-list--expanded .c-article-author-list__show-more{display:none!important}.js .c-article-author-list--expanded .c-article-author-list__item--hide-small-screen{display:inline!important}.c-article-author-list__button,.c-button-author-list{background:#ebf1f5;border:4px solid #ebf1f5;border-radius:20px;color:#666;font-size:.875rem;line-height:1.4;padding:2px 11px 2px 8px;text-decoration:none}.c-article-author-list__button svg,.c-button-author-list svg{margin:1px 4px 0 0}.c-article-author-list__button:hover,.c-button-author-list:hover{background:#025e8d;border-color:transparent;color:#fff}.c-article-body .c-article-access-provider{padding:8px 16px}.c-article-body .c-article-access-provider,.c-notes{border:1px solid #d5d5d5;border-image:initial;border-left:none;border-right:none;margin:24px 0}.c-article-body .c-article-access-provider__text{color:#555}.c-article-body .c-article-access-provider__text,.c-notes__text{font-size:1rem;margin-bottom:0;padding-bottom:2px;padding-top:2px;text-align:center}.c-article-body .c-article-author-affiliation__address{color:inherit;font-weight:700;margin:0}.c-article-body .c-article-author-affiliation__authors-list{list-style:none;margin:0;padding:0}.c-article-body .c-article-author-affiliation__authors-item{display:inline;margin-left:0}.c-article-authors-search{margin-bottom:24px;margin-top:0}.c-article-authors-search__item,.c-article-authors-search__title{font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif}.c-article-authors-search__title{color:#626262;font-size:1.05rem;font-weight:700;margin:0;padding:0}.c-article-authors-search__item{font-size:1rem}.c-article-authors-search__text{margin:0}.c-code-block{border:1px solid #fff;font-family:monospace;margin:0 0 24px;padding:20px}.c-code-block__heading{font-weight:400;margin-bottom:16px}.c-code-block__line{display:block;overflow-wrap:break-word;white-space:pre-wrap}.c-article-share-box{font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;margin-bottom:24px}.c-article-share-box__description{font-size:1rem;margin-bottom:8px}.c-article-share-box__no-sharelink-info{font-size:.813rem;font-weight:700;margin-bottom:24px;padding-top:4px}.c-article-share-box__only-read-input{border:1px solid #d5d5d5;box-sizing:content-box;display:inline-block;font-size:.875rem;font-weight:700;height:24px;margin-bottom:8px;padding:8px 10px}.c-article-share-box__additional-info{color:#626262;font-size:.813rem}.c-article-share-box__button{background:#fff;box-sizing:content-box;text-align:center}.c-article-share-box__button--link-like{background-color:transparent;border:0;color:#025e8d;cursor:pointer;font-size:.875rem;margin-bottom:8px;margin-left:10px}.c-article-associated-content__container .c-article-associated-content__collection-label{font-size:.875rem;line-height:1.4}.c-article-associated-content__container .c-article-associated-content__collection-title{line-height:1.3}.c-reading-companion{clear:both;min-height:389px}.c-reading-companion__figures-list,.c-reading-companion__references-list{list-style:none;min-height:389px;padding:0}.c-reading-companion__references-list--numeric{list-style:decimal inside}.c-reading-companion__figure-item{border-top:1px solid #d5d5d5;font-size:1rem;padding:16px 8px 16px 0}.c-reading-companion__figure-item:first-child{border-top:none;padding-top:8px}.c-reading-companion__reference-item{font-size:1rem}.c-reading-companion__reference-item:first-child{border-top:none}.c-reading-companion__reference-item a{word-break:break-word}.c-reading-companion__reference-citation{display:inline}.c-reading-companion__reference-links{font-size:.813rem;font-weight:700;list-style:none;margin:8px 0 0;padding:0;text-align:right}.c-reading-companion__reference-links>a{display:inline-block;padding-left:8px}.c-reading-companion__reference-links>a:first-child{display:inline-block;padding-left:0}.c-reading-companion__figure-title{display:block;font-size:1.25rem;font-weight:700;line-height:1.2;margin:0 0 8px}.c-reading-companion__figure-links{display:flex;justify-content:space-between;margin:8px 0 0}.c-reading-companion__figure-links>a{align-items:center;display:flex}.c-article-section__figure-caption{display:block;margin-bottom:8px;word-break:break-word}.c-article-section__figure .video,p.app-article-masthead__access--above-download{margin:0 0 16px}.c-article-section__figure-description{font-size:1rem}.c-article-section__figure-description>*{margin-bottom:0}.c-cod{display:block;font-size:1rem;width:100%}.c-cod__form{background:#ebf0f3}.c-cod__prompt{font-size:1.125rem;line-height:1.3;margin:0 0 24px}.c-cod__label{display:block;margin:0 0 4px}.c-cod__row{display:flex;margin:0 0 16px}.c-cod__row:last-child{margin:0}.c-cod__input{border:1px solid #d5d5d5;border-radius:2px;flex-shrink:0;margin:0;padding:13px}.c-cod__input--submit{background-color:#025e8d;border:1px solid #025e8d;color:#fff;flex-shrink:1;margin-left:8px;transition:background-color .2s ease-out 0s,color .2s ease-out 0s}.c-cod__input--submit-single{flex-basis:100%;flex-shrink:0;margin:0}.c-cod__input--submit:focus,.c-cod__input--submit:hover{background-color:#fff;color:#025e8d}.save-data .c-article-author-institutional-author__sub-division,.save-data .c-article-equation__number,.save-data .c-article-figure-description,.save-data .c-article-fullwidth-content,.save-data .c-article-main-column,.save-data .c-article-satellite-article-link,.save-data .c-article-satellite-subtitle,.save-data .c-article-table-container,.save-data .c-blockquote__body,.save-data .c-code-block__heading,.save-data .c-reading-companion__figure-title,.save-data .c-reading-companion__reference-citation,.save-data .c-site-messages--nature-briefing-email-variant .serif,.save-data .c-site-messages--nature-briefing-email-variant.serif,.save-data .serif,.save-data .u-serif,.save-data h1,.save-data h2,.save-data h3{font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif}.c-pdf-download__link{display:flex;flex:1 1 0%;padding:13px 24px}.c-pdf-download__link:hover{text-decoration:none}@media only screen and (min-width:768px){.c-context-bar--sticky .c-pdf-download__link{align-items:center;flex:1 1 183px}}@media only screen and (max-width:320px){.c-context-bar--sticky .c-pdf-download__link{padding:16px}}.c-article-body .c-article-recommendations-list,.c-book-body .c-article-recommendations-list{display:flex;flex-direction:row;gap:16px 16px;margin:0;max-width:100%;padding:16px 0 0}.c-article-body .c-article-recommendations-list__item,.c-book-body .c-article-recommendations-list__item{flex:1 1 0%}@media only screen and (max-width:767px){.c-article-body .c-article-recommendations-list,.c-book-body .c-article-recommendations-list{flex-direction:column}}.c-article-body .c-article-recommendations-card__authors{display:none;font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;font-size:.875rem;line-height:1.5;margin:0 0 8px}@media only screen and (max-width:767px){.c-article-body .c-article-recommendations-card__authors{display:block;margin:0}}.c-article-body .c-article-history{margin-top:24px}.app-article-metrics-bar p{margin:0}.app-article-masthead{display:flex;flex-direction:column;gap:16px 16px;padding:16px 0 24px}.app-article-masthead__info{display:flex;flex-direction:column;flex-grow:1}.app-article-masthead__brand{border-top:1px solid hsla(0,0%,100%,.8);display:flex;flex-direction:column;flex-shrink:0;gap:8px 8px;min-height:96px;padding:16px 0 0}.app-article-masthead__brand img{border:1px solid #fff;border-radius:8px;box-shadow:0 4px 15px 0 hsla(0,0%,50%,.25);height:auto;left:0;position:absolute;width:72px}.app-article-masthead__journal-link{display:block;font-size:1.125rem;font-weight:700;margin:0 0 8px;max-width:400px;padding:0 0 0 88px;position:relative}.app-article-masthead__journal-title{-webkit-box-orient:vertical;-webkit-line-clamp:3;display:-webkit-box;overflow:hidden}.app-article-masthead__submission-link{align-items:center;display:flex;font-size:1rem;gap:4px 4px;margin:0 0 0 88px}.app-article-masthead__access{align-items:center;display:flex;flex-wrap:wrap;font-size:.875rem;font-weight:300;gap:4px 4px;margin:0}.app-article-masthead__buttons{display:flex;flex-flow:column wrap;gap:16px 16px}.app-article-masthead__access svg,.app-masthead--pastel .c-pdf-download .u-button--primary svg,.app-masthead--pastel .c-pdf-download .u-button--secondary svg,.c-context-bar--sticky .c-context-bar__container .c-pdf-download .u-button--primary svg,.c-context-bar--sticky .c-context-bar__container .c-pdf-download .u-button--secondary svg{fill:currentcolor}.app-article-masthead a{color:#fff}.app-masthead--pastel .c-pdf-download .u-button--primary,.c-context-bar--sticky .c-context-bar__container .c-pdf-download .u-button--primary{background-color:#025e8d;background-image:none;border:2px solid transparent;box-shadow:none;color:#fff;font-weight:700}.app-masthead--pastel .c-pdf-download .u-button--primary:visited,.c-context-bar--sticky .c-context-bar__container .c-pdf-download .u-button--primary:visited{color:#fff}.app-masthead--pastel .c-pdf-download .u-button--primary:hover,.c-context-bar--sticky .c-context-bar__container .c-pdf-download .u-button--primary:hover{text-decoration:none}.app-masthead--pastel .c-pdf-download .u-button--primary:focus,.c-context-bar--sticky .c-context-bar__container .c-pdf-download .u-button--primary:focus{border:4px solid #fc0;box-shadow:none;outline:0;text-decoration:none}.app-masthead--pastel .c-pdf-download .u-button--primary:focus,.app-masthead--pastel .c-pdf-download .u-button--primary:hover,.c-context-bar--sticky .c-context-bar__container .c-pdf-download .u-button--primary:focus,.c-context-bar--sticky .c-context-bar__container .c-pdf-download .u-button--primary:hover{background-color:#fff;background-image:none;color:#01324b}.app-masthead--pastel .c-pdf-download .u-button--primary:hover,.c-context-bar--sticky .c-context-bar__container .c-pdf-download .u-button--primary:hover{background:0 0;border:2px solid #025e8d;box-shadow:none;color:#025e8d}.app-masthead--pastel .c-pdf-download .u-button--secondary,.c-context-bar--sticky .c-context-bar__container .c-pdf-download .u-button--secondary{background:0 0;border:2px solid #025e8d;color:#025e8d;font-weight:700}.app-masthead--pastel .c-pdf-download .u-button--secondary:visited,.c-context-bar--sticky .c-context-bar__container .c-pdf-download .u-button--secondary:visited{color:#01324b}.app-masthead--pastel .c-pdf-download .u-button--secondary:hover,.c-context-bar--sticky .c-context-bar__container .c-pdf-download .u-button--secondary:hover{background-color:#01324b;background-color:#025e8d;border:2px solid transparent;box-shadow:none;color:#fff}.app-masthead--pastel .c-pdf-download .u-button--secondary:focus,.c-context-bar--sticky .c-context-bar__container .c-pdf-download .u-button--secondary:focus{background-color:#fff;background-image:none;border:4px solid #fc0;color:#01324b}@media only screen and (min-width:768px){.app-article-masthead{flex-direction:row;gap:64px 64px;padding:24px 0}.app-article-masthead__brand{border:0;padding:0}.app-article-masthead__brand img{height:auto;position:static;width:auto}.app-article-masthead__buttons{align-items:center;flex-direction:row;margin-top:auto}.app-article-masthead__journal-link{display:flex;flex-direction:column;gap:24px 24px;margin:0 0 8px;padding:0}.app-article-masthead__submission-link{margin:0}}@media only screen and (min-width:1024px){.app-article-masthead__brand{flex-basis:400px}}.app-article-masthead .c-article-identifiers{font-size:.875rem;font-weight:300;line-height:1;margin:0 0 8px;overflow:hidden;padding:0}.app-article-masthead .c-article-identifiers--cite-list{margin:0 0 16px}.app-article-masthead .c-article-identifiers *{color:#fff}.app-article-masthead .c-cod{display:none}.app-article-masthead .c-article-identifiers__item{border-left:1px solid #fff;border-right:0;margin:0 17px 8px -9px;padding:0 0 0 8px}.app-article-masthead .c-article-identifiers__item--cite{border-left:0}.app-article-metrics-bar{display:flex;flex-wrap:wrap;font-size:1rem;padding:16px 0 0;row-gap:24px}.app-article-metrics-bar__item{padding:0 16px 0 0}.app-article-metrics-bar__count{font-weight:700}.app-article-metrics-bar__label{font-weight:400;padding-left:4px}.app-article-metrics-bar__icon{height:auto;margin-right:4px;margin-top:-4px;width:auto}.app-article-metrics-bar__arrow-icon{margin:4px 0 0 4px}.app-article-metrics-bar a{color:#000}.app-article-metrics-bar .app-article-metrics-bar__item--metrics{padding-right:0}.app-overview-section .c-article-author-list,.app-overview-section__authors{line-height:2}.app-article-metrics-bar{margin-top:8px}.c-book-toc-pagination+.c-book-section__back-to-top{margin-top:0}.c-article-body .c-article-access-provider__text--chapter{color:#222;font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;padding:20px 0}.c-article-body .c-article-access-provider__text--chapter svg.c-status-message__icon{fill:#003f8d;vertical-align:middle}.c-article-body-section__content--separator{padding-top:40px}.c-pdf-download__link{max-height:44px}.app-article-access .u-button--primary,.app-article-access .u-button--primary:visited{color:#fff}.c-article-sidebar{display:none}@media only screen and (min-width:1024px){.c-article-sidebar{display:block}}.c-cod__form{border-radius:12px}.c-cod__label{font-size:.875rem}.c-cod .c-status-message{align-items:center;justify-content:center;margin-bottom:16px;padding-bottom:16px}@media only screen and (min-width:1024px){.c-cod .c-status-message{align-items:inherit}}.c-cod .c-status-message__icon{margin-top:4px}.c-cod .c-cod__prompt{font-size:1rem;margin-bottom:16px}.c-article-body .app-article-access,.c-book-body .app-article-access{display:block}@media only screen and (min-width:1024px){.c-article-body .app-article-access,.c-book-body .app-article-access{display:none}}.c-article-body .app-card-service{margin-bottom:32px}@media only screen and (min-width:1024px){.c-article-body .app-card-service{display:none}}.app-article-access .buybox__buy .u-button--secondary,.app-article-access .u-button--primary,.c-cod__row .u-button--primary{background-color:#025e8d;border:2px solid #025e8d;box-shadow:none;font-size:1rem;font-weight:700;gap:8px 8px;justify-content:center;line-height:1.5;padding:8px 24px}.app-article-access .buybox__buy .u-button--secondary,.app-article-access .u-button--primary:hover,.c-cod__row .u-button--primary:hover{background-color:#fff;color:#025e8d}.app-article-access .buybox__buy .u-button--secondary:hover{background-color:#025e8d;color:#fff}.buybox__buy .c-notes__text{color:#666;font-size:.875rem;padding:0 16px 8px}.c-cod__input{flex-basis:auto;width:100%}.c-article-title{font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;font-size:2.25rem;font-weight:700;line-height:1.2;margin:12px 0}.c-reading-companion__figure-item figure{margin:0}@media only screen and (min-width:768px){.c-article-title{margin:16px 0}}.app-article-access{border:1px solid #c5e0f4;border-radius:12px}.app-article-access__heading{border-bottom:1px solid #c5e0f4;font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;font-size:1.125rem;font-weight:700;margin:0;padding:16px;text-align:center}.app-article-access .buybox__info svg{vertical-align:middle}.c-article-body .app-article-access p{margin-bottom:0}.app-article-access .buybox__info{font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif;font-size:1rem;margin:0}.app-article-access{margin:0 0 32px}@media only screen and (min-width:1024px){.app-article-access{margin:0 0 24px}}.c-status-message{font-size:1rem}.c-article-body{font-size:1.125rem}.c-article-body dl,.c-article-body ol,.c-article-body p,.c-article-body ul{margin-bottom:32px;margin-top:0}.c-article-access-provider__text:last-of-type,.c-article-body .c-notes__text:last-of-type{margin-bottom:0}.c-article-body ol p,.c-article-body ul p{margin-bottom:16px}.c-article-section__figure-caption{font-family:Merriweather Sans,Helvetica Neue,Helvetica,Arial,sans-serif}.c-reading-companion__figure-item{border-top-color:#c5e0f4}.c-reading-companion__sticky{max-width:400px}.c-article-section .c-article-section__figure-description>*{font-size:1rem;margin-bottom:16px}.c-reading-companion__reference-item{border-top:1px solid #d5d5d5;padding:16px 0}.c-reading-companion__reference-item:first-child{padding-top:0}.c-article-share-box__button,.js .c-article-authors-search__item .c-article-button{background:0 0;border:2px solid #025e8d;border-radius:32px;box-shadow:none;color:#025e8d;font-size:1rem;font-weight:700;line-height:1.5;margin:0;padding:8px 24px;transition:all .2s ease 0s}.c-article-authors-search__item .c-article-button{width:100%}.c-pdf-download .u-button{background-color:#fff;border:2px solid #fff;color:#01324b;justify-content:center}.c-context-bar__container .c-pdf-download .u-button svg,.c-pdf-download .u-button svg{fill:currentcolor}.c-pdf-download .u-button:visited{color:#01324b}.c-pdf-download .u-button:hover{border:4px solid #01324b;box-shadow:none}.c-pdf-download .u-button:focus,.c-pdf-download .u-button:hover{background-color:#01324b}.c-pdf-download .u-button:focus svg path,.c-pdf-download .u-button:hover svg path{fill:#fff}.c-context-bar__container .c-pdf-download .u-button{background-image:none;border:2px solid;color:#fff}.c-context-bar__container .c-pdf-download .u-button:visited{color:#fff}.c-context-bar__container .c-pdf-download .u-button:hover{text-decoration:none}.c-context-bar__container .c-pdf-download .u-button:focus{box-shadow:none;outline:0;text-decoration:none}.c-context-bar__container .c-pdf-download .u-button:focus,.c-context-bar__container .c-pdf-download .u-button:hover{background-color:#fff;background-image:none;color:#01324b}.c-context-bar__container .c-pdf-download .u-button:focus svg path,.c-context-bar__container .c-pdf-download .u-button:hover svg path{fill:#01324b}.c-context-bar__container .c-pdf-download .u-button,.c-pdf-download .u-button{box-shadow:none;font-size:1rem;font-weight:700;line-height:1.5;padding:8px 24px}.c-context-bar__container .c-pdf-download .u-button{background-color:#025e8d}.c-pdf-download .u-button:hover{border:2px solid #fff}.c-pdf-download .u-button:focus,.c-pdf-download .u-button:hover{background:0 0;box-shadow:none;color:#fff}.c-context-bar__container .c-pdf-download .u-button:hover{border:2px solid #025e8d;box-shadow:none;color:#025e8d}.c-context-bar__container .c-pdf-download .u-button:focus,.c-pdf-download .u-button:focus{border:2px solid #025e8d}.c-article-share-box__button:focus:focus,.c-article__pill-button:focus:focus,.c-context-bar__container .c-pdf-download .u-button:focus:focus,.c-pdf-download .u-button:focus:focus{outline:3px solid #08c;will-change:transform}.c-pdf-download__link .u-icon{padding-top:0}.c-bibliographic-information__column button{margin-bottom:16px}.c-article-body .c-article-author-affiliation__list p,.c-article-body .c-article-author-information__list p,figure{margin:0}.c-article-share-box__button{margin-right:16px}.c-status-message--boxed{border-radius:12px}.c-article-associated-content__collection-title{font-size:1rem}.app-card-service__description,.c-article-body .app-card-service__description{color:#222;margin-bottom:0;margin-top:8px}.app-article-access__subscriptions a,.app-article-access__subscriptions a:visited,.app-book-series-listing__item a,.app-book-series-listing__item a:hover,.app-book-series-listing__item a:visited,.c-article-author-list a,.c-article-author-list a:visited,.c-article-buy-box a,.c-article-buy-box a:visited,.c-article-peer-review a,.c-article-peer-review a:visited,.c-article-satellite-subtitle a,.c-article-satellite-subtitle a:visited,.c-breadcrumbs__link,.c-breadcrumbs__link:hover,.c-breadcrumbs__link:visited{color:#000}.c-article-author-list svg{height:24px;margin:0 0 0 6px;width:24px}.c-article-header{margin-bottom:32px}@media only screen and (min-width:876px){.js .c-ad--conditional{display:block}}.u-lazy-ad-wrapper{background-color:#fff;display:none;min-height:149px}@media only screen and (min-width:876px){.u-lazy-ad-wrapper{display:block}}p.c-ad__label{margin-bottom:4px}.c-ad--728x90{background-color:#fff;border-bottom:2px solid #cedbe0} } </style>
    <style>@media only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark) {  .eds-c-header__brand img{height:24px;width:203px}.app-article-masthead__journal-link img{height:93px;width:72px}@media only screen and (min-width:769px){.app-article-masthead__journal-link img{height:161px;width:122px}} } </style>



        
        <link rel="stylesheet" data-test="critical-css-handler" data-inline-css-source="critical-css" href="/oscar-static/app-springerlink/css/core-darwin-9fe647df8f.css" media="all" onload="this.media='all';this.onload=null">
        <link rel="stylesheet" data-test="critical-css-handler" data-inline-css-source="critical-css" href="/oscar-static/app-springerlink/css/enhanced-darwin-article-868332661c.css" media="only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark)" onload="this.media='only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark)';this.onload=null">
    

        
        
    <script async="" src="//cdn.jsdelivr.net/npm/mathjax@2.7.5/MathJax.js?config=TeX-AMS-MML_SVG.js"></script><script type="text/javascript" async="" src="https://pagead2.googlesyndication.com/tag/js/gpt.js"></script><script async="" src="https://www.googletagmanager.com/gtm.js?id=GTM-MRVXSHQ"></script><script type="text/javascript">
        config = {
            env: 'live',
            site: 'bmcmedimaging.biomedcentral.com',
            siteWithPath: 'bmcmedimaging.biomedcentral.com' + window.location.pathname,
            twitterHashtag: '',
            cmsPrefix: 'https://studio-cms.springernature.com/studio/',
            
            
            figshareScriptUrl: 'https://widgets.figshare.com/static/figshare.js',
            hasFigshareInvoked: false,
            publisherBrand: 'BioMed Central',
            mustardcut: false
        };
    </script><script src="https://cmp.springer.com/production_live/en/consent-bundle-17-55.js" onload="initGTM(window,document,'script','dataLayer','GTM-MRVXSHQ')"></script>

        
                




    <script>
        window.dataLayer = [{"GA Key":"UA-26408784-1","DOI":"10.1186/s12880-022-00818-1","Page":"article","springerJournal":false,"Publishing Model":"Open Access","Country":"IN","japan":false,"doi":"10.1186-s12880-022-00818-1","Journal Id":12880,"Journal Title":"BMC Medical Imaging","imprint":"BioMed Central","Keywords":"White blood cell, Classification, Medical image, CNN, Deep learning","kwrd":["White_blood_cell","Classification","Medical_image","CNN","Deep_learning"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":[],"Open Access":"Y","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":[],"businessPartnerIDString":""}},"Access Type":"open","Bpids":"","Bpnames":"","BPID":["1"],"VG Wort Identifier":"vgzm.415900-10.1186-s12880-022-00818-1","Full HTML":"Y","Subject Codes":["SCH","SCH29005"],"pmc":["H","H29005"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1471-2342"},"type":"Article","category":{"pmc":{"primarySubject":"Medicine \u0026 Public Health","primarySubjectCode":"H","secondarySubjects":{"1":"Imaging / Radiology"},"secondarySubjectCodes":{"1":"H29005"}},"sucode":"SC11","articleType":"Research"},"attributes":{"deliveryPlatform":"oscar"}},"page":{"attributes":{"environment":"live"},"category":{"pageType":"article"}},"Event Category":"Article"}];
    </script>











    <script data-test="springer-link-article-datalayer">
        window.dataLayer = window.dataLayer || [];
        window.dataLayer.push({
            ga4MeasurementId: 'G-B3E4QL2TPR',
            ga360TrackingId: 'UA-26408784-1',
            twitterId: 'o47a7',
            baiduId: 'aef3043f025ccf2305af8a194652d70b',
            ga4ServerUrl: 'https://collect.springer.com',
            imprint: 'springerlink',
                page: {
                    attributes:{
                        featureFlags: [{ name: 'darwin-orion', active: true }],
                        darwinAvailable: true
                    }
                }
            
        });
    </script>



        
        <script>
    (function(w, d) {
        w.config = w.config || {};
        w.config.mustardcut = false;

        
        if (w.matchMedia && w.matchMedia('only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark)').matches) {
            w.config.mustardcut = true;
            d.classList.add('js');
            d.classList.remove('grade-c');
            d.classList.remove('no-js');
        }
    })(window, document.documentElement);
</script>


        <script class="js-entry">
    if (window.config.mustardcut) {
        (function(w, d) {
            
            
            
                window.Component = {};
                window.suppressShareButton = false;
                window.onArticlePage = true;
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                {'src': '/oscar-static/js/polyfill-es5-bundle-572d4fec60.js', 'async': false}
            ];

            var bodyScripts = [
                
                    {'src': '/oscar-static/js/global-article-es5-bundle-237659debf.js', 'async': false, 'module': false},
                    {'src': '/oscar-static/js/global-article-es6-bundle-2c19ea9e42.js', 'async': false, 'module': true}
                
                
                    
                
                
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i = 0; i < headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i = 0; i < bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        })(window, document);
    }
</script><script src="/oscar-static/js/polyfill-es5-bundle-572d4fec60.js"></script>


<script data-src="https://cdn.optimizely.com/js/27195530232.js" data-cc-script="C03"></script>



        
            
            
                
<script data-test="gtm-head">
    window.initGTM = function() {
        if (window.config.mustardcut) {
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                    j = d.createElement(s),
                    dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-MRVXSHQ');
        }
    }
</script>

            
            
            
        

        <script>
(function (w, d, t) {
    function cc() {
        var h = w.location.hostname;
        var e = d.createElement(t),
        s = d.getElementsByTagName(t)[0];

        
        if (h.indexOf('springer.com') > -1 && h.indexOf('biomedcentral.com') === -1 && h.indexOf('springeropen.com') === -1) {
            if (h.indexOf('link-qa.springer.com') > -1 || h.indexOf('test-www.springer.com') > -1) {
                e.src = 'https://cmp.springer.com/production_live/en/consent-bundle-17-55.js';
                e.setAttribute('onload', "initGTM(window,document,'script','dataLayer','GTM-MRVXSHQ')");
            } else {
                e.src = 'https://cmp.springer.com/production_live/en/consent-bundle-17-55.js';
                e.setAttribute('onload', "initGTM(window,document,'script','dataLayer','GTM-MRVXSHQ')");
            }
        } else if (h.indexOf('biomedcentral.com') > -1) {
            if (h.indexOf('biomedcentral.com.qa') > -1) {
                e.src = 'https://cmp.biomedcentral.com/production_live/en/consent-bundle-15-42.js';
                e.setAttribute('onload', "initGTM(window,document,'script','dataLayer','GTM-MRVXSHQ')");
            } else {
                e.src = 'https://cmp.biomedcentral.com/production_live/en/consent-bundle-15-42.js';
                e.setAttribute('onload', "initGTM(window,document,'script','dataLayer','GTM-MRVXSHQ')");
            }
        } else if (h.indexOf('springeropen.com') > -1) {
            if (h.indexOf('springeropen.com.qa') > -1) {
                e.src = 'https://cmp.springernature.com/production_live/en/consent-bundle-16-37.js';
                e.setAttribute('onload', "initGTM(window,document,'script','dataLayer','GTM-MRVXSHQ')");
            } else {
                e.src = 'https://cmp.springernature.com/production_live/en/consent-bundle-16-37.js';
                e.setAttribute('onload', "initGTM(window,document,'script','dataLayer','GTM-MRVXSHQ')");
            }
        } else if (h.indexOf('springernature.com') > -1) {
            if (h.indexOf('beta-qa.springernature.com') > -1) {
                e.src = 'https://cmp.springernature.com/production_live/en/consent-bundle-49-43.js';
                e.setAttribute('onload', "initGTM(window,document,'script','dataLayer','GTM-NK22KLS')");
            } else {
                e.src = 'https://cmp.springernature.com/production_live/en/consent-bundle-49-43.js';
                e.setAttribute('onload', "initGTM(window,document,'script','dataLayer','GTM-NK22KLS')");
            }
        } else {
            e.src = '/oscar-static/js/cookie-consent-es5-bundle-cb57c2c98a.js';
            e.setAttribute('data-consent', h);
        }
        s.insertAdjacentElement('afterend', e);
    }

    cc();
})(window, document, 'script');
</script>


        
        
        
    
        
    


        
    
    <link rel="canonical" href="https://bmcmedimaging.biomedcentral.com/articles/10.1186/s12880-022-00818-1">
    

        
        
        
        

        
    <script type="application/ld+json">{"mainEntity":{"headline":"WBC image classification and generative models based on convolutional neural network","description":"Computer-aided methods for analyzing white blood cells (WBC) are popular due to the complexity of the manual alternatives. Recent works have shown highly accurate segmentation and detection of white blood cells from microscopic blood images. However, the classification of the observed cells is still a challenge, in part due to the distribution of the five types that affect the condition of the immune system. (i) This work proposes W-Net, a CNN-based method for WBC classification. We evaluate W-Net on a real-world large-scale dataset that includes 6562 real images of the five WBC types. (ii) For further benefits, we generate synthetic WBC images using Generative Adversarial Network to be used for education and research purposes through sharing. (i) W-Net achieves an average accuracy of 97%. In comparison to state-of-the-art methods in the field of WBC classification, we show that W-Net outperforms other CNN- and RNN-based model architectures. Moreover, we show the benefits of using pre-trained W-Net in a transfer learning context when fine-tuned to specific task or accommodating another dataset. (ii) The synthetic WBC images are confirmed by experiments and a domain expert to have a high degree of similarity to the original images. The pre-trained W-Net and the generated WBC dataset are available for the community to facilitate reproducibility and follow up research work. This work proposed W-Net, a CNN-based architecture with a small number of layers, to accurately classify the five WBC types. We evaluated W-Net on a real-world large-scale dataset and addressed several challenges such as the transfer learning property and the class imbalance. W-Net achieved an average classification accuracy of 97%. We synthesized a dataset of new WBC image samples using DCGAN, which we released to the public for education and research purposes.","datePublished":"2022-05-20T00:00:00Z","dateModified":"2022-05-20T00:00:00Z","pageStart":"1","pageEnd":"16","license":"http://creativecommons.org/publicdomain/zero/1.0/","sameAs":"https://doi.org/10.1186/s12880-022-00818-1","keywords":["White blood cell","Classification","Medical image","CNN","Deep learning","Imaging / Radiology"],"image":["https://media.springernature.com/lw1200/springer-static/image/art%3A10.1186%2Fs12880-022-00818-1/MediaObjects/12880_2022_818_Fig1_HTML.png","https://media.springernature.com/lw1200/springer-static/image/art%3A10.1186%2Fs12880-022-00818-1/MediaObjects/12880_2022_818_Fig2_HTML.png","https://media.springernature.com/lw1200/springer-static/image/art%3A10.1186%2Fs12880-022-00818-1/MediaObjects/12880_2022_818_Fig3_HTML.png","https://media.springernature.com/lw1200/springer-static/image/art%3A10.1186%2Fs12880-022-00818-1/MediaObjects/12880_2022_818_Fig4_HTML.png"],"isPartOf":{"name":"BMC Medical Imaging","issn":["1471-2342"],"volumeNumber":"22","@type":["Periodical","PublicationVolume"]},"publisher":{"name":"BioMed Central","logo":{"url":"https://www.springernature.com/app-sn/public/images/logo-springernature.png","@type":"ImageObject"},"@type":"Organization"},"author":[{"name":"Changhun Jung","affiliation":[{"name":"Ewha Womans University","address":{"name":"Department of Cyber Security, Ewha Womans University, Seoul, Republic of Korea","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Mohammed Abuhamad","affiliation":[{"name":"Loyola University Chicago","address":{"name":"Department of Computer Science, Loyola University Chicago, Chicago, USA","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"David Mohaisen","affiliation":[{"name":"University of Central Florida","address":{"name":"Department of Computer Science, University of Central Florida, Orlando, USA","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Kyungja Han","affiliation":[{"name":"The Catholic University of Korea Seoul St. Mary’s Hospital","address":{"name":"Department of Laboratory Medicine and College of Medicine, The Catholic University of Korea Seoul St. Mary’s Hospital, Seoul, Republic of Korea","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"DaeHun Nyang","affiliation":[{"name":"Ewha Womans University","address":{"name":"Department of Cyber Security, Ewha Womans University, Seoul, Republic of Korea","@type":"PostalAddress"},"@type":"Organization"}],"email":"nyang@ewha.ac.kr","@type":"Person"}],"isAccessibleForFree":true,"@type":"ScholarlyArticle"},"@context":"https://schema.org","@type":"WebPage"}</script>

        


        
    <style type="text/css">.cc-banner{background-color:#01324b;border:none!important;bottom:0;box-sizing:border-box;color:#fff!important;left:0;line-height:1.3;margin:auto 0 0;max-width:100%;outline:0;overflow:visible;padding:0;position:fixed;right:0;width:100%;z-index:99999}.cc-banner::backdrop{background-color:#0000004d}.cc-banner *{color:inherit!important}.cc-banner:focus{box-shadow:none;outline:0}.cc-banner a{color:#fff!important;text-decoration:underline}.cc-banner a:active,.cc-banner a:focus,.cc-banner a:hover{color:inherit;text-decoration:none}.cc-banner a:focus{outline:3px solid #08c!important}.cc-banner h2,.cc-banner h3,.cc-banner h4,.cc-banner h5,.cc-banner h6{font-family:sans-serif;font-style:normal;font-weight:700;margin:0 0 .5em}.cc-banner .cc-h2,.cc-banner h2{font-size:18px}.cc-banner .cc-h3,.cc-banner h3{font-size:16px}.cc-banner .cc-h4,.cc-banner .cc-h5,.cc-banner .cc-h6,.cc-banner h4,.cc-banner h5,.cc-banner h6{font-size:14px}.cc-banner .cc-button{font-size:16px}.cc-banner__content{background-color:#01324b;display:flex;flex-direction:column;margin:0 auto;max-height:90vh;max-width:100%;padding:16px;position:relative}.cc-banner__content:focus{outline:0}@media (min-width:680px){.cc-banner__content{padding:12px}}@media (min-width:980px){.cc-banner__content{max-height:60vh;padding-bottom:20px;padding-top:20px}}@media (min-width:1320px){.cc-banner__content{padding-bottom:40px;padding-top:40px}}.cc-banner__container{display:flex;flex-direction:column;margin:auto;max-width:1320px;overflow:auto}.cc-banner__title{background:none!important;border:0;flex-shrink:0;font-size:18px!important;font-size:22px!important;font-weight:700!important;letter-spacing:normal;margin:0 0 12px!important;padding:0}@media (min-width:680px){.cc-banner__title{font-size:24px!important;margin:0 0 16px!important}}@media (min-width:1320px){.cc-banner__title{font-size:26px!important;margin:0 0 24px!important}}.cc-banner__body{display:flex;flex-direction:column;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif!important;overflow-x:hidden;overflow-y:auto;padding:3px 3px 16px}@media (min-width:980px){.cc-banner__body{flex-direction:row}}.cc-banner__policy p{font-size:16px!important;margin:0;max-width:none;padding:0}.cc-banner__policy p:not(:last-child){margin:0 0 16px}@media (min-width:980px){.cc-banner__policy p:not(:last-child){margin:0 0 24px}}.cc-banner__policy p a{font-size:16px!important;font-weight:700}.cc-banner__footer{box-shadow:none;flex-shrink:0;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif !important!important;margin:0;padding:12px 0 3px 3px;position:relative}@media (min-width:380px){.cc-banner__footer{align-items:stretch;display:flex;flex-wrap:wrap}}@media (min-width:680px){.cc-banner__footer{box-shadow:none;flex-wrap:nowrap;width:40%}}.cc-banner__footer .cc-banner__button{flex:1 1 auto;font-weight:700;overflow:hidden;padding:.5em 1em;width:100%}@media (min-width:680px){.cc-banner__footer .cc-banner__button{max-width:275px}}.cc-banner__footer .cc-banner__button:not(:last-child){margin-bottom:12px}@media (min-width:680px){.cc-banner__footer .cc-banner__button:not(:last-child){margin-bottom:0;margin-right:16px}}@media (min-width:980px){.cc-banner__footer .cc-banner__button:not(:last-child){margin-right:24px}}.cc-banner__button-preferences{padding-left:0;padding-right:0}@media (min-width:380px){.cc-banner__button-preferences{flex:0 0 auto;margin:auto}}@media (min-width:680px){.cc-banner__button-preferences{margin:0}}@media (min-width:380px) and (max-width:680px){.cc-banner__button-break{display:block}}@media (min-width:680px){.cc-banner--is-tcf .cc-banner__footer{width:auto}}@media (min-width:980px){.cc-banner--is-tcf .cc-banner__footer{padding-right:48px;width:66%}}.cc-banner--is-tcf .cc-banner__stacks-intro{font-size:16px!important}.cc-banner--is-tcf .cc-banner__stacks{margin:16px 0 0}@media (min-width:980px){.cc-banner--is-tcf .cc-banner__stacks{display:flex;flex:0 0 33%;flex-direction:column;margin:0 0 0 48px;overflow:auto}}.cc-banner--is-tcf .cc-banner__stacks-intro{font-size:16px;margin:0 0 12px}.cc-banner--is-tcf .cc-banner__stacks-details{padding:0 3px}.cc-banner--is-tcf .cc-banner .cc-details{border-color:#fff3!important}.cc-box{border-radius:10px;padding:12px}.cc-box--info{background-color:#eff6fb}.cc-box--light{background-color:#faf9f6}.cc-button{border:2px solid #0000;border-radius:32px!important;color:inherit;cursor:pointer;font-size:14px;font-weight:700!important;left:auto;line-height:1.2;margin:0;padding:.5em 1em;right:auto;text-transform:none!important;transition:all .2s}@media (min-width:680px){.cc-button{font-size:14px;padding:.75em 1em}}.cc-button--sm{font-size:12px;padding:2px 8px!important}.cc-button--primary{background-color:#025e8d!important;border-color:#025e8d!important;color:#fff!important}.cc-button--primary:focus{outline:3px solid #08c}.cc-button--primary:focus,.cc-button--primary:hover,.cc-button--secondary{background-color:#fff!important;border-color:#025e8d!important;color:#025e8d!important}.cc-button--secondary:focus{background-color:#025e8d!important;border-color:#fff!important;color:#fff!important;outline:3px solid #08c!important}.cc-button--secondary:hover{background-color:#025e8d!important;border-color:#025e8d!important;color:#fff!important}.cc-button--secondary:active{border:2px solid #01324b!important}.cc-button--secondary.cc-button--contrast{border-color:#fff!important}.cc-button--secondary.cc-button--contrast:hover{background-color:initial!important;color:#fff!important}.cc-button--tertiary{background-color:#f0f7fc!important;background-image:none;border:2px solid #f0f7fc;color:#025e8d!important;text-decoration:underline!important;text-underline-offset:.25em}.cc-button--tertiary:focus{outline:3px solid #08c!important}.cc-button--tertiary:hover{text-decoration-thickness:.25em}.cc-button--tertiary:active{color:#013c5b}.cc-button--link{background-color:initial!important;border-color:#0000!important;border-radius:0!important;color:inherit!important;padding:0!important;text-decoration:underline!important;width:inherit!important}.cc-button--link:focus{outline:3px solid #08c}.cc-button--link:hover{background-color:initial!important;box-shadow:none;text-decoration:none}.cc-button--text{border-radius:0;padding:0}.cc-button--details{padding-right:24px!important;position:relative;width:auto}.cc-button--details:after,.cc-button--details:before{background-color:currentColor;content:"";position:absolute;transition:transform .25s ease-out}.cc-button--details:before{height:10px;margin-top:-5px;right:12px;top:50%;width:2px}.cc-button--details:after{height:2px;margin-top:-1px;right:8px;top:50%;width:10px}.cc-button--details.cc-active:before{transform:rotate(90deg);transform-origin:center}.cc-button--details.cc-active:after{display:none}.cc-details{border-bottom:1px solid #0000001a;padding:12px 0;position:relative;width:100%}.cc-details__summary{align-items:center;display:flex;font-size:14px;font-weight:700;list-style-type:none}.cc-details__summary:focus,.cc-details__summary:focus-visible{outline:3px solid #08c!important;will-change:auto}.cc-details__title{align-items:baseline;display:flex}.cc-details__title h2,.cc-details__title h3,.cc-details__title h4,.cc-details__title h5{line-height:1.4;margin:0!important;padding:0!important}.cc-details__title svg{flex-shrink:0;height:auto;margin-right:8px;position:relative;top:-1px;transition:all .2s;width:auto}.cc-details[open] .cc-details__title svg{top:2px;transform:rotate(90deg)}.cc-details__switch{margin-left:auto}.cc-details__section{padding:16px 0 0 18px}.cc-details__section p{margin:0}.cc-details__section p:not(:last-child){margin-bottom:12px}details summary::-webkit-details-marker{display:none}.cc-radio{align-items:center;display:flex;position:relative}.cc-radio *{cursor:pointer}.cc-radio__input{height:22px;left:0;position:absolute;top:0;width:22px}.cc-radio__input:focus{outline:none}.cc-radio__label{color:inherit;font-size:14px;font-weight:700;line-height:23px;margin:0;padding-left:28px}.cc-radio__label:after,.cc-radio__label:before{background-color:#fff;content:"";display:block;position:absolute;transition:transform .25s ease-out}.cc-radio__label:before{border:1px solid #777;border-radius:50%;height:22px;left:0;top:0;width:22px}.cc-radio__label:after{border:7px solid #025e8d;border-radius:50%;height:0;left:4px;opacity:0;top:4px;width:0}.cc-radio__input:focus+.cc-radio__label:before{box-shadow:0 0 0 2px #08c;outline:none}.cc-radio__label--hidden{display:none}.cc-radio__input:checked+.cc-radio__label:after{opacity:1}.cc-radio__input:disabled{cursor:default}.cc-radio__input:disabled+.cc-radio__label{cursor:default;opacity:.5}.cc-switch *{cursor:pointer}.cc-switch{align-items:center;display:flex;line-height:1}.cc-switch__label{cursor:pointer;display:inline-block;font-size:14px;font-weight:700;margin:0;-webkit-user-select:none;user-select:none}.cc-switch__input[type=checkbox]{-webkit-appearance:none;appearance:none;background-color:#fff!important;background-image:url("data:image/svg+xml;charset=utf-8,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='-4 -4 8 8'%3E%3Ccircle r='3' fill='rgba(0, 0, 0, 0.25)'/%3E%3C/svg%3E")!important;background-position:0!important;background-repeat:no-repeat!important;border:1px solid #777!important;border-radius:16px!important;height:18px!important;margin:0 8px 0 0!important;transition:background-position .2s ease-in-out!important;width:32px!important}.cc-switch__input[type=checkbox]:focus{background-image:url("data:image/svg+xml;charset=utf-8,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='-4 -4 8 8'%3E%3Ccircle r='3' fill='%2380b3cc'/%3E%3C/svg%3E")!important}.cc-switch__input[type=checkbox]:checked{background-color:#025e8d!important;background-image:url("data:image/svg+xml;charset=utf-8,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='-4 -4 8 8'%3E%3Ccircle r='3' fill='%23fff'/%3E%3C/svg%3E")!important;background-position:100%!important}.cc-switch__input[type=checkbox]:checked~.cc-switch__label>.cc-switch__text-off,.cc-switch__input[type=checkbox]:not(:checked)~.cc-switch__label>.cc-switch__text-on{display:none}.cc-switch__input[type=checkbox]:hover{cursor:pointer;outline:none}.cc-switch__input[type=checkbox]:focus-visible{cursor:pointer;outline:3px solid #fc0}.cc-switch__input[type=checkbox]:checked:before{background-color:none!important;color:#0000!important}.cc-list>ul,ul.cc-list{list-style-type:disc;margin-left:0;padding-left:0}.cc-list>ul>li,ul.cc-list>li{margin-left:1em}.cc-list>ul>li:not(:last-child),ul.cc-list>li:not(:last-child){margin-bottom:4px}.cc-list__title{margin-bottom:8px!important}.cc-list--inline>ul,ul.cc-list--inline{display:flex;flex-wrap:wrap;list-style-type:none}.cc-list--inline>ul>li,ul.cc-list--inline>li{margin:0 .5em .5em 0}.cc-overlay{background-color:#0000004d;z-index:99998}.cc-overlay,.cc-preferences{bottom:0;left:0;position:fixed;right:0;top:0}.cc-preferences{background-color:#050a14f2!important;border:0;box-sizing:border-box;color:#111;font-family:sans-serif!important;line-height:1.4;margin:auto;max-height:100vh;overflow:auto;padding:0;z-index:100000}.cc-preferences:focus{outline:none}.cc-preferences *,.cc-preferences :after,.cc-preferences :before{box-sizing:inherit!important}.cc-preferences h2,.cc-preferences h3,.cc-preferences h4,.cc-preferences h5,.cc-preferences h6{font-family:sans-serif;font-style:normal;font-weight:700;margin:0 0 .5em}.cc-preferences .cc-h2,.cc-preferences h2{font-size:18px}.cc-preferences .cc-h3,.cc-preferences h3{font-size:16px}.cc-preferences .cc-h4,.cc-preferences .cc-h5,.cc-preferences .cc-h6,.cc-preferences h4,.cc-preferences h5,.cc-preferences h6{font-size:14px}.cc-preferences a{color:#025e8d;text-decoration:underline}.cc-preferences a:hover{color:inherit;text-decoration:none}.cc-preferences h3{background:none;color:#111;padding:0;text-transform:none}dialog.cc-preferences{background-color:initial}dialog.cc-preferences::backdrop{background-color:#000000e6}.cc-preferences__dialog{display:flex;flex-direction:column;margin:auto;max-height:100vh;max-width:860px;padding:12px;position:relative}.cc-preferences__dialog>:last-child{border-bottom-left-radius:10px;border-bottom-right-radius:10px}@media (min-width:980px){.cc-preferences__dialog{padding:16px}}.cc-preferences__close{background:#0000!important;border:1px solid #ececec;border-radius:50%;color:#111!important;cursor:pointer;font-family:Times New Roman,serif;font-size:40px;height:40px;left:auto;line-height:1;margin-top:-20px;padding:0!important;position:absolute;right:20px;top:50%;width:40px}.cc-preferences__close:focus{outline:3px solid #08c}.cc-preferences__close-label{border:0;clip:rect(0,0,0,0);height:1px;margin:-100%;overflow:hidden;padding:0;position:absolute!important;width:1px}.cc-preferences__header{background:#fff!important;border-bottom:1px solid #0000001a;border-top-left-radius:10px;border-top-right-radius:10px;padding:16px;position:relative;text-align:center}.cc-preferences__title{background:none!important;color:#111!important;font-family:sans-serif!important;font-size:18px!important;font-weight:700!important;margin:0!important;padding:0 16px 0 0!important}@media (min-width:480px){.cc-preferences__title{padding-right:0!important}}@media (min-width:980px){.cc-preferences__title{font-size:22px!important}}.cc-preferences__body{background:#fff!important;flex:1 1 auto;min-height:200px;overflow-x:hidden;overflow-y:auto;padding:16px}.cc-preferences__footer{background:#fff!important;border-top:1px solid #d0d0d0;box-shadow:0 0 5px 0 #0003;margin-bottom:0;padding:12px;position:relative}@media (min-width:480px){.cc-preferences__footer{align-items:stretch;display:flex}}.cc-preferences__footer>.cc-button{display:block;width:100%}@media (min-width:480px){.cc-preferences__footer>.cc-button{flex:1 1 auto}}@media (min-width:980px){.cc-preferences__footer>.cc-button{flex-basis:auto}}@media (min-width:480px){.cc-preferences__footer>.cc-button:not(:first-child){margin-left:12px}}.cc-preferences__footer>.cc-button:not(:last-child){margin-bottom:8px}@media (min-width:480px){.cc-preferences__footer>.cc-button:not(:last-child){margin-bottom:0}}.cc-preferences__categories{list-style:none;margin:0;padding:0}.cc-preferences__category:not(:last-child){border-bottom:1px solid #0000001a;margin-bottom:12px;padding-bottom:12px}.cc-preferences__category-description{font-size:14px;margin:0 0 8px;padding:0}.cc-preferences__category-footer{align-items:center;display:flex;justify-content:space-between}.cc-preferences__status{border:0;clip:rect(0,0,0,0);height:1px;margin:-100%;overflow:hidden;padding:0;position:absolute!important;width:1px}.cc-preferences__controls{display:flex;margin:0}.cc-preferences__controls>:not(:last-child){margin-right:12px}.cc-preferences__always-on{font-size:14px;font-weight:700;padding-left:26px;position:relative}.cc-preferences__always-on:before{background:#0000!important;border:solid;border-top-color:#0000;border-width:0 0 4px 4px;content:"";display:block;height:10px;left:0;position:absolute;top:2px;transform:rotate(-45deg);width:18px}.cc-preferences__details{background-color:#eff6fb!important;border-radius:10px;font-size:14px;margin:12px 0 0;padding:12px}.cc-preferences__cookie-list,.cc-preferences__provider-list{list-style:none;margin:0;padding:0}.cc-preferences__provider-list{columns:170px}.cc-preferences__cookie-title{font-size:1em;margin:0}.cc-preferences__cookie-description{font-size:1em;margin:0 0 8px}.cc-preferences__cookie-domain,.cc-preferences__cookie-lifespan{border-left:1px solid #999;color:#666;margin-left:8px;padding-left:4px}body.cc-has-preferences-open{overflow:hidden;position:relative}.cc-table{border-collapse:collapse;width:100%}.cc-table tbody tr{border-top:1px solid #0000001a}.cc-table td,.cc-table th{font-size:14px;padding:4px 8px;vertical-align:top}.cc-table+.cc-table{margin-top:8px}.cc-table--purposes td:first-child,.cc-table--purposes th:first-child{width:70%}@media (min-width:680px){.cc-table--purposes td:first-child,.cc-table--purposes th:first-child{width:80%}}.cc-table--purposes td:last-child,.cc-table--purposes th:last-child{padding-left:8px;width:40%}@media (min-width:680px){.cc-table--purposes td:last-child,.cc-table--purposes th:last-child{width:30%}}.cc-tabs,.cc-tabs>ul{background-color:#faf9f6;display:flex;list-style:none;margin:0;padding:0}.cc-tabs__panel{border-top:1px solid #ececec;margin-top:-1px}.cc-tabs__button{background-color:initial;border:0;border-left:1px solid #0000;border-right:1px solid #0000;color:#111;font-size:16px;font-weight:700;margin:0;padding:15px 16px 12px;position:relative}.cc-tabs__button:before{background-color:initial;content:"";display:block;height:3px;left:0;position:absolute;top:0;width:100%}.cc-tabs__button.cc-active{background-color:#fff;border-color:#025e8d #ececec #ececec;color:#111}.cc-tabs__button.cc-active:before{background-color:#025e8d}.cc-tabs__content{background-color:#fff;display:none;font-size:14px;padding:16px}.cc-tabs__section:not(:first-child){padding-top:24px}.cc-tcf{font-size:14px;margin-top:16px}.cc-tcf__list{list-style:none;margin:0;padding:0}.cc-vendor-count{color:#666;font-size:16px;margin:0 0 8px}.cc-hide{display:none!important}.cc-show{display:block!important}.cc-external-link{background-color:#ececec;background-image:url("data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyMCIgaGVpZ2h0PSIyMCI+PHBhdGggZD0iTTcgMGExIDEgMCAxIDEgMCAySDIuNmMtLjM3MSAwLS42LjIwOS0uNi41djE1YzAgLjI5MS4yMjkuNS42LjVoMTQuOGMuMzcxIDAgLjYtLjIwOS42LS41VjEzYTEgMSAwIDAgMSAyIDB2NC41YzAgMS40MzgtMS4xNjIgMi41LTIuNiAyLjVIMi42QzEuMTYyIDIwIDAgMTguOTM4IDAgMTcuNXYtMTVDMCAxLjA2MiAxLjE2MiAwIDIuNiAwem02IDBoNmwuMDc1LjAwMy4xMjYuMDE3LjExMS4wMy4xMTEuMDQ0LjA5OC4wNTIuMDk2LjA2Ny4wOS4wOGExIDEgMCAwIDEgLjA5Ny4xMTJsLjA3MS4xMS4wNTQuMTE0LjAzNS4xMDUuMDMuMTQ4TDIwIDF2NmExIDEgMCAwIDEtMiAwVjMuNDE0bC02LjY5MyA2LjY5M2ExIDEgMCAwIDEtMS40MTQtMS40MTRMMTYuNTg0IDJIMTNhMSAxIDAgMCAxLS45OTMtLjg4M0wxMiAxYTEgMSAwIDAgMSAxLTEiLz48L3N2Zz4=");background-position:right 8px center;background-repeat:no-repeat;background-size:10px auto;border-radius:3px;box-shadow:0 1px 0 #0003;color:#111!important;display:inline-block;font-size:12px;padding:4px 26px 4px 8px;text-decoration:none!important}.cc-external-link:hover{text-decoration:underline!important}.cc-m-0{margin:0!important}.cc-grey{color:#666}</style><meta http-equiv="origin-trial" content="AlK2UR5SkAlj8jjdEc9p3F3xuFYlF6LYjAML3EOqw1g26eCwWPjdmecULvBH5MVPoqKYrOfPhYVL71xAXI1IBQoAAAB8eyJvcmlnaW4iOiJodHRwczovL2RvdWJsZWNsaWNrLm5ldDo0NDMiLCJmZWF0dXJlIjoiV2ViVmlld1hSZXF1ZXN0ZWRXaXRoRGVwcmVjYXRpb24iLCJleHBpcnkiOjE3NTgwNjcxOTksImlzU3ViZG9tYWluIjp0cnVlfQ=="><meta http-equiv="origin-trial" content="Amm8/NmvvQfhwCib6I7ZsmUxiSCfOxWxHayJwyU1r3gRIItzr7bNQid6O8ZYaE1GSQTa69WwhPC9flq/oYkRBwsAAACCeyJvcmlnaW4iOiJodHRwczovL2dvb2dsZXN5bmRpY2F0aW9uLmNvbTo0NDMiLCJmZWF0dXJlIjoiV2ViVmlld1hSZXF1ZXN0ZWRXaXRoRGVwcmVjYXRpb24iLCJleHBpcnkiOjE3NTgwNjcxOTksImlzU3ViZG9tYWluIjp0cnVlfQ=="><meta http-equiv="origin-trial" content="A9wSqI5i0iwGdf6L1CERNdmsTPgVu44ewj8QxTBYgsv1LCPUVF7YmWOvTappqB1139jAymxUW/RO8zmMqo4zlAAAAACNeyJvcmlnaW4iOiJodHRwczovL2RvdWJsZWNsaWNrLm5ldDo0NDMiLCJmZWF0dXJlIjoiRmxlZGdlQmlkZGluZ0FuZEF1Y3Rpb25TZXJ2ZXIiLCJleHBpcnkiOjE3MzY4MTI4MDAsImlzU3ViZG9tYWluIjp0cnVlLCJpc1RoaXJkUGFydHkiOnRydWV9"><meta http-equiv="origin-trial" content="A+d7vJfYtay4OUbdtRPZA3y7bKQLsxaMEPmxgfhBGqKXNrdkCQeJlUwqa6EBbSfjwFtJWTrWIioXeMW+y8bWAgQAAACTeyJvcmlnaW4iOiJodHRwczovL2dvb2dsZXN5bmRpY2F0aW9uLmNvbTo0NDMiLCJmZWF0dXJlIjoiRmxlZGdlQmlkZGluZ0FuZEF1Y3Rpb25TZXJ2ZXIiLCJleHBpcnkiOjE3MzY4MTI4MDAsImlzU3ViZG9tYWluIjp0cnVlLCJpc1RoaXJkUGFydHkiOnRydWV9"><script src="https://pagead2.googlesyndication.com/pagead/managed/js/gpt/m202504030101/pubads_impl.js" async=""></script><link href="https://pagead2.googlesyndication.com/pagead/managed/dict/m202504080101/gpt" rel="compression-dictionary"><script async="" src="https://cdn.pbgrd.com/core-spl.js"></script><style type="text/css">.MathJax_Hover_Frame {border-radius: .25em; -webkit-border-radius: .25em; -moz-border-radius: .25em; -khtml-border-radius: .25em; box-shadow: 0px 0px 15px #83A; -webkit-box-shadow: 0px 0px 15px #83A; -moz-box-shadow: 0px 0px 15px #83A; -khtml-box-shadow: 0px 0px 15px #83A; border: 1px solid #A6D ! important; display: inline-block; position: absolute}
.MathJax_Menu_Button .MathJax_Hover_Arrow {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 4px; -webkit-border-radius: 4px; -moz-border-radius: 4px; -khtml-border-radius: 4px; font-family: 'Courier New',Courier; font-size: 9px; color: #F0F0F0}
.MathJax_Menu_Button .MathJax_Hover_Arrow span {display: block; background-color: #AAA; border: 1px solid; border-radius: 3px; line-height: 0; padding: 4px}
.MathJax_Hover_Arrow:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_Hover_Arrow:hover span {background-color: #CCC!important}
</style><style type="text/css">#MathJax_About {position: fixed; left: 50%; width: auto; text-align: center; border: 3px outset; padding: 1em 2em; background-color: #DDDDDD; color: black; cursor: default; font-family: message-box; font-size: 120%; font-style: normal; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 15px; -webkit-border-radius: 15px; -moz-border-radius: 15px; -khtml-border-radius: 15px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_About.MathJax_MousePost {outline: none}
.MathJax_Menu {position: absolute; background-color: white; color: black; width: auto; padding: 2px; border: 1px solid #CCCCCC; margin: 0; cursor: default; font: menu; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
.MathJax_MenuItem {padding: 2px 2em; background: transparent}
.MathJax_MenuArrow {position: absolute; right: .5em; padding-top: .25em; color: #666666; font-size: .75em}
.MathJax_MenuActive .MathJax_MenuArrow {color: white}
.MathJax_MenuArrow.RTL {left: .5em; right: auto}
.MathJax_MenuCheck {position: absolute; left: .7em}
.MathJax_MenuCheck.RTL {right: .7em; left: auto}
.MathJax_MenuRadioCheck {position: absolute; left: 1em}
.MathJax_MenuRadioCheck.RTL {right: 1em; left: auto}
.MathJax_MenuLabel {padding: 2px 2em 4px 1.33em; font-style: italic}
.MathJax_MenuRule {border-top: 1px solid #CCCCCC; margin: 4px 1px 0px}
.MathJax_MenuDisabled {color: GrayText}
.MathJax_MenuActive {background-color: Highlight; color: HighlightText}
.MathJax_MenuDisabled:focus, .MathJax_MenuLabel:focus {background-color: #E8E8E8}
.MathJax_ContextMenu:focus {outline: none}
.MathJax_ContextMenu .MathJax_MenuItem:focus {outline: none}
#MathJax_AboutClose {top: .2em; right: .2em}
.MathJax_Menu .MathJax_MenuClose {top: -10px; left: -10px}
.MathJax_MenuClose {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; font-family: 'Courier New',Courier; font-size: 24px; color: #F0F0F0}
.MathJax_MenuClose span {display: block; background-color: #AAA; border: 1.5px solid; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; line-height: 0; padding: 8px 0 6px}
.MathJax_MenuClose:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_MenuClose:hover span {background-color: #CCC!important}
.MathJax_MenuClose:hover:focus {outline: none}
</style><style type="text/css">.MJX_Assistive_MathML {position: absolute!important; top: 0; left: 0; clip: rect(1px, 1px, 1px, 1px); padding: 1px 0 0 0!important; border: 0!important; height: 1px!important; width: 1px!important; overflow: hidden!important; display: block!important; -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none}
.MJX_Assistive_MathML.MJX_Assistive_MathML_Block {width: 100%!important}
</style><style type="text/css">#MathJax_Zoom {position: absolute; background-color: #F0F0F0; overflow: auto; display: block; z-index: 301; padding: .5em; border: 1px solid black; margin: 0; font-weight: normal; font-style: normal; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; -webkit-box-sizing: content-box; -moz-box-sizing: content-box; box-sizing: content-box; box-shadow: 5px 5px 15px #AAAAAA; -webkit-box-shadow: 5px 5px 15px #AAAAAA; -moz-box-shadow: 5px 5px 15px #AAAAAA; -khtml-box-shadow: 5px 5px 15px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_ZoomOverlay {position: absolute; left: 0; top: 0; z-index: 300; display: inline-block; width: 100%; height: 100%; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
#MathJax_ZoomFrame {position: relative; display: inline-block; height: 0; width: 0}
#MathJax_ZoomEventTrap {position: absolute; left: 0; top: 0; z-index: 302; display: inline-block; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
</style><style type="text/css">.MathJax_Preview {color: #888}
#MathJax_Message {position: fixed; left: 1em; bottom: 1.5em; background-color: #E6E6E6; border: 1px solid #959595; margin: 0px; padding: 2px 8px; z-index: 102; color: black; font-size: 80%; width: auto; white-space: nowrap}
#MathJax_MSIE_Frame {position: absolute; top: 0; left: 0; width: 0px; z-index: 101; border: 0px; margin: 0px; padding: 0px}
.MathJax_Error {color: #CC0000; font-style: italic}
</style><style type="text/css">.MJXp-script {font-size: .8em}
.MJXp-right {-webkit-transform-origin: right; -moz-transform-origin: right; -ms-transform-origin: right; -o-transform-origin: right; transform-origin: right}
.MJXp-bold {font-weight: bold}
.MJXp-italic {font-style: italic}
.MJXp-scr {font-family: MathJax_Script,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-frak {font-family: MathJax_Fraktur,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-sf {font-family: MathJax_SansSerif,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-cal {font-family: MathJax_Caligraphic,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-mono {font-family: MathJax_Typewriter,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-largeop {font-size: 150%}
.MJXp-largeop.MJXp-int {vertical-align: -.2em}
.MJXp-math {display: inline-block; line-height: 1.2; text-indent: 0; font-family: 'Times New Roman',Times,STIXGeneral,serif; white-space: nowrap; border-collapse: collapse}
.MJXp-display {display: block; text-align: center; margin: 1em 0}
.MJXp-math span {display: inline-block}
.MJXp-box {display: block!important; text-align: center}
.MJXp-box:after {content: " "}
.MJXp-rule {display: block!important; margin-top: .1em}
.MJXp-char {display: block!important}
.MJXp-mo {margin: 0 .15em}
.MJXp-mfrac {margin: 0 .125em; vertical-align: .25em}
.MJXp-denom {display: inline-table!important; width: 100%}
.MJXp-denom > * {display: table-row!important}
.MJXp-surd {vertical-align: top}
.MJXp-surd > * {display: block!important}
.MJXp-script-box > *  {display: table!important; height: 50%}
.MJXp-script-box > * > * {display: table-cell!important; vertical-align: top}
.MJXp-script-box > *:last-child > * {vertical-align: bottom}
.MJXp-script-box > * > * > * {display: block!important}
.MJXp-mphantom {visibility: hidden}
.MJXp-munderover, .MJXp-munder {display: inline-table!important}
.MJXp-over {display: inline-block!important; text-align: center}
.MJXp-over > * {display: block!important}
.MJXp-munderover > *, .MJXp-munder > * {display: table-row!important}
.MJXp-mtable {vertical-align: .25em; margin: 0 .125em}
.MJXp-mtable > * {display: inline-table!important; vertical-align: middle}
.MJXp-mtr {display: table-row!important}
.MJXp-mtd {display: table-cell!important; text-align: center; padding: .5em 0 0 .5em}
.MJXp-mtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-mlabeledtr {display: table-row!important}
.MJXp-mlabeledtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mlabeledtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}
.MJXp-scale0 {-webkit-transform: scaleX(.0); -moz-transform: scaleX(.0); -ms-transform: scaleX(.0); -o-transform: scaleX(.0); transform: scaleX(.0)}
.MJXp-scale1 {-webkit-transform: scaleX(.1); -moz-transform: scaleX(.1); -ms-transform: scaleX(.1); -o-transform: scaleX(.1); transform: scaleX(.1)}
.MJXp-scale2 {-webkit-transform: scaleX(.2); -moz-transform: scaleX(.2); -ms-transform: scaleX(.2); -o-transform: scaleX(.2); transform: scaleX(.2)}
.MJXp-scale3 {-webkit-transform: scaleX(.3); -moz-transform: scaleX(.3); -ms-transform: scaleX(.3); -o-transform: scaleX(.3); transform: scaleX(.3)}
.MJXp-scale4 {-webkit-transform: scaleX(.4); -moz-transform: scaleX(.4); -ms-transform: scaleX(.4); -o-transform: scaleX(.4); transform: scaleX(.4)}
.MJXp-scale5 {-webkit-transform: scaleX(.5); -moz-transform: scaleX(.5); -ms-transform: scaleX(.5); -o-transform: scaleX(.5); transform: scaleX(.5)}
.MJXp-scale6 {-webkit-transform: scaleX(.6); -moz-transform: scaleX(.6); -ms-transform: scaleX(.6); -o-transform: scaleX(.6); transform: scaleX(.6)}
.MJXp-scale7 {-webkit-transform: scaleX(.7); -moz-transform: scaleX(.7); -ms-transform: scaleX(.7); -o-transform: scaleX(.7); transform: scaleX(.7)}
.MJXp-scale8 {-webkit-transform: scaleX(.8); -moz-transform: scaleX(.8); -ms-transform: scaleX(.8); -o-transform: scaleX(.8); transform: scaleX(.8)}
.MJXp-scale9 {-webkit-transform: scaleX(.9); -moz-transform: scaleX(.9); -ms-transform: scaleX(.9); -o-transform: scaleX(.9); transform: scaleX(.9)}
.MathJax_PHTML .noError {vertical-align: ; font-size: 90%; text-align: left; color: black; padding: 1px 3px; border: 1px solid}
</style></head>

    <body class=""><div id="MathJax_Message" style="display: none;"></div><dialog class="cc-banner" data-cc-banner="" data-nosnippet="" aria-labelledby="cc-banner-label" open="">
		<div class="cc-banner__content" autofocus="" tabindex="-1">
			<div class="cc-banner__container">
				<div class="cc-banner__header">
					<h2 class="cc-banner__title" id="cc-banner-label">Your privacy, your choice</h2>
				</div>
				<div class="cc-banner__body">
					<div class="cc-banner__policy">
						<p>We use essential cookies to make sure the site can function. We also use optional cookies for advertising, personalisation of content, usage analysis, and social media.</p><p>By accepting optional cookies, you consent to the processing of your personal data - including transfers to third parties. Some third parties are outside of the European Economic Area, with varying standards of data protection.</p><p>See our <a href="https://link.springer.com/privacystatement" data-cc-action="privacy">privacy policy</a> for more information on the use of your personal data.</p><p><button type="button" data-cc-action="preferences" class="cc-button cc-button--link cc-button--text">Manage preferences</button> for further information and to change your choices.</p>
					</div>
					
				</div>
				<div class="cc-banner__footer">
					
			<button data-cc-action="accept" class="cc-button cc-button--secondary cc-button--contrast cc-banner__button cc-banner__button-accept">Accept all cookies</button>
			
		
				</div>
			</div>
		</div>
	</dialog>
        
    
        
            <!-- Google Tag Manager (noscript) -->
            <noscript>
                <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-MRVXSHQ"
                        height="0" width="0" style="display:none;visibility:hidden"></iframe>
            </noscript>
            <!-- End Google Tag Manager (noscript) -->
        
    

        
    
        
            <!-- Google Tag Manager (noscript) -->
            <noscript data-test="gtm-body">
                <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-MRVXSHQ"
                height="0" width="0" style="display:none;visibility:hidden"></iframe>
            </noscript>
            <!-- End Google Tag Manager (noscript) -->
        
    


        <div class="u-visually-hidden" aria-hidden="true" data-test="darwin-icons">
    <!--?xml version="1.0" encoding="UTF-8"?--><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><radialGradient id="a" cx="0" cy="0" r="1" gradientTransform="rotate(45 -59.62 655.522) scale(44.8128 213.2632)" gradientUnits="userSpaceOnUse"><stop stop-color="#F58220"></stop><stop offset=".33" stop-color="#C75301"></stop><stop offset=".66" stop-color="#785BA7"></stop><stop offset="1" stop-color="#08C"></stop></radialGradient></defs><symbol id="icon-eds-i-accesses-medium" viewBox="0 0 24 24"><path d="M15.59 1a1 1 0 0 1 .706.291l5.41 5.385a1 1 0 0 1 .294.709v13.077c0 .674-.269 1.32-.747 1.796a2.549 2.549 0 0 1-1.798.742H15a1 1 0 0 1 0-2h4.455a.549.549 0 0 0 .387-.16.535.535 0 0 0 .158-.378V7.8L15.178 3H5.545a.543.543 0 0 0-.538.451L5 3.538v8.607a1 1 0 0 1-2 0V3.538A2.542 2.542 0 0 1 5.545 1h10.046ZM8 13c2.052 0 4.66 1.61 6.36 3.4l.124.141c.333.41.516.925.516 1.459 0 .6-.232 1.178-.64 1.599C12.666 21.388 10.054 23 8 23c-2.052 0-4.66-1.61-6.353-3.393A2.31 2.31 0 0 1 1 18c0-.6.232-1.178.64-1.6C3.34 14.61 5.948 13 8 13Zm0 2c-1.369 0-3.552 1.348-4.917 2.785A.31.31 0 0 0 3 18c0 .083.031.161.09.222C4.447 19.652 6.631 21 8 21c1.37 0 3.556-1.35 4.917-2.785A.31.31 0 0 0 13 18a.32.32 0 0 0-.048-.17l-.042-.052C11.553 16.348 9.369 15 8 15Zm0 1a2 2 0 1 1 0 4 2 2 0 0 1 0-4Z"></path></symbol><symbol id="icon-eds-i-altmetric-medium" viewBox="0 0 24 24"><path d="M12 1c5.978 0 10.843 4.77 10.996 10.712l.004.306-.002.022-.002.248C22.843 18.23 17.978 23 12 23 5.925 23 1 18.075 1 12S5.925 1 12 1Zm-1.726 9.246L8.848 12.53a1 1 0 0 1-.718.461L8.003 13l-4.947.014a9.001 9.001 0 0 0 17.887-.001L16.553 13l-2.205 3.53a1 1 0 0 1-1.735-.068l-.05-.11-2.289-6.106ZM12 3a9.001 9.001 0 0 0-8.947 8.013l4.391-.012L9.652 7.47a1 1 0 0 1 1.784.179l2.288 6.104 1.428-2.283a1 1 0 0 1 .722-.462l.129-.008 4.943.012A9.001 9.001 0 0 0 12 3Z"></path></symbol><symbol id="icon-eds-i-arrow-bend-down-medium" viewBox="0 0 24 24"><path d="m11.852 20.989.058.007L12 21l.075-.003.126-.017.111-.03.111-.044.098-.052.104-.074.082-.073 6-6a1 1 0 0 0-1.414-1.414L13 17.585v-12.2C13 4.075 11.964 3 10.667 3H4a1 1 0 1 0 0 2h6.667c.175 0 .333.164.333.385v12.2l-4.293-4.292a1 1 0 0 0-1.32-.083l-.094.083a1 1 0 0 0 0 1.414l6 6c.035.036.073.068.112.097l.11.071.114.054.105.035.118.025Z"></path></symbol><symbol id="icon-eds-i-arrow-bend-down-small" viewBox="0 0 16 16"><path d="M1 2a1 1 0 0 0 1 1h5v8.585L3.707 8.293a1 1 0 0 0-1.32-.083l-.094.083a1 1 0 0 0 0 1.414l5 5 .063.059.093.069.081.048.105.048.104.035.105.022.096.01h.136l.122-.018.113-.03.103-.04.1-.053.102-.07.052-.043 5.04-5.037a1 1 0 1 0-1.415-1.414L9 11.583V3a2 2 0 0 0-2-2H2a1 1 0 0 0-1 1Z"></path></symbol><symbol id="icon-eds-i-arrow-bend-up-medium" viewBox="0 0 24 24"><path d="m11.852 3.011.058-.007L12 3l.075.003.126.017.111.03.111.044.098.052.104.074.082.073 6 6a1 1 0 1 1-1.414 1.414L13 6.415v12.2C13 19.925 11.964 21 10.667 21H4a1 1 0 0 1 0-2h6.667c.175 0 .333-.164.333-.385v-12.2l-4.293 4.292a1 1 0 0 1-1.32.083l-.094-.083a1 1 0 0 1 0-1.414l6-6c.035-.036.073-.068.112-.097l.11-.071.114-.054.105-.035.118-.025Z"></path></symbol><symbol id="icon-eds-i-arrow-bend-up-small" viewBox="0 0 16 16"><path d="M1 13.998a1 1 0 0 1 1-1h5V4.413L3.707 7.705a1 1 0 0 1-1.32.084l-.094-.084a1 1 0 0 1 0-1.414l5-5 .063-.059.093-.068.081-.05.105-.047.104-.035.105-.022L7.94 1l.136.001.122.017.113.03.103.04.1.053.102.07.052.043 5.04 5.037a1 1 0 1 1-1.415 1.414L9 4.415v8.583a2 2 0 0 1-2 2H2a1 1 0 0 1-1-1Z"></path></symbol><symbol id="icon-eds-i-arrow-diagonal-medium" viewBox="0 0 24 24"><path d="M14 3h6l.075.003.126.017.111.03.111.044.098.052.096.067.09.08c.036.035.068.073.097.112l.071.11.054.114.035.105.03.148L21 4v6a1 1 0 0 1-2 0V6.414l-4.293 4.293a1 1 0 0 1-1.414-1.414L17.584 5H14a1 1 0 0 1-.993-.883L13 4a1 1 0 0 1 1-1ZM4 13a1 1 0 0 1 1 1v3.584l4.293-4.291a1 1 0 1 1 1.414 1.414L6.414 19H10a1 1 0 0 1 .993.883L11 20a1 1 0 0 1-1 1l-6.075-.003-.126-.017-.111-.03-.111-.044-.098-.052-.096-.067-.09-.08a1.01 1.01 0 0 1-.097-.112l-.071-.11-.054-.114-.035-.105-.025-.118-.007-.058L3 20v-6a1 1 0 0 1 1-1Z"></path></symbol><symbol id="icon-eds-i-arrow-diagonal-small" viewBox="0 0 16 16"><path d="m2 15-.082-.004-.119-.016-.111-.03-.111-.044-.098-.052-.096-.067-.09-.08a1.008 1.008 0 0 1-.097-.112l-.071-.11-.031-.062-.034-.081-.024-.076-.025-.118-.007-.058L1 14.02V9a1 1 0 1 1 2 0v2.584l2.793-2.791a1 1 0 1 1 1.414 1.414L4.414 13H7a1 1 0 0 1 .993.883L8 14a1 1 0 0 1-1 1H2ZM14 1l.081.003.12.017.111.03.111.044.098.052.096.067.09.08c.036.035.068.073.097.112l.071.11.031.062.034.081.024.076.03.148L15 2v5a1 1 0 0 1-2 0V4.414l-2.96 2.96A1 1 0 1 1 8.626 5.96L11.584 3H9a1 1 0 0 1-.993-.883L8 2a1 1 0 0 1 1-1h5Z"></path></symbol><symbol id="icon-eds-i-arrow-down-medium" viewBox="0 0 24 24"><path d="m20.707 12.728-7.99 7.98a.996.996 0 0 1-.561.281l-.157.011a.998.998 0 0 1-.788-.384l-7.918-7.908a1 1 0 0 1 1.414-1.416L11 17.576V4a1 1 0 0 1 2 0v13.598l6.293-6.285a1 1 0 0 1 1.32-.082l.095.083a1 1 0 0 1-.001 1.414Z"></path></symbol><symbol id="icon-eds-i-arrow-down-small" viewBox="0 0 16 16"><path d="m1.293 8.707 6 6 .063.059.093.069.081.048.105.049.104.034.056.013.118.017L8 15l.076-.003.122-.017.113-.03.085-.032.063-.03.098-.058.06-.043.05-.043 6.04-6.037a1 1 0 0 0-1.414-1.414L9 11.583V2a1 1 0 1 0-2 0v9.585L2.707 7.293a1 1 0 0 0-1.32-.083l-.094.083a1 1 0 0 0 0 1.414Z"></path></symbol><symbol id="icon-eds-i-arrow-left-medium" viewBox="0 0 24 24"><path d="m11.272 3.293-7.98 7.99a.996.996 0 0 0-.281.561L3 12.001c0 .32.15.605.384.788l7.908 7.918a1 1 0 0 0 1.416-1.414L6.424 13H20a1 1 0 0 0 0-2H6.402l6.285-6.293a1 1 0 0 0 .082-1.32l-.083-.095a1 1 0 0 0-1.414.001Z"></path></symbol><symbol id="icon-eds-i-arrow-left-small" viewBox="0 0 16 16"><path d="m7.293 1.293-6 6-.059.063-.069.093-.048.081-.049.105-.034.104-.013.056-.017.118L1 8l.003.076.017.122.03.113.032.085.03.063.058.098.043.06.043.05 6.037 6.04a1 1 0 0 0 1.414-1.414L4.417 9H14a1 1 0 0 0 0-2H4.415l4.292-4.293a1 1 0 0 0 .083-1.32l-.083-.094a1 1 0 0 0-1.414 0Z"></path></symbol><symbol id="icon-eds-i-arrow-right-medium" viewBox="0 0 24 24"><path d="m12.728 3.293 7.98 7.99a.996.996 0 0 1 .281.561l.011.157c0 .32-.15.605-.384.788l-7.908 7.918a1 1 0 0 1-1.416-1.414L17.576 13H4a1 1 0 0 1 0-2h13.598l-6.285-6.293a1 1 0 0 1-.082-1.32l.083-.095a1 1 0 0 1 1.414.001Z"></path></symbol><symbol id="icon-eds-i-arrow-right-small" viewBox="0 0 16 16"><path d="m8.707 1.293 6 6 .059.063.069.093.048.081.049.105.034.104.013.056.017.118L15 8l-.003.076-.017.122-.03.113-.032.085-.03.063-.058.098-.043.06-.043.05-6.037 6.04a1 1 0 0 1-1.414-1.414L11.583 9H2a1 1 0 1 1 0-2h9.585L7.293 2.707a1 1 0 0 1-.083-1.32l.083-.094a1 1 0 0 1 1.414 0Z"></path></symbol><symbol id="icon-eds-i-arrow-up-medium" viewBox="0 0 24 24"><path d="m3.293 11.272 7.99-7.98a.996.996 0 0 1 .561-.281L12.001 3c.32 0 .605.15.788.384l7.918 7.908a1 1 0 0 1-1.414 1.416L13 6.424V20a1 1 0 0 1-2 0V6.402l-6.293 6.285a1 1 0 0 1-1.32.082l-.095-.083a1 1 0 0 1 .001-1.414Z"></path></symbol><symbol id="icon-eds-i-arrow-up-small" viewBox="0 0 16 16"><path d="m1.293 7.293 6-6 .063-.059.093-.069.081-.048.105-.049.104-.034.056-.013.118-.017L8 1l.076.003.122.017.113.03.085.032.063.03.098.058.06.043.05.043 6.04 6.037a1 1 0 0 1-1.414 1.414L9 4.417V14a1 1 0 0 1-2 0V4.415L2.707 8.707a1 1 0 0 1-1.32.083l-.094-.083a1 1 0 0 1 0-1.414Z"></path></symbol><symbol id="icon-eds-i-article-medium" viewBox="0 0 24 24"><path d="M8 7a1 1 0 0 0 0 2h4a1 1 0 1 0 0-2H8ZM8 11a1 1 0 1 0 0 2h8a1 1 0 1 0 0-2H8ZM7 16a1 1 0 0 1 1-1h8a1 1 0 1 1 0 2H8a1 1 0 0 1-1-1Z"></path><path d="M5.545 1A2.542 2.542 0 0 0 3 3.538v16.924A2.542 2.542 0 0 0 5.545 23h12.91A2.542 2.542 0 0 0 21 20.462V3.5A2.5 2.5 0 0 0 18.5 1H5.545ZM5 3.538C5 3.245 5.24 3 5.545 3H18.5a.5.5 0 0 1 .5.5v16.962c0 .293-.24.538-.546.538H5.545A.542.542 0 0 1 5 20.462V3.538Z" clip-rule="evenodd"></path></symbol><symbol id="icon-eds-i-book-medium" viewBox="0 0 24 24"><path d="M18.5 1A2.5 2.5 0 0 1 21 3.5v12c0 1.16-.79 2.135-1.86 2.418l-.14.031V21h1a1 1 0 0 1 .993.883L21 22a1 1 0 0 1-1 1H6.5A3.5 3.5 0 0 1 3 19.5v-15A3.5 3.5 0 0 1 6.5 1h12ZM17 18H6.5a1.5 1.5 0 0 0-1.493 1.356L5 19.5A1.5 1.5 0 0 0 6.5 21H17v-3Zm1.5-15h-12A1.5 1.5 0 0 0 5 4.5v11.837l.054-.025a3.481 3.481 0 0 1 1.254-.307L6.5 16h12a.5.5 0 0 0 .492-.41L19 15.5v-12a.5.5 0 0 0-.5-.5ZM15 6a1 1 0 0 1 0 2H9a1 1 0 1 1 0-2h6Z"></path></symbol><symbol id="icon-eds-i-book-series-medium" viewBox="0 0 24 24"><path fill-rule="evenodd" d="M1 3.786C1 2.759 1.857 2 2.82 2H6.18c.964 0 1.82.759 1.82 1.786V4h3.168c.668 0 1.298.364 1.616.938.158-.109.333-.195.523-.252l3.216-.965c.923-.277 1.962.204 2.257 1.187l4.146 13.82c.296.984-.307 1.957-1.23 2.234l-3.217.965c-.923.277-1.962-.203-2.257-1.187L13 10.005v10.21c0 1.04-.878 1.785-1.834 1.785H7.833c-.291 0-.575-.07-.83-.195A1.849 1.849 0 0 1 6.18 22H2.821C1.857 22 1 21.241 1 20.214V3.786ZM3 4v11h3V4H3Zm0 16v-3h3v3H3Zm15.075-.04-.814-2.712 2.874-.862.813 2.712-2.873.862Zm1.485-5.49-2.874.862-2.634-8.782 2.873-.862 2.635 8.782ZM8 20V6h3v14H8Z" clip-rule="evenodd"></path></symbol><symbol id="icon-eds-i-calendar-acceptance-medium" viewBox="0 0 24 24"><path d="M17 2a1 1 0 0 1 1 1v1h1.5C20.817 4 22 5.183 22 6.5v13c0 1.317-1.183 2.5-2.5 2.5h-15C3.183 22 2 20.817 2 19.5v-13C2 5.183 3.183 4 4.5 4a1 1 0 1 1 0 2c-.212 0-.5.288-.5.5v13c0 .212.288.5.5.5h15c.212 0 .5-.288.5-.5v-13c0-.212-.288-.5-.5-.5H18v1a1 1 0 0 1-2 0V3a1 1 0 0 1 1-1Zm-.534 7.747a1 1 0 0 1 .094 1.412l-4.846 5.538a1 1 0 0 1-1.352.141l-2.77-2.076a1 1 0 0 1 1.2-1.6l2.027 1.519 4.236-4.84a1 1 0 0 1 1.411-.094ZM7.5 2a1 1 0 0 1 1 1v1H14a1 1 0 0 1 0 2H8.5v1a1 1 0 1 1-2 0V3a1 1 0 0 1 1-1Z"></path></symbol><symbol id="icon-eds-i-calendar-date-medium" viewBox="0 0 24 24"><path d="M17 2a1 1 0 0 1 1 1v1h1.5C20.817 4 22 5.183 22 6.5v13c0 1.317-1.183 2.5-2.5 2.5h-15C3.183 22 2 20.817 2 19.5v-13C2 5.183 3.183 4 4.5 4a1 1 0 1 1 0 2c-.212 0-.5.288-.5.5v13c0 .212.288.5.5.5h15c.212 0 .5-.288.5-.5v-13c0-.212-.288-.5-.5-.5H18v1a1 1 0 0 1-2 0V3a1 1 0 0 1 1-1ZM8 15a1 1 0 1 1 0 2 1 1 0 0 1 0-2Zm4 0a1 1 0 1 1 0 2 1 1 0 0 1 0-2Zm-4-4a1 1 0 1 1 0 2 1 1 0 0 1 0-2Zm4 0a1 1 0 1 1 0 2 1 1 0 0 1 0-2Zm4 0a1 1 0 1 1 0 2 1 1 0 0 1 0-2ZM7.5 2a1 1 0 0 1 1 1v1H14a1 1 0 0 1 0 2H8.5v1a1 1 0 1 1-2 0V3a1 1 0 0 1 1-1Z"></path></symbol><symbol id="icon-eds-i-calendar-decision-medium" viewBox="0 0 24 24"><path d="M17 2a1 1 0 0 1 1 1v1h1.5C20.817 4 22 5.183 22 6.5v13c0 1.317-1.183 2.5-2.5 2.5h-15C3.183 22 2 20.817 2 19.5v-13C2 5.183 3.183 4 4.5 4a1 1 0 1 1 0 2c-.212 0-.5.288-.5.5v13c0 .212.288.5.5.5h15c.212 0 .5-.288.5-.5v-13c0-.212-.288-.5-.5-.5H18v1a1 1 0 0 1-2 0V3a1 1 0 0 1 1-1Zm-2.935 8.246 2.686 2.645c.34.335.34.883 0 1.218l-2.686 2.645a.858.858 0 0 1-1.213-.009.854.854 0 0 1 .009-1.21l1.05-1.035H7.984a.992.992 0 0 1-.984-1c0-.552.44-1 .984-1h5.928l-1.051-1.036a.854.854 0 0 1-.085-1.121l.076-.088a.858.858 0 0 1 1.213-.009ZM7.5 2a1 1 0 0 1 1 1v1H14a1 1 0 0 1 0 2H8.5v1a1 1 0 1 1-2 0V3a1 1 0 0 1 1-1Z"></path></symbol><symbol id="icon-eds-i-calendar-impact-factor-medium" viewBox="0 0 24 24"><path d="M17 2a1 1 0 0 1 1 1v1h1.5C20.817 4 22 5.183 22 6.5v13c0 1.317-1.183 2.5-2.5 2.5h-15C3.183 22 2 20.817 2 19.5v-13C2 5.183 3.183 4 4.5 4a1 1 0 1 1 0 2c-.212 0-.5.288-.5.5v13c0 .212.288.5.5.5h15c.212 0 .5-.288.5-.5v-13c0-.212-.288-.5-.5-.5H18v1a1 1 0 0 1-2 0V3a1 1 0 0 1 1-1Zm-3.2 6.924a.48.48 0 0 1 .125.544l-1.52 3.283h2.304c.27 0 .491.215.491.483a.477.477 0 0 1-.13.327l-4.18 4.484a.498.498 0 0 1-.69.031.48.48 0 0 1-.125-.544l1.52-3.284H9.291a.487.487 0 0 1-.491-.482c0-.121.047-.238.13-.327l4.18-4.484a.498.498 0 0 1 .69-.031ZM7.5 2a1 1 0 0 1 1 1v1H14a1 1 0 0 1 0 2H8.5v1a1 1 0 1 1-2 0V3a1 1 0 0 1 1-1Z"></path></symbol><symbol id="icon-eds-i-call-papers-medium" viewBox="0 0 24 24"><g><path d="m20.707 2.883-1.414 1.414a1 1 0 0 0 1.414 1.414l1.414-1.414a1 1 0 0 0-1.414-1.414Z"></path><path d="M6 16.054c0 2.026 1.052 2.943 3 2.943a1 1 0 1 1 0 2c-2.996 0-5-1.746-5-4.943v-1.227a4.068 4.068 0 0 1-1.83-1.189 4.553 4.553 0 0 1-.87-1.455 4.868 4.868 0 0 1-.3-1.686c0-1.17.417-2.298 1.17-3.14.38-.426.834-.767 1.338-1 .51-.237 1.06-.36 1.617-.36L6.632 6H7l7.932-2.895A2.363 2.363 0 0 1 18 5.36v9.28a2.36 2.36 0 0 1-3.069 2.25l.084.03L7 14.997H6v1.057Zm9.637-11.057a.415.415 0 0 0-.083.008L8 7.638v5.536l7.424 1.786.104.02c.035.01.072.02.109.02.2 0 .363-.16.363-.36V5.36c0-.2-.163-.363-.363-.363Zm-9.638 3h-.874a1.82 1.82 0 0 0-.625.111l-.15.063a2.128 2.128 0 0 0-.689.517c-.42.47-.661 1.123-.661 1.81 0 .34.06.678.176.992.114.308.28.585.485.816.4.447.925.691 1.464.691h.874v-5Z" clip-rule="evenodd"></path><path d="M20 8.997h2a1 1 0 1 1 0 2h-2a1 1 0 1 1 0-2ZM20.707 14.293l1.414 1.414a1 1 0 0 1-1.414 1.414l-1.414-1.414a1 1 0 0 1 1.414-1.414Z"></path></g></symbol><symbol id="icon-eds-i-card-medium" viewBox="0 0 24 24"><path d="M19.615 2c.315 0 .716.067 1.14.279.76.38 1.245 1.107 1.245 2.106v15.23c0 .315-.067.716-.279 1.14-.38.76-1.107 1.245-2.106 1.245H4.385a2.56 2.56 0 0 1-1.14-.279C2.485 21.341 2 20.614 2 19.615V4.385c0-.315.067-.716.279-1.14C2.659 2.485 3.386 2 4.385 2h15.23Zm0 2H4.385c-.213 0-.265.034-.317.14A.71.71 0 0 0 4 4.385v15.23c0 .213.034.265.14.317a.71.71 0 0 0 .245.068h15.23c.213 0 .265-.034.317-.14a.71.71 0 0 0 .068-.245V4.385c0-.213-.034-.265-.14-.317A.71.71 0 0 0 19.615 4ZM17 16a1 1 0 0 1 0 2H7a1 1 0 0 1 0-2h10Zm0-3a1 1 0 0 1 0 2H7a1 1 0 0 1 0-2h10Zm-.5-7A1.5 1.5 0 0 1 18 7.5v3a1.5 1.5 0 0 1-1.5 1.5h-9A1.5 1.5 0 0 1 6 10.5v-3A1.5 1.5 0 0 1 7.5 6h9ZM16 8H8v2h8V8Z"></path></symbol><symbol id="icon-eds-i-cart-medium" viewBox="0 0 24 24"><path d="M5.76 1a1 1 0 0 1 .994.902L7.155 6h13.34c.18 0 .358.02.532.057l.174.045a2.5 2.5 0 0 1 1.693 3.103l-2.069 7.03c-.36 1.099-1.398 1.823-2.49 1.763H8.65c-1.272.015-2.352-.927-2.546-2.244L4.852 3H2a1 1 0 0 1-.993-.883L1 2a1 1 0 0 1 1-1h3.76Zm2.328 14.51a.555.555 0 0 0 .55.488l9.751.001a.533.533 0 0 0 .527-.357l2.059-7a.5.5 0 0 0-.48-.642H7.351l.737 7.51ZM18 19a2 2 0 1 1 0 4 2 2 0 0 1 0-4ZM8 19a2 2 0 1 1 0 4 2 2 0 0 1 0-4Z"></path></symbol><symbol id="icon-eds-i-check-circle-medium" viewBox="0 0 24 24"><path d="M12 1c6.075 0 11 4.925 11 11s-4.925 11-11 11S1 18.075 1 12 5.925 1 12 1Zm0 2a9 9 0 1 0 0 18 9 9 0 0 0 0-18Zm5.125 4.72a1 1 0 0 1 .156 1.405l-6 7.5a1 1 0 0 1-1.421.143l-3-2.5a1 1 0 0 1 1.28-1.536l2.217 1.846 5.362-6.703a1 1 0 0 1 1.406-.156Z"></path></symbol><symbol id="icon-eds-i-check-filled-medium" viewBox="0 0 24 24"><path d="M12 1c6.075 0 11 4.925 11 11s-4.925 11-11 11S1 18.075 1 12 5.925 1 12 1Zm5.125 6.72a1 1 0 0 0-1.406.155l-5.362 6.703-2.217-1.846a1 1 0 1 0-1.28 1.536l3 2.5a1 1 0 0 0 1.42-.143l6-7.5a1 1 0 0 0-.155-1.406Z"></path></symbol><symbol id="icon-eds-i-chevron-down-medium" viewBox="0 0 24 24"><path d="M3.305 8.28a1 1 0 0 0-.024 1.415l7.495 7.762c.314.345.757.543 1.224.543.467 0 .91-.198 1.204-.522l7.515-7.783a1 1 0 1 0-1.438-1.39L12 15.845l-7.28-7.54A1 1 0 0 0 3.4 8.2l-.096.082Z"></path></symbol><symbol id="icon-eds-i-chevron-down-small" viewBox="0 0 16 16"><path d="M13.692 5.278a1 1 0 0 1 .03 1.414L9.103 11.51a1.491 1.491 0 0 1-2.188.019L2.278 6.692a1 1 0 0 1 1.444-1.384L8 9.771l4.278-4.463a1 1 0 0 1 1.318-.111l.096.081Z"></path></symbol><symbol id="icon-eds-i-chevron-left-medium" viewBox="0 0 24 24"><path d="M15.72 3.305a1 1 0 0 0-1.415-.024l-7.762 7.495A1.655 1.655 0 0 0 6 12c0 .467.198.91.522 1.204l7.783 7.515a1 1 0 1 0 1.39-1.438L8.155 12l7.54-7.28A1 1 0 0 0 15.8 3.4l-.082-.096Z"></path></symbol><symbol id="icon-eds-i-chevron-left-small" viewBox="0 0 16 16"><path d="M10.722 2.308a1 1 0 0 0-1.414-.03L4.49 6.897a1.491 1.491 0 0 0-.019 2.188l4.838 4.637a1 1 0 1 0 1.384-1.444L6.229 8l4.463-4.278a1 1 0 0 0 .111-1.318l-.081-.096Z"></path></symbol><symbol id="icon-eds-i-chevron-right-medium" viewBox="0 0 24 24"><path d="M8.28 3.305a1 1 0 0 1 1.415-.024l7.762 7.495c.345.314.543.757.543 1.224 0 .467-.198.91-.522 1.204l-7.783 7.515a1 1 0 1 1-1.39-1.438L15.845 12l-7.54-7.28A1 1 0 0 1 8.2 3.4l.082-.096Z"></path></symbol><symbol id="icon-eds-i-chevron-right-small" viewBox="0 0 16 16"><path d="M5.278 2.308a1 1 0 0 1 1.414-.03l4.819 4.619a1.491 1.491 0 0 1 .019 2.188l-4.838 4.637a1 1 0 1 1-1.384-1.444L9.771 8 5.308 3.722a1 1 0 0 1-.111-1.318l.081-.096Z"></path></symbol><symbol id="icon-eds-i-chevron-up-medium" viewBox="0 0 24 24"><path d="M20.695 15.72a1 1 0 0 0 .024-1.415l-7.495-7.762A1.655 1.655 0 0 0 12 6c-.467 0-.91.198-1.204.522l-7.515 7.783a1 1 0 1 0 1.438 1.39L12 8.155l7.28 7.54a1 1 0 0 0 1.319.106l.096-.082Z"></path></symbol><symbol id="icon-eds-i-chevron-up-small" viewBox="0 0 16 16"><path d="M13.692 10.722a1 1 0 0 0 .03-1.414L9.103 4.49a1.491 1.491 0 0 0-2.188-.019L2.278 9.308a1 1 0 0 0 1.444 1.384L8 6.229l4.278 4.463a1 1 0 0 0 1.318.111l.096-.081Z"></path></symbol><symbol id="icon-eds-i-citations-medium" viewBox="0 0 24 24"><path d="M15.59 1a1 1 0 0 1 .706.291l5.41 5.385a1 1 0 0 1 .294.709v13.077c0 .674-.269 1.32-.747 1.796a2.549 2.549 0 0 1-1.798.742h-5.843a1 1 0 1 1 0-2h5.843a.549.549 0 0 0 .387-.16.535.535 0 0 0 .158-.378V7.8L15.178 3H5.545a.543.543 0 0 0-.538.451L5 3.538v8.607a1 1 0 0 1-2 0V3.538A2.542 2.542 0 0 1 5.545 1h10.046ZM5.483 14.35c.197.26.17.62-.049.848l-.095.083-.016.011c-.36.24-.628.45-.804.634-.393.409-.59.93-.59 1.562.077-.019.192-.028.345-.028.442 0 .84.158 1.195.474.355.316.532.716.532 1.2 0 .501-.173.9-.518 1.198-.345.298-.767.446-1.266.446-.672 0-1.209-.195-1.612-.585-.403-.39-.604-.976-.604-1.757 0-.744.11-1.39.33-1.938.222-.549.49-1.009.807-1.38a4.28 4.28 0 0 1 .992-.88c.07-.043.148-.087.232-.133a.881.881 0 0 1 1.121.245Zm5 0c.197.26.17.62-.049.848l-.095.083-.016.011c-.36.24-.628.45-.804.634-.393.409-.59.93-.59 1.562.077-.019.192-.028.345-.028.442 0 .84.158 1.195.474.355.316.532.716.532 1.2 0 .501-.173.9-.518 1.198-.345.298-.767.446-1.266.446-.672 0-1.209-.195-1.612-.585-.403-.39-.604-.976-.604-1.757 0-.744.11-1.39.33-1.938.222-.549.49-1.009.807-1.38a4.28 4.28 0 0 1 .992-.88c.07-.043.148-.087.232-.133a.881.881 0 0 1 1.121.245Z"></path></symbol><symbol id="icon-eds-i-clipboard-check-medium" viewBox="0 0 24 24"><path d="M14.4 1c1.238 0 2.274.865 2.536 2.024L18.5 3C19.886 3 21 4.14 21 5.535v14.93C21 21.86 19.886 23 18.5 23h-13C4.114 23 3 21.86 3 20.465V5.535C3 4.14 4.114 3 5.5 3h1.57c.27-1.147 1.3-2 2.53-2h4.8Zm4.115 4-1.59.024A2.601 2.601 0 0 1 14.4 7H9.6c-1.23 0-2.26-.853-2.53-2H5.5c-.27 0-.5.234-.5.535v14.93c0 .3.23.535.5.535h13c.27 0 .5-.234.5-.535V5.535c0-.3-.23-.535-.485-.535Zm-1.909 4.205a1 1 0 0 1 .19 1.401l-5.334 7a1 1 0 0 1-1.344.23l-2.667-1.75a1 1 0 1 1 1.098-1.672l1.887 1.238 4.769-6.258a1 1 0 0 1 1.401-.19ZM14.4 3H9.6a.6.6 0 0 0-.6.6v.8a.6.6 0 0 0 .6.6h4.8a.6.6 0 0 0 .6-.6v-.8a.6.6 0 0 0-.6-.6Z"></path></symbol><symbol id="icon-eds-i-clipboard-report-medium" viewBox="0 0 24 24"><path d="M14.4 1c1.238 0 2.274.865 2.536 2.024L18.5 3C19.886 3 21 4.14 21 5.535v14.93C21 21.86 19.886 23 18.5 23h-13C4.114 23 3 21.86 3 20.465V5.535C3 4.14 4.114 3 5.5 3h1.57c.27-1.147 1.3-2 2.53-2h4.8Zm4.115 4-1.59.024A2.601 2.601 0 0 1 14.4 7H9.6c-1.23 0-2.26-.853-2.53-2H5.5c-.27 0-.5.234-.5.535v14.93c0 .3.23.535.5.535h13c.27 0 .5-.234.5-.535V5.535c0-.3-.23-.535-.485-.535Zm-2.658 10.929a1 1 0 0 1 0 2H8a1 1 0 0 1 0-2h7.857Zm0-3.929a1 1 0 0 1 0 2H8a1 1 0 0 1 0-2h7.857ZM14.4 3H9.6a.6.6 0 0 0-.6.6v.8a.6.6 0 0 0 .6.6h4.8a.6.6 0 0 0 .6-.6v-.8a.6.6 0 0 0-.6-.6Z"></path></symbol><symbol id="icon-eds-i-close-medium" viewBox="0 0 24 24"><path d="M12 1c6.075 0 11 4.925 11 11s-4.925 11-11 11S1 18.075 1 12 5.925 1 12 1Zm0 2a9 9 0 1 0 0 18 9 9 0 0 0 0-18ZM8.707 7.293 12 10.585l3.293-3.292a1 1 0 0 1 1.414 1.414L13.415 12l3.292 3.293a1 1 0 0 1-1.414 1.414L12 13.415l-3.293 3.292a1 1 0 1 1-1.414-1.414L10.585 12 7.293 8.707a1 1 0 0 1 1.414-1.414Z"></path></symbol><symbol id="icon-eds-i-cloud-upload-medium" viewBox="0 0 24 24"><path d="m12.852 10.011.028-.004L13 10l.075.003.126.017.086.022.136.052.098.052.104.074.082.073 3 3a1 1 0 0 1 0 1.414l-.094.083a1 1 0 0 1-1.32-.083L14 13.416V20a1 1 0 0 1-2 0v-6.586l-1.293 1.293a1 1 0 0 1-1.32.083l-.094-.083a1 1 0 0 1 0-1.414l3-3 .112-.097.11-.071.114-.054.105-.035.118-.025Zm.587-7.962c3.065.362 5.497 2.662 5.992 5.562l.013.085.207.073c2.117.782 3.496 2.845 3.337 5.097l-.022.226c-.297 2.561-2.503 4.491-5.124 4.502a1 1 0 1 1-.009-2c1.619-.007 2.967-1.186 3.147-2.733.179-1.542-.86-2.979-2.487-3.353-.512-.149-.894-.579-.981-1.165-.21-2.237-2-4.035-4.308-4.308-2.31-.273-4.497 1.06-5.25 3.19l-.049.113c-.234.468-.718.756-1.176.743-1.418.057-2.689.857-3.32 2.084a3.668 3.668 0 0 0 .262 3.798c.796 1.136 2.169 1.764 3.583 1.635a1 1 0 1 1 .182 1.992c-2.125.194-4.193-.753-5.403-2.48a5.668 5.668 0 0 1-.403-5.86c.85-1.652 2.449-2.79 4.323-3.092l.287-.039.013-.028c1.207-2.741 4.125-4.404 7.186-4.042Z"></path></symbol><symbol id="icon-eds-i-collection-medium" viewBox="0 0 24 24"><path d="M21 7a1 1 0 0 1 1 1v12.5a2.5 2.5 0 0 1-2.5 2.5H8a1 1 0 0 1 0-2h11.5a.5.5 0 0 0 .5-.5V8a1 1 0 0 1 1-1Zm-5.5-5A2.5 2.5 0 0 1 18 4.5v12a2.5 2.5 0 0 1-2.5 2.5h-11A2.5 2.5 0 0 1 2 16.5v-12A2.5 2.5 0 0 1 4.5 2h11Zm0 2h-11a.5.5 0 0 0-.5.5v12a.5.5 0 0 0 .5.5h11a.5.5 0 0 0 .5-.5v-12a.5.5 0 0 0-.5-.5ZM13 13a1 1 0 0 1 0 2H7a1 1 0 0 1 0-2h6Zm0-3.5a1 1 0 0 1 0 2H7a1 1 0 0 1 0-2h6ZM13 6a1 1 0 0 1 0 2H7a1 1 0 1 1 0-2h6Z"></path></symbol><symbol id="icon-eds-i-conference-series-medium" viewBox="0 0 24 24"><path fill-rule="evenodd" d="M4.5 2A2.5 2.5 0 0 0 2 4.5v11A2.5 2.5 0 0 0 4.5 18h2.37l-2.534 2.253a1 1 0 0 0 1.328 1.494L9.88 18H11v3a1 1 0 1 0 2 0v-3h1.12l4.216 3.747a1 1 0 0 0 1.328-1.494L17.13 18h2.37a2.5 2.5 0 0 0 2.5-2.5v-11A2.5 2.5 0 0 0 19.5 2h-15ZM20 6V4.5a.5.5 0 0 0-.5-.5h-15a.5.5 0 0 0-.5.5V6h16ZM4 8v7.5a.5.5 0 0 0 .5.5h15a.5.5 0 0 0 .5-.5V8H4Z" clip-rule="evenodd"></path></symbol><symbol id="icon-eds-i-delivery-medium" viewBox="0 0 24 24"><path d="M8.51 20.598a3.037 3.037 0 0 1-3.02 0A2.968 2.968 0 0 1 4.161 19L3.5 19A2.5 2.5 0 0 1 1 16.5v-11A2.5 2.5 0 0 1 3.5 3h10a2.5 2.5 0 0 1 2.45 2.004L16 5h2.527c.976 0 1.855.585 2.27 1.49l2.112 4.62a1 1 0 0 1 .091.416v4.856C23 17.814 21.889 19 20.484 19h-.523a1.01 1.01 0 0 1-.121-.007 2.96 2.96 0 0 1-1.33 1.605 3.037 3.037 0 0 1-3.02 0A2.968 2.968 0 0 1 14.161 19H9.838a2.968 2.968 0 0 1-1.327 1.597Zm-2.024-3.462a.955.955 0 0 0-.481.73L5.999 18l.001.022a.944.944 0 0 0 .388.777l.098.065c.316.181.712.181 1.028 0A.97.97 0 0 0 8 17.978a.95.95 0 0 0-.486-.842 1.037 1.037 0 0 0-1.028 0Zm10 0a.955.955 0 0 0-.481.73l-.005.156a.944.944 0 0 0 .388.777l.098.065c.316.181.712.181 1.028 0a.97.97 0 0 0 .486-.886.95.95 0 0 0-.486-.842 1.037 1.037 0 0 0-1.028 0ZM21 12h-5v3.17a3.038 3.038 0 0 1 2.51.232 2.993 2.993 0 0 1 1.277 1.45l.058.155.058-.005.581-.002c.27 0 .516-.263.516-.618V12Zm-7.5-7h-10a.5.5 0 0 0-.5.5v11a.5.5 0 0 0 .5.5h.662a2.964 2.964 0 0 1 1.155-1.491l.172-.107a3.037 3.037 0 0 1 3.022 0A2.987 2.987 0 0 1 9.843 17H13.5a.5.5 0 0 0 .5-.5v-11a.5.5 0 0 0-.5-.5Zm5.027 2H16v3h4.203l-1.224-2.677a.532.532 0 0 0-.375-.316L18.527 7Z"></path></symbol><symbol id="icon-eds-i-download-medium" viewBox="0 0 24 24"><path d="M22 18.5a3.5 3.5 0 0 1-3.5 3.5h-13A3.5 3.5 0 0 1 2 18.5V18a1 1 0 0 1 2 0v.5A1.5 1.5 0 0 0 5.5 20h13a1.5 1.5 0 0 0 1.5-1.5V18a1 1 0 0 1 2 0v.5Zm-3.293-7.793-6 6-.063.059-.093.069-.081.048-.105.049-.104.034-.056.013-.118.017L12 17l-.076-.003-.122-.017-.113-.03-.085-.032-.063-.03-.098-.058-.06-.043-.05-.043-6.04-6.037a1 1 0 0 1 1.414-1.414l4.294 4.29L11 3a1 1 0 0 1 2 0l.001 10.585 4.292-4.292a1 1 0 0 1 1.32-.083l.094.083a1 1 0 0 1 0 1.414Z"></path></symbol><symbol id="icon-eds-i-edit-medium" viewBox="0 0 24 24"><path d="M17.149 2a2.38 2.38 0 0 1 1.699.711l2.446 2.46a2.384 2.384 0 0 1 .005 3.38L10.01 19.906a1 1 0 0 1-.434.257l-6.3 1.8a1 1 0 0 1-1.237-1.237l1.8-6.3a1 1 0 0 1 .257-.434L15.443 2.718A2.385 2.385 0 0 1 17.15 2Zm-3.874 5.689-7.586 7.536-1.234 4.319 4.318-1.234 7.54-7.582-3.038-3.039ZM17.149 4a.395.395 0 0 0-.286.126L14.695 6.28l3.029 3.029 2.162-2.173a.384.384 0 0 0 .106-.197L20 6.864c0-.103-.04-.2-.119-.278l-2.457-2.47A.385.385 0 0 0 17.149 4Z"></path></symbol><symbol id="icon-eds-i-education-medium" viewBox="0 0 24 24"><path fill-rule="evenodd" d="M12.41 2.088a1 1 0 0 0-.82 0l-10 4.5a1 1 0 0 0 0 1.824L3 9.047v7.124A3.001 3.001 0 0 0 4 22a3 3 0 0 0 1-5.83V9.948l1 .45V14.5a1 1 0 0 0 .087.408L7 14.5c-.913.408-.912.41-.912.41l.001.003.003.006.007.015a1.988 1.988 0 0 0 .083.16c.054.097.131.225.236.373.21.297.53.68.993 1.057C8.351 17.292 9.824 18 12 18c2.176 0 3.65-.707 4.589-1.476.463-.378.783-.76.993-1.057a4.162 4.162 0 0 0 .319-.533l.007-.015.003-.006v-.003h.002s0-.002-.913-.41l.913.408A1 1 0 0 0 18 14.5v-4.103l4.41-1.985a1 1 0 0 0 0-1.824l-10-4.5ZM16 11.297l-3.59 1.615a1 1 0 0 1-.82 0L8 11.297v2.94a3.388 3.388 0 0 0 .677.739C9.267 15.457 10.294 16 12 16s2.734-.543 3.323-1.024a3.388 3.388 0 0 0 .677-.739v-2.94ZM4.437 7.5 12 4.097 19.563 7.5 12 10.903 4.437 7.5ZM3 19a1 1 0 1 1 2 0 1 1 0 0 1-2 0Z" clip-rule="evenodd"></path></symbol><symbol id="icon-eds-i-error-diamond-medium" viewBox="0 0 24 24"><path d="M12.002 1c.702 0 1.375.279 1.871.775l8.35 8.353a2.646 2.646 0 0 1 .001 3.744l-8.353 8.353a2.646 2.646 0 0 1-3.742 0l-8.353-8.353a2.646 2.646 0 0 1 0-3.744l8.353-8.353.156-.142c.424-.362.952-.58 1.507-.625l.21-.008Zm0 2a.646.646 0 0 0-.38.123l-.093.08-8.34 8.34a.646.646 0 0 0-.18.355L3 12c0 .171.068.336.19.457l8.353 8.354a.646.646 0 0 0 .914 0l8.354-8.354a.646.646 0 0 0-.001-.914l-8.351-8.354A.646.646 0 0 0 12.002 3ZM12 14.5a1.5 1.5 0 0 1 .144 2.993L12 17.5a1.5 1.5 0 0 1 0-3ZM12 6a1 1 0 0 1 1 1v5a1 1 0 0 1-2 0V7a1 1 0 0 1 1-1Z"></path></symbol><symbol id="icon-eds-i-error-filled-medium" viewBox="0 0 24 24"><path d="M12.002 1c.702 0 1.375.279 1.871.775l8.35 8.353a2.646 2.646 0 0 1 .001 3.744l-8.353 8.353a2.646 2.646 0 0 1-3.742 0l-8.353-8.353a2.646 2.646 0 0 1 0-3.744l8.353-8.353.156-.142c.424-.362.952-.58 1.507-.625l.21-.008ZM12 14.5a1.5 1.5 0 0 0 0 3l.144-.007A1.5 1.5 0 0 0 12 14.5ZM12 6a1 1 0 0 0-1 1v5a1 1 0 0 0 2 0V7a1 1 0 0 0-1-1Z"></path></symbol><symbol id="icon-eds-i-external-link-medium" viewBox="0 0 24 24"><path d="M9 2a1 1 0 1 1 0 2H4.6c-.371 0-.6.209-.6.5v15c0 .291.229.5.6.5h14.8c.371 0 .6-.209.6-.5V15a1 1 0 0 1 2 0v4.5c0 1.438-1.162 2.5-2.6 2.5H4.6C3.162 22 2 20.938 2 19.5v-15C2 3.062 3.162 2 4.6 2H9Zm6 0h6l.075.003.126.017.111.03.111.044.098.052.096.067.09.08c.036.035.068.073.097.112l.071.11.054.114.035.105.03.148L22 3v6a1 1 0 0 1-2 0V5.414l-6.693 6.693a1 1 0 0 1-1.414-1.414L18.584 4H15a1 1 0 0 1-.993-.883L14 3a1 1 0 0 1 1-1Z"></path></symbol><symbol id="icon-eds-i-external-link-small" viewBox="0 0 16 16"><path d="M5 1a1 1 0 1 1 0 2l-2-.001V13L13 13v-2a1 1 0 0 1 2 0v2c0 1.15-.93 2-2.067 2H3.067C1.93 15 1 14.15 1 13V3c0-1.15.93-2 2.067-2H5Zm4 0h5l.075.003.126.017.111.03.111.044.098.052.096.067.09.08.044.047.073.093.051.083.054.113.035.105.03.148L15 2v5a1 1 0 0 1-2 0V4.414L9.107 8.307a1 1 0 0 1-1.414-1.414L11.584 3H9a1 1 0 0 1-.993-.883L8 2a1 1 0 0 1 1-1Z"></path></symbol><symbol id="icon-eds-i-file-download-medium" viewBox="0 0 24 24"><path d="M14.5 1a1 1 0 0 1 .707.293l5.5 5.5A1 1 0 0 1 21 7.5v12.962A2.542 2.542 0 0 1 18.455 23H5.545A2.542 2.542 0 0 1 3 20.462V3.538A2.542 2.542 0 0 1 5.545 1H14.5Zm-.415 2h-8.54A.542.542 0 0 0 5 3.538v16.924c0 .296.243.538.545.538h12.91a.542.542 0 0 0 .545-.538V7.915L14.085 3ZM12 7a1 1 0 0 1 1 1v6.585l2.293-2.292a1 1 0 0 1 1.32-.083l.094.083a1 1 0 0 1 0 1.414l-4 4a1.008 1.008 0 0 1-.112.097l-.11.071-.114.054-.105.035-.149.03L12 18l-.075-.003-.126-.017-.111-.03-.111-.044-.098-.052-.096-.067-.09-.08-4-4a1 1 0 0 1 1.414-1.414L11 14.585V8a1 1 0 0 1 1-1Z"></path></symbol><symbol id="icon-eds-i-file-report-medium" viewBox="0 0 24 24"><path d="M14.5 1a1 1 0 0 1 .707.293l5.5 5.5A1 1 0 0 1 21 7.5v12.962c0 .674-.269 1.32-.747 1.796a2.549 2.549 0 0 1-1.798.742H5.545c-.674 0-1.32-.267-1.798-.742A2.535 2.535 0 0 1 3 20.462V3.538A2.542 2.542 0 0 1 5.545 1H14.5Zm-.415 2h-8.54A.542.542 0 0 0 5 3.538v16.924c0 .142.057.278.158.379.102.102.242.159.387.159h12.91a.549.549 0 0 0 .387-.16.535.535 0 0 0 .158-.378V7.915L14.085 3ZM16 17a1 1 0 0 1 0 2H8a1 1 0 0 1 0-2h8Zm0-3a1 1 0 0 1 0 2H8a1 1 0 0 1 0-2h8Zm-4.793-6.207L13 9.585l1.793-1.792a1 1 0 0 1 1.32-.083l.094.083a1 1 0 0 1 0 1.414l-2.5 2.5a1 1 0 0 1-1.414 0L10.5 9.915l-1.793 1.792a1 1 0 0 1-1.32.083l-.094-.083a1 1 0 0 1 0-1.414l2.5-2.5a1 1 0 0 1 1.414 0Z"></path></symbol><symbol id="icon-eds-i-file-text-medium" viewBox="0 0 24 24"><path d="M14.5 1a1 1 0 0 1 .707.293l5.5 5.5A1 1 0 0 1 21 7.5v12.962A2.542 2.542 0 0 1 18.455 23H5.545A2.542 2.542 0 0 1 3 20.462V3.538A2.542 2.542 0 0 1 5.545 1H14.5Zm-.415 2h-8.54A.542.542 0 0 0 5 3.538v16.924c0 .296.243.538.545.538h12.91a.542.542 0 0 0 .545-.538V7.915L14.085 3ZM16 15a1 1 0 0 1 0 2H8a1 1 0 0 1 0-2h8Zm0-4a1 1 0 0 1 0 2H8a1 1 0 0 1 0-2h8Zm-5-4a1 1 0 0 1 0 2H8a1 1 0 1 1 0-2h3Z"></path></symbol><symbol id="icon-eds-i-file-upload-medium" viewBox="0 0 24 24"><path d="M14.5 1a1 1 0 0 1 .707.293l5.5 5.5A1 1 0 0 1 21 7.5v12.962A2.542 2.542 0 0 1 18.455 23H5.545A2.542 2.542 0 0 1 3 20.462V3.538A2.542 2.542 0 0 1 5.545 1H14.5Zm-.415 2h-8.54A.542.542 0 0 0 5 3.538v16.924c0 .296.243.538.545.538h12.91a.542.542 0 0 0 .545-.538V7.915L14.085 3Zm-2.233 4.011.058-.007L12 7l.075.003.126.017.111.03.111.044.098.052.104.074.082.073 4 4a1 1 0 0 1 0 1.414l-.094.083a1 1 0 0 1-1.32-.083L13 10.415V17a1 1 0 0 1-2 0v-6.585l-2.293 2.292a1 1 0 0 1-1.32.083l-.094-.083a1 1 0 0 1 0-1.414l4-4 .112-.097.11-.071.114-.054.105-.035.118-.025Z"></path></symbol><symbol id="icon-eds-i-filter-medium" viewBox="0 0 24 24"><path d="M21 2a1 1 0 0 1 .82 1.573L15 13.314V18a1 1 0 0 1-.31.724l-.09.076-4 3A1 1 0 0 1 9 21v-7.684L2.18 3.573a1 1 0 0 1 .707-1.567L3 2h18Zm-1.921 2H4.92l5.9 8.427a1 1 0 0 1 .172.45L11 13v6l2-1.5V13a1 1 0 0 1 .117-.469l.064-.104L19.079 4Z"></path></symbol><symbol id="icon-eds-i-funding-medium" viewBox="0 0 24 24"><path fill-rule="evenodd" d="M23 8A7 7 0 1 0 9 8a7 7 0 0 0 14 0ZM9.006 12.225A4.07 4.07 0 0 0 6.12 11.02H2a.979.979 0 1 0 0 1.958h4.12c.558 0 1.094.222 1.489.617l2.207 2.288c.27.27.27.687.012.944a.656.656 0 0 1-.928 0L7.744 15.67a.98.98 0 0 0-1.386 1.384l1.157 1.158c.535.536 1.244.791 1.946.765l.041.002h6.922c.874 0 1.597.748 1.597 1.688 0 .203-.146.354-.309.354H7.755c-.487 0-.96-.178-1.339-.504L2.64 17.259a.979.979 0 0 0-1.28 1.482L5.137 22c.733.631 1.66.979 2.618.979h9.957c1.26 0 2.267-1.043 2.267-2.312 0-2.006-1.584-3.646-3.555-3.646h-4.529a2.617 2.617 0 0 0-.681-2.509l-2.208-2.287ZM16 3a5 5 0 1 0 0 10 5 5 0 0 0 0-10Zm.979 3.5a.979.979 0 1 0-1.958 0v3a.979.979 0 1 0 1.958 0v-3Z" clip-rule="evenodd"></path></symbol><symbol id="icon-eds-i-hashtag-medium" viewBox="0 0 24 24"><path d="M12 1c6.075 0 11 4.925 11 11s-4.925 11-11 11S1 18.075 1 12 5.925 1 12 1Zm0 2a9 9 0 1 0 0 18 9 9 0 0 0 0-18ZM9.52 18.189a1 1 0 1 1-1.964-.378l.437-2.274H6a1 1 0 1 1 0-2h2.378l.592-3.076H6a1 1 0 0 1 0-2h3.354l.51-2.65a1 1 0 1 1 1.964.378l-.437 2.272h3.04l.51-2.65a1 1 0 1 1 1.964.378l-.438 2.272H18a1 1 0 0 1 0 2h-1.917l-.592 3.076H18a1 1 0 0 1 0 2h-2.893l-.51 2.652a1 1 0 1 1-1.964-.378l.437-2.274h-3.04l-.51 2.652Zm.895-4.652h3.04l.591-3.076h-3.04l-.591 3.076Z"></path></symbol><symbol id="icon-eds-i-home-medium" viewBox="0 0 24 24"><path d="M5 22a1 1 0 0 1-1-1v-8.586l-1.293 1.293a1 1 0 0 1-1.32.083l-.094-.083a1 1 0 0 1 0-1.414l10-10a1 1 0 0 1 1.414 0l10 10a1 1 0 0 1-1.414 1.414L20 12.415V21a1 1 0 0 1-1 1H5Zm7-17.585-6 5.999V20h5v-4a1 1 0 0 1 2 0v4h5v-9.585l-6-6Z"></path></symbol><symbol id="icon-eds-i-image-medium" viewBox="0 0 24 24"><path d="M19.615 2A2.385 2.385 0 0 1 22 4.385v15.23A2.385 2.385 0 0 1 19.615 22H4.385A2.385 2.385 0 0 1 2 19.615V4.385A2.385 2.385 0 0 1 4.385 2h15.23Zm0 2H4.385A.385.385 0 0 0 4 4.385v15.23c0 .213.172.385.385.385h1.244l10.228-8.76a1 1 0 0 1 1.254-.037L20 13.392V4.385A.385.385 0 0 0 19.615 4Zm-3.07 9.283L8.703 20h10.912a.385.385 0 0 0 .385-.385v-3.713l-3.455-2.619ZM9.5 6a3.5 3.5 0 1 1 0 7 3.5 3.5 0 0 1 0-7Zm0 2a1.5 1.5 0 1 0 0 3 1.5 1.5 0 0 0 0-3Z"></path></symbol><symbol id="icon-eds-i-impact-factor-medium" viewBox="0 0 24 24"><path d="M16.49 2.672c.74.694.986 1.765.632 2.712l-.04.1-1.549 3.54h1.477a2.496 2.496 0 0 1 2.485 2.34l.005.163c0 .618-.23 1.21-.642 1.675l-7.147 7.961a2.48 2.48 0 0 1-3.554.165 2.512 2.512 0 0 1-.633-2.712l.042-.103L9.108 15H7.46c-1.393 0-2.379-1.11-2.455-2.369L5 12.473c0-.593.142-1.145.628-1.692l7.307-7.944a2.48 2.48 0 0 1 3.555-.165ZM14.43 4.164l-7.33 7.97c-.083.093-.101.214-.101.34 0 .277.19.526.46.526h4.163l.097-.009c.015 0 .03.003.046.009.181.078.264.32.186.5l-2.554 5.817a.512.512 0 0 0 .127.552.48.48 0 0 0 .69-.033l7.155-7.97a.513.513 0 0 0 .13-.34.497.497 0 0 0-.49-.502h-3.988a.355.355 0 0 1-.328-.497l2.555-5.844a.512.512 0 0 0-.127-.552.48.48 0 0 0-.69.033Z"></path></symbol><symbol id="icon-eds-i-info-circle-medium" viewBox="0 0 24 24"><path d="M12 1c6.075 0 11 4.925 11 11s-4.925 11-11 11S1 18.075 1 12 5.925 1 12 1Zm0 2a9 9 0 1 0 0 18 9 9 0 0 0 0-18Zm0 7a1 1 0 0 1 1 1v5h1.5a1 1 0 0 1 0 2h-5a1 1 0 0 1 0-2H11v-4h-.5a1 1 0 0 1-.993-.883L9.5 11a1 1 0 0 1 1-1H12Zm0-4.5a1.5 1.5 0 0 1 .144 2.993L12 8.5a1.5 1.5 0 0 1 0-3Z"></path></symbol><symbol id="icon-eds-i-info-filled-medium" viewBox="0 0 24 24"><path d="M12 1c6.075 0 11 4.925 11 11s-4.925 11-11 11S1 18.075 1 12 5.925 1 12 1Zm0 9h-1.5a1 1 0 0 0-1 1l.007.117A1 1 0 0 0 10.5 12h.5v4H9.5a1 1 0 0 0 0 2h5a1 1 0 0 0 0-2H13v-5a1 1 0 0 0-1-1Zm0-4.5a1.5 1.5 0 0 0 0 3l.144-.007A1.5 1.5 0 0 0 12 5.5Z"></path></symbol><symbol id="icon-eds-i-journal-medium" viewBox="0 0 24 24"><path d="M18.5 1A2.5 2.5 0 0 1 21 3.5v14a2.5 2.5 0 0 1-2.5 2.5h-13a.5.5 0 1 0 0 1H20a1 1 0 0 1 0 2H5.5A2.5 2.5 0 0 1 3 20.5v-17A2.5 2.5 0 0 1 5.5 1h13ZM7 3H5.5a.5.5 0 0 0-.5.5v14.549l.016-.002c.104-.02.211-.035.32-.042L5.5 18H7V3Zm11.5 0H9v15h9.5a.5.5 0 0 0 .5-.5v-14a.5.5 0 0 0-.5-.5ZM16 5a1 1 0 0 1 1 1v4a1 1 0 0 1-1 1h-5a1 1 0 0 1-1-1V6a1 1 0 0 1 1-1h5Zm-1 2h-3v2h3V7Z"></path></symbol><symbol id="icon-eds-i-mail-medium" viewBox="0 0 24 24"><path d="M20.462 3C21.875 3 23 4.184 23 5.619v12.762C23 19.816 21.875 21 20.462 21H3.538C2.125 21 1 19.816 1 18.381V5.619C1 4.184 2.125 3 3.538 3h16.924ZM21 8.158l-7.378 6.258a2.549 2.549 0 0 1-3.253-.008L3 8.16v10.222c0 .353.253.619.538.619h16.924c.285 0 .538-.266.538-.619V8.158ZM20.462 5H3.538c-.264 0-.5.228-.534.542l8.65 7.334c.2.165.492.165.684.007l8.656-7.342-.001-.025c-.044-.3-.274-.516-.531-.516Z"></path></symbol><symbol id="icon-eds-i-mail-send-medium" viewBox="0 0 24 24"><path d="M20.444 5a2.562 2.562 0 0 1 2.548 2.37l.007.078.001.123v7.858A2.564 2.564 0 0 1 20.444 18H9.556A2.564 2.564 0 0 1 7 15.429l.001-7.977.007-.082A2.561 2.561 0 0 1 9.556 5h10.888ZM21 9.331l-5.46 3.51a1 1 0 0 1-1.08 0L9 9.332v6.097c0 .317.251.571.556.571h10.888a.564.564 0 0 0 .556-.571V9.33ZM20.444 7H9.556a.543.543 0 0 0-.32.105l5.763 3.706 5.766-3.706a.543.543 0 0 0-.32-.105ZM4.308 5a1 1 0 1 1 0 2H2a1 1 0 1 1 0-2h2.308Zm0 5.5a1 1 0 0 1 0 2H2a1 1 0 0 1 0-2h2.308Zm0 5.5a1 1 0 0 1 0 2H2a1 1 0 0 1 0-2h2.308Z"></path></symbol><symbol id="icon-eds-i-mentions-medium" viewBox="0 0 24 24"><path d="m9.452 1.293 5.92 5.92 2.92-2.92a1 1 0 0 1 1.415 1.414l-2.92 2.92 5.92 5.92a1 1 0 0 1 0 1.415 10.371 10.371 0 0 1-10.378 2.584l.652 3.258A1 1 0 0 1 12 23H2a1 1 0 0 1-.874-1.486l4.789-8.62C4.194 9.074 4.9 4.43 8.038 1.292a1 1 0 0 1 1.414 0Zm-2.355 13.59L3.699 21h7.081l-.689-3.442a10.392 10.392 0 0 1-2.775-2.396l-.22-.28Zm1.69-11.427-.07.09a8.374 8.374 0 0 0 11.737 11.737l.089-.071L8.787 3.456Z"></path></symbol><symbol id="icon-eds-i-menu-medium" viewBox="0 0 24 24"><path d="M21 4a1 1 0 0 1 0 2H3a1 1 0 1 1 0-2h18Zm-4 7a1 1 0 0 1 0 2H3a1 1 0 0 1 0-2h14Zm4 7a1 1 0 0 1 0 2H3a1 1 0 0 1 0-2h18Z"></path></symbol><symbol id="icon-eds-i-metrics-medium" viewBox="0 0 24 24"><path d="M3 22a1 1 0 0 1-1-1V3a1 1 0 0 1 1-1h6a1 1 0 0 1 1 1v7h4V8a1 1 0 0 1 1-1h6a1 1 0 0 1 1 1v13a1 1 0 0 1-.883.993L21 22H3Zm17-2V9h-4v11h4Zm-6-8h-4v8h4v-8ZM8 4H4v16h4V4Z"></path></symbol><symbol id="icon-eds-i-news-medium" viewBox="0 0 24 24"><path d="M17.384 3c.975 0 1.77.787 1.77 1.762v13.333c0 .462.354.846.815.899l.107.006.109-.006a.915.915 0 0 0 .809-.794l.006-.105V8.19a1 1 0 0 1 2 0v9.905A2.914 2.914 0 0 1 20.077 21H3.538a2.547 2.547 0 0 1-1.644-.601l-.147-.135A2.516 2.516 0 0 1 1 18.476V4.762C1 3.787 1.794 3 2.77 3h14.614Zm-.231 2H3v13.476c0 .11.035.216.1.304l.054.063c.101.1.24.157.384.157l13.761-.001-.026-.078a2.88 2.88 0 0 1-.115-.655l-.004-.17L17.153 5ZM14 15.021a.979.979 0 1 1 0 1.958H6a.979.979 0 1 1 0-1.958h8Zm0-8c.54 0 .979.438.979.979v4c0 .54-.438.979-.979.979H6A.979.979 0 0 1 5.021 12V8c0-.54.438-.979.979-.979h8Zm-.98 1.958H6.979v2.041h6.041V8.979Z"></path></symbol><symbol id="icon-eds-i-newsletter-medium" viewBox="0 0 24 24"><path d="M21 10a1 1 0 0 1 1 1v9.5a2.5 2.5 0 0 1-2.5 2.5h-15A2.5 2.5 0 0 1 2 20.5V11a1 1 0 0 1 2 0v.439l8 4.888 8-4.889V11a1 1 0 0 1 1-1Zm-1 3.783-7.479 4.57a1 1 0 0 1-1.042 0l-7.48-4.57V20.5a.5.5 0 0 0 .501.5h15a.5.5 0 0 0 .5-.5v-6.717ZM15 9a1 1 0 0 1 0 2H9a1 1 0 0 1 0-2h6Zm2.5-8A2.5 2.5 0 0 1 20 3.5V9a1 1 0 0 1-2 0V3.5a.5.5 0 0 0-.5-.5h-11a.5.5 0 0 0-.5.5V9a1 1 0 1 1-2 0V3.5A2.5 2.5 0 0 1 6.5 1h11ZM15 5a1 1 0 0 1 0 2H9a1 1 0 1 1 0-2h6Z"></path></symbol><symbol id="icon-eds-i-notifcation-medium" viewBox="0 0 24 24"><path d="M14 20a1 1 0 0 1 0 2h-4a1 1 0 0 1 0-2h4ZM3 18l-.133-.007c-1.156-.124-1.156-1.862 0-1.986l.3-.012C4.32 15.923 5 15.107 5 14V9.5C5 5.368 8.014 2 12 2s7 3.368 7 7.5V14c0 1.107.68 1.923 1.832 1.995l.301.012c1.156.124 1.156 1.862 0 1.986L21 18H3Zm9-14C9.17 4 7 6.426 7 9.5V14c0 .671-.146 1.303-.416 1.858L6.51 16h10.979l-.073-.142a4.192 4.192 0 0 1-.412-1.658L17 14V9.5C17 6.426 14.83 4 12 4Z"></path></symbol><symbol id="icon-eds-i-publish-medium" viewBox="0 0 24 24"><g><path d="M16.296 1.291A1 1 0 0 0 15.591 1H5.545A2.542 2.542 0 0 0 3 3.538V13a1 1 0 1 0 2 0V3.538l.007-.087A.543.543 0 0 1 5.545 3h9.633L20 7.8v12.662a.534.534 0 0 1-.158.379.548.548 0 0 1-.387.159H11a1 1 0 1 0 0 2h8.455c.674 0 1.32-.267 1.798-.742A2.534 2.534 0 0 0 22 20.462V7.385a1 1 0 0 0-.294-.709l-5.41-5.385Z"></path><path d="M10.762 16.647a1 1 0 0 0-1.525-1.294l-4.472 5.271-2.153-1.665a1 1 0 1 0-1.224 1.582l2.91 2.25a1 1 0 0 0 1.374-.144l5.09-6ZM16 10a1 1 0 1 1 0 2H8a1 1 0 1 1 0-2h8ZM12 7a1 1 0 0 0-1-1H8a1 1 0 1 0 0 2h3a1 1 0 0 0 1-1Z"></path></g></symbol><symbol id="icon-eds-i-refresh-medium" viewBox="0 0 24 24"><g><path d="M7.831 5.636H6.032A8.76 8.76 0 0 1 9 3.631 8.549 8.549 0 0 1 12.232 3c.603 0 1.192.063 1.76.182C17.979 4.017 21 7.632 21 12a1 1 0 1 0 2 0c0-5.296-3.674-9.746-8.591-10.776A10.61 10.61 0 0 0 5 3.851V2.805a1 1 0 0 0-.987-1H4a1 1 0 0 0-1 1v3.831a1 1 0 0 0 1 1h3.831a1 1 0 0 0 .013-2h-.013ZM17.968 18.364c-1.59 1.632-3.784 2.636-6.2 2.636C6.948 21 3 16.993 3 12a1 1 0 1 0-2 0c0 6.053 4.799 11 10.768 11 2.788 0 5.324-1.082 7.232-2.85v1.045a1 1 0 1 0 2 0v-3.831a1 1 0 0 0-1-1h-3.831a1 1 0 0 0 0 2h1.799Z"></path></g></symbol><symbol id="icon-eds-i-search-medium" viewBox="0 0 24 24"><path d="M11 1c5.523 0 10 4.477 10 10 0 2.4-.846 4.604-2.256 6.328l3.963 3.965a1 1 0 0 1-1.414 1.414l-3.965-3.963A9.959 9.959 0 0 1 11 21C5.477 21 1 16.523 1 11S5.477 1 11 1Zm0 2a8 8 0 1 0 0 16 8 8 0 0 0 0-16Z"></path></symbol><symbol id="icon-eds-i-settings-medium" viewBox="0 0 24 24"><path d="M11.382 1h1.24a2.508 2.508 0 0 1 2.334 1.63l.523 1.378 1.59.933 1.444-.224c.954-.132 1.89.3 2.422 1.101l.095.155.598 1.066a2.56 2.56 0 0 1-.195 2.848l-.894 1.161v1.896l.92 1.163c.6.768.707 1.812.295 2.674l-.09.17-.606 1.08a2.504 2.504 0 0 1-2.531 1.25l-1.428-.223-1.589.932-.523 1.378a2.512 2.512 0 0 1-2.155 1.625L12.65 23h-1.27a2.508 2.508 0 0 1-2.334-1.63l-.524-1.379-1.59-.933-1.443.225c-.954.132-1.89-.3-2.422-1.101l-.095-.155-.598-1.066a2.56 2.56 0 0 1 .195-2.847l.891-1.161v-1.898l-.919-1.162a2.562 2.562 0 0 1-.295-2.674l.09-.17.606-1.08a2.504 2.504 0 0 1 2.531-1.25l1.43.223 1.618-.938.524-1.375.07-.167A2.507 2.507 0 0 1 11.382 1Zm.003 2a.509.509 0 0 0-.47.338l-.65 1.71a1 1 0 0 1-.434.51L7.6 6.85a1 1 0 0 1-.655.123l-1.762-.275a.497.497 0 0 0-.498.252l-.61 1.088a.562.562 0 0 0 .04.619l1.13 1.43a1 1 0 0 1 .216.62v2.585a1 1 0 0 1-.207.61L4.15 15.339a.568.568 0 0 0-.036.634l.601 1.072a.494.494 0 0 0 .484.26l1.78-.278a1 1 0 0 1 .66.126l2.2 1.292a1 1 0 0 1 .43.507l.648 1.71a.508.508 0 0 0 .467.338h1.263a.51.51 0 0 0 .47-.34l.65-1.708a1 1 0 0 1 .428-.507l2.201-1.292a1 1 0 0 1 .66-.126l1.763.275a.497.497 0 0 0 .498-.252l.61-1.088a.562.562 0 0 0-.04-.619l-1.13-1.43a1 1 0 0 1-.216-.62v-2.585a1 1 0 0 1 .207-.61l1.105-1.437a.568.568 0 0 0 .037-.634l-.601-1.072a.494.494 0 0 0-.484-.26l-1.78.278a1 1 0 0 1-.66-.126l-2.2-1.292a1 1 0 0 1-.43-.507l-.649-1.71A.508.508 0 0 0 12.62 3h-1.234ZM12 8a4 4 0 1 1 0 8 4 4 0 0 1 0-8Zm0 2a2 2 0 1 0 0 4 2 2 0 0 0 0-4Z"></path></symbol><symbol id="icon-eds-i-shipping-medium" viewBox="0 0 24 24"><path d="M16.515 2c1.406 0 2.706.728 3.352 1.902l2.02 3.635.02.042.036.089.031.105.012.058.01.073.004.075v11.577c0 .64-.244 1.255-.683 1.713a2.356 2.356 0 0 1-1.701.731H4.386a2.356 2.356 0 0 1-1.702-.731 2.476 2.476 0 0 1-.683-1.713V7.948c.01-.217.083-.43.22-.6L4.2 3.905C4.833 2.755 6.089 2.032 7.486 2h9.029ZM20 9H4v10.556a.49.49 0 0 0 .075.26l.053.07a.356.356 0 0 0 .257.114h15.23c.094 0 .186-.04.258-.115a.477.477 0 0 0 .127-.33V9Zm-2 7.5a1 1 0 0 1 0 2h-4a1 1 0 0 1 0-2h4ZM16.514 4H13v3h6.3l-1.183-2.13c-.288-.522-.908-.87-1.603-.87ZM11 3.999H7.51c-.679.017-1.277.36-1.566.887L4.728 7H11V3.999Z"></path></symbol><symbol id="icon-eds-i-step-guide-medium" viewBox="0 0 24 24"><path d="M11.394 9.447a1 1 0 1 0-1.788-.894l-.88 1.759-.019-.02a1 1 0 1 0-1.414 1.415l1 1a1 1 0 0 0 1.601-.26l1.5-3ZM12 11a1 1 0 0 1 1-1h3a1 1 0 1 1 0 2h-3a1 1 0 0 1-1-1ZM12 17a1 1 0 0 1 1-1h3a1 1 0 1 1 0 2h-3a1 1 0 0 1-1-1ZM10.947 14.105a1 1 0 0 1 .447 1.342l-1.5 3a1 1 0 0 1-1.601.26l-1-1a1 1 0 1 1 1.414-1.414l.02.019.879-1.76a1 1 0 0 1 1.341-.447Z"></path><path d="M5.545 1A2.542 2.542 0 0 0 3 3.538v16.924A2.542 2.542 0 0 0 5.545 23h12.91A2.542 2.542 0 0 0 21 20.462V7.5a1 1 0 0 0-.293-.707l-5.5-5.5A1 1 0 0 0 14.5 1H5.545ZM5 3.538C5 3.245 5.24 3 5.545 3h8.54L19 7.914v12.547c0 .294-.24.539-.546.539H5.545A.542.542 0 0 1 5 20.462V3.538Z" clip-rule="evenodd"></path></symbol><symbol id="icon-eds-i-submission-medium" viewBox="0 0 24 24"><g><path d="M5 3.538C5 3.245 5.24 3 5.545 3h9.633L20 7.8v12.662a.535.535 0 0 1-.158.379.549.549 0 0 1-.387.159H6a1 1 0 0 1-1-1v-2.5a1 1 0 1 0-2 0V20a3 3 0 0 0 3 3h13.455c.673 0 1.32-.266 1.798-.742A2.535 2.535 0 0 0 22 20.462V7.385a1 1 0 0 0-.294-.709l-5.41-5.385A1 1 0 0 0 15.591 1H5.545A2.542 2.542 0 0 0 3 3.538V7a1 1 0 0 0 2 0V3.538Z"></path><path d="m13.707 13.707-4 4a1 1 0 0 1-1.414 0l-.083-.094a1 1 0 0 1 .083-1.32L10.585 14 2 14a1 1 0 1 1 0-2l8.583.001-2.29-2.294a1 1 0 0 1 1.414-1.414l4.037 4.04.043.05.043.06.059.098.03.063.031.085.03.113.017.122L14 13l-.004.087-.017.118-.013.056-.034.104-.049.105-.048.081-.07.093-.058.063Z"></path></g></symbol><symbol id="icon-eds-i-table-1-medium" viewBox="0 0 24 24"><path d="M4.385 22a2.56 2.56 0 0 1-1.14-.279C2.485 21.341 2 20.614 2 19.615V4.385c0-.315.067-.716.279-1.14C2.659 2.485 3.386 2 4.385 2h15.23c.315 0 .716.067 1.14.279.76.38 1.245 1.107 1.245 2.106v15.23c0 .315-.067.716-.279 1.14-.38.76-1.107 1.245-2.106 1.245H4.385ZM4 19.615c0 .213.034.265.14.317a.71.71 0 0 0 .245.068H8v-4H4v3.615ZM20 16H10v4h9.615c.213 0 .265-.034.317-.14a.71.71 0 0 0 .068-.245V16Zm0-2v-4H10v4h10ZM4 14h4v-4H4v4ZM19.615 4H10v4h10V4.385c0-.213-.034-.265-.14-.317A.71.71 0 0 0 19.615 4ZM8 4H4.385l-.082.002c-.146.01-.19.047-.235.138A.71.71 0 0 0 4 4.385V8h4V4Z"></path></symbol><symbol id="icon-eds-i-table-2-medium" viewBox="0 0 24 24"><path d="M4.384 22A2.384 2.384 0 0 1 2 19.616V4.384A2.384 2.384 0 0 1 4.384 2h15.232A2.384 2.384 0 0 1 22 4.384v15.232A2.384 2.384 0 0 1 19.616 22H4.384ZM10 15H4v4.616c0 .212.172.384.384.384H10v-5Zm5 0h-3v5h3v-5Zm5 0h-3v5h2.616a.384.384 0 0 0 .384-.384V15ZM10 9H4v4h6V9Zm5 0h-3v4h3V9Zm5 0h-3v4h3V9Zm-.384-5H4.384A.384.384 0 0 0 4 4.384V7h16V4.384A.384.384 0 0 0 19.616 4Z"></path></symbol><symbol id="icon-eds-i-tag-medium" viewBox="0 0 24 24"><path d="m12.621 1.998.127.004L20.496 2a1.5 1.5 0 0 1 1.497 1.355L22 3.5l-.005 7.669c.038.456-.133.905-.447 1.206l-9.02 9.018a2.075 2.075 0 0 1-2.932 0l-6.99-6.99a2.075 2.075 0 0 1 .001-2.933L11.61 2.47c.246-.258.573-.418.881-.46l.131-.011Zm.286 2-8.885 8.886a.075.075 0 0 0 0 .106l6.987 6.988c.03.03.077.03.106 0l8.883-8.883L19.999 4l-7.092-.002ZM16 6.5a1.5 1.5 0 0 1 .144 2.993L16 9.5a1.5 1.5 0 0 1 0-3Z"></path></symbol><symbol id="icon-eds-i-trash-medium" viewBox="0 0 24 24"><path d="M12 1c2.717 0 4.913 2.232 4.997 5H21a1 1 0 0 1 0 2h-1v12.5c0 1.389-1.152 2.5-2.556 2.5H6.556C5.152 23 4 21.889 4 20.5V8H3a1 1 0 1 1 0-2h4.003l.001-.051C7.114 3.205 9.3 1 12 1Zm6 7H6v12.5c0 .238.19.448.454.492l.102.008h10.888c.315 0 .556-.232.556-.5V8Zm-4 3a1 1 0 0 1 1 1v6.005a1 1 0 0 1-2 0V12a1 1 0 0 1 1-1Zm-4 0a1 1 0 0 1 1 1v6a1 1 0 0 1-2 0v-6a1 1 0 0 1 1-1Zm2-8c-1.595 0-2.914 1.32-2.996 3h5.991v-.02C14.903 4.31 13.589 3 12 3Z"></path></symbol><symbol id="icon-eds-i-user-account-medium" viewBox="0 0 24 24"><path d="M12 1c6.075 0 11 4.925 11 11s-4.925 11-11 11S1 18.075 1 12 5.925 1 12 1Zm0 16c-1.806 0-3.52.994-4.664 2.698A8.947 8.947 0 0 0 12 21a8.958 8.958 0 0 0 4.664-1.301C15.52 17.994 13.806 17 12 17Zm0-14a9 9 0 0 0-6.25 15.476C7.253 16.304 9.54 15 12 15s4.747 1.304 6.25 3.475A9 9 0 0 0 12 3Zm0 3a4 4 0 1 1 0 8 4 4 0 0 1 0-8Zm0 2a2 2 0 1 0 0 4 2 2 0 0 0 0-4Z"></path></symbol><symbol id="icon-eds-i-user-add-medium" viewBox="0 0 24 24"><path d="M9 1a5 5 0 1 1 0 10A5 5 0 0 1 9 1Zm0 2a3 3 0 1 0 0 6 3 3 0 0 0 0-6Zm9 10a1 1 0 0 1 1 1v3h3a1 1 0 0 1 0 2h-3v3a1 1 0 0 1-2 0v-3h-3a1 1 0 0 1 0-2h3v-3a1 1 0 0 1 1-1Zm-5.545-.15a1 1 0 1 1-.91 1.78 5.713 5.713 0 0 0-5.705.282c-1.67 1.068-2.728 2.927-2.832 4.956L3.004 20 11.5 20a1 1 0 0 1 .993.883L12.5 21a1 1 0 0 1-1 1H2a1 1 0 0 1-1-1v-.876c.028-2.812 1.446-5.416 3.763-6.897a7.713 7.713 0 0 1 7.692-.378Z"></path></symbol><symbol id="icon-eds-i-user-assign-medium" viewBox="0 0 24 24"><path d="M16.226 13.298a1 1 0 0 1 1.414-.01l.084.093a1 1 0 0 1-.073 1.32L15.39 17H22a1 1 0 0 1 0 2h-6.611l2.262 2.298a1 1 0 0 1-1.425 1.404l-3.939-4a1 1 0 0 1 0-1.404l3.94-4Zm-3.771-.449a1 1 0 1 1-.91 1.781 5.713 5.713 0 0 0-5.705.282c-1.67 1.068-2.728 2.927-2.832 4.956L3.004 20 10.5 20a1 1 0 0 1 .993.883L11.5 21a1 1 0 0 1-1 1H2a1 1 0 0 1-1-1v-.876c.028-2.812 1.446-5.416 3.763-6.897a7.713 7.713 0 0 1 7.692-.378ZM9 1a5 5 0 1 1 0 10A5 5 0 0 1 9 1Zm0 2a3 3 0 1 0 0 6 3 3 0 0 0 0-6Z"></path></symbol><symbol id="icon-eds-i-user-block-medium" viewBox="0 0 24 24"><path d="M9 1a5 5 0 1 1 0 10A5 5 0 0 1 9 1Zm0 2a3 3 0 1 0 0 6 3 3 0 0 0 0-6Zm9 10a5 5 0 1 1 0 10 5 5 0 0 1 0-10Zm-5.545-.15a1 1 0 1 1-.91 1.78 5.713 5.713 0 0 0-5.705.282c-1.67 1.068-2.728 2.927-2.832 4.956L3.004 20 11.5 20a1 1 0 0 1 .993.883L12.5 21a1 1 0 0 1-1 1H2a1 1 0 0 1-1-1v-.876c.028-2.812 1.446-5.416 3.763-6.897a7.713 7.713 0 0 1 7.692-.378ZM15 18a3 3 0 0 0 4.294 2.707l-4.001-4c-.188.391-.293.83-.293 1.293Zm3-3c-.463 0-.902.105-1.294.293l4.001 4A3 3 0 0 0 18 15Z"></path></symbol><symbol id="icon-eds-i-user-check-medium" viewBox="0 0 24 24"><path d="M9 1a5 5 0 1 1 0 10A5 5 0 0 1 9 1Zm0 2a3 3 0 1 0 0 6 3 3 0 0 0 0-6Zm13.647 12.237a1 1 0 0 1 .116 1.41l-5.091 6a1 1 0 0 1-1.375.144l-2.909-2.25a1 1 0 1 1 1.224-1.582l2.153 1.665 4.472-5.271a1 1 0 0 1 1.41-.116Zm-8.139-.977c.22.214.428.44.622.678a1 1 0 1 1-1.548 1.266 6.025 6.025 0 0 0-1.795-1.49.86.86 0 0 1-.163-.048l-.079-.036a5.721 5.721 0 0 0-2.62-.63l-.194.006c-2.76.134-5.022 2.177-5.592 4.864l-.035.175-.035.213c-.03.201-.05.405-.06.61L3.003 20 10 20a1 1 0 0 1 .993.883L11 21a1 1 0 0 1-1 1H2a1 1 0 0 1-1-1v-.876l.005-.223.02-.356.02-.222.03-.248.022-.15c.02-.133.044-.265.071-.397.44-2.178 1.725-4.105 3.595-5.301a7.75 7.75 0 0 1 3.755-1.215l.12-.004a7.908 7.908 0 0 1 5.87 2.252Z"></path></symbol><symbol id="icon-eds-i-user-delete-medium" viewBox="0 0 24 24"><path d="M9 1a5 5 0 1 1 0 10A5 5 0 0 1 9 1Zm0 2a3 3 0 1 0 0 6 3 3 0 0 0 0-6ZM4.763 13.227a7.713 7.713 0 0 1 7.692-.378 1 1 0 1 1-.91 1.781 5.713 5.713 0 0 0-5.705.282c-1.67 1.068-2.728 2.927-2.832 4.956L3.004 20H11.5a1 1 0 0 1 .993.883L12.5 21a1 1 0 0 1-1 1H2a1 1 0 0 1-1-1v-.876c.028-2.812 1.446-5.416 3.763-6.897Zm11.421 1.543 2.554 2.553 2.555-2.553a1 1 0 0 1 1.414 1.414l-2.554 2.554 2.554 2.555a1 1 0 0 1-1.414 1.414l-2.555-2.554-2.554 2.554a1 1 0 0 1-1.414-1.414l2.553-2.555-2.553-2.554a1 1 0 0 1 1.414-1.414Z"></path></symbol><symbol id="icon-eds-i-user-edit-medium" viewBox="0 0 24 24"><path d="m19.876 10.77 2.831 2.83a1 1 0 0 1 0 1.415l-7.246 7.246a1 1 0 0 1-.572.284l-3.277.446a1 1 0 0 1-1.125-1.13l.461-3.277a1 1 0 0 1 .283-.567l7.23-7.246a1 1 0 0 1 1.415-.001Zm-7.421 2.08a1 1 0 1 1-.91 1.78 5.713 5.713 0 0 0-5.705.282c-1.67 1.068-2.728 2.927-2.832 4.956L3.004 20 7.5 20a1 1 0 0 1 .993.883L8.5 21a1 1 0 0 1-1 1H2a1 1 0 0 1-1-1v-.876c.028-2.812 1.446-5.416 3.763-6.897a7.713 7.713 0 0 1 7.692-.378Zm6.715.042-6.29 6.3-.23 1.639 1.633-.222 6.302-6.302-1.415-1.415ZM9 1a5 5 0 1 1 0 10A5 5 0 0 1 9 1Zm0 2a3 3 0 1 0 0 6 3 3 0 0 0 0-6Z"></path></symbol><symbol id="icon-eds-i-user-linked-medium" viewBox="0 0 24 24"><path d="M15.65 6c.31 0 .706.066 1.122.274C17.522 6.65 18 7.366 18 8.35v12.3c0 .31-.066.706-.274 1.122-.375.75-1.092 1.228-2.076 1.228H3.35a2.52 2.52 0 0 1-1.122-.274C1.478 22.35 1 21.634 1 20.65V8.35c0-.31.066-.706.274-1.122C1.65 6.478 2.366 6 3.35 6h12.3Zm0 2-12.376.002c-.134.007-.17.04-.21.12A.672.672 0 0 0 3 8.35v12.3c0 .198.028.24.122.287.09.044.2.063.228.063h.887c.788-2.269 2.814-3.5 5.263-3.5 2.45 0 4.475 1.231 5.263 3.5h.887c.198 0 .24-.028.287-.122.044-.09.063-.2.063-.228V8.35c0-.198-.028-.24-.122-.287A.672.672 0 0 0 15.65 8ZM9.5 19.5c-1.36 0-2.447.51-3.06 1.5h6.12c-.613-.99-1.7-1.5-3.06-1.5ZM20.65 1A2.35 2.35 0 0 1 23 3.348V15.65A2.35 2.35 0 0 1 20.65 18H20a1 1 0 0 1 0-2h.65a.35.35 0 0 0 .35-.35V3.348A.35.35 0 0 0 20.65 3H8.35a.35.35 0 0 0-.35.348V4a1 1 0 1 1-2 0v-.652A2.35 2.35 0 0 1 8.35 1h12.3ZM9.5 10a3.5 3.5 0 1 1 0 7 3.5 3.5 0 0 1 0-7Zm0 2a1.5 1.5 0 1 0 0 3 1.5 1.5 0 0 0 0-3Z"></path></symbol><symbol id="icon-eds-i-user-multiple-medium" viewBox="0 0 24 24"><path d="M9 1a5 5 0 1 1 0 10A5 5 0 0 1 9 1Zm6 0a5 5 0 0 1 0 10 1 1 0 0 1-.117-1.993L15 9a3 3 0 0 0 0-6 1 1 0 0 1 0-2ZM9 3a3 3 0 1 0 0 6 3 3 0 0 0 0-6Zm8.857 9.545a7.99 7.99 0 0 1 2.651 1.715A8.31 8.31 0 0 1 23 20.134V21a1 1 0 0 1-1 1h-3a1 1 0 0 1 0-2h1.995l-.005-.153a6.307 6.307 0 0 0-1.673-3.945l-.204-.209a5.99 5.99 0 0 0-1.988-1.287 1 1 0 1 1 .732-1.861Zm-3.349 1.715A8.31 8.31 0 0 1 17 20.134V21a1 1 0 0 1-1 1H2a1 1 0 0 1-1-1v-.877c.044-4.343 3.387-7.908 7.638-8.115a7.908 7.908 0 0 1 5.87 2.252ZM9.016 14l-.285.006c-3.104.15-5.58 2.718-5.725 5.9L3.004 20h11.991l-.005-.153a6.307 6.307 0 0 0-1.673-3.945l-.204-.209A5.924 5.924 0 0 0 9.3 14.008L9.016 14Z"></path></symbol><symbol id="icon-eds-i-user-notify-medium" viewBox="0 0 24 24"><path d="M9 1a5 5 0 1 1 0 10A5 5 0 0 1 9 1Zm0 2a3 3 0 1 0 0 6 3 3 0 0 0 0-6Zm10 18v1a1 1 0 0 1-2 0v-1h-3a1 1 0 0 1 0-2v-2.818C14 13.885 15.777 12 18 12s4 1.885 4 4.182V19a1 1 0 0 1 0 2h-3Zm-6.545-8.15a1 1 0 1 1-.91 1.78 5.713 5.713 0 0 0-5.705.282c-1.67 1.068-2.728 2.927-2.832 4.956L3.004 20 11.5 20a1 1 0 0 1 .993.883L12.5 21a1 1 0 0 1-1 1H2a1 1 0 0 1-1-1v-.876c.028-2.812 1.446-5.416 3.763-6.897a7.713 7.713 0 0 1 7.692-.378ZM18 14c-1.091 0-2 .964-2 2.182V19h4v-2.818c0-1.165-.832-2.098-1.859-2.177L18 14Z"></path></symbol><symbol id="icon-eds-i-user-remove-medium" viewBox="0 0 24 24"><path d="M9 1a5 5 0 1 1 0 10A5 5 0 0 1 9 1Zm0 2a3 3 0 1 0 0 6 3 3 0 0 0 0-6Zm3.455 9.85a1 1 0 1 1-.91 1.78 5.713 5.713 0 0 0-5.705.282c-1.67 1.068-2.728 2.927-2.832 4.956L3.004 20 11.5 20a1 1 0 0 1 .993.883L12.5 21a1 1 0 0 1-1 1H2a1 1 0 0 1-1-1v-.876c.028-2.812 1.446-5.416 3.763-6.897a7.713 7.713 0 0 1 7.692-.378ZM22 17a1 1 0 0 1 0 2h-8a1 1 0 0 1 0-2h8Z"></path></symbol><symbol id="icon-eds-i-user-single-medium" viewBox="0 0 24 24"><path d="M12 1a5 5 0 1 1 0 10 5 5 0 0 1 0-10Zm0 2a3 3 0 1 0 0 6 3 3 0 0 0 0-6Zm-.406 9.008a8.965 8.965 0 0 1 6.596 2.494A9.161 9.161 0 0 1 21 21.025V22a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1v-.985c.05-4.825 3.815-8.777 8.594-9.007Zm.39 1.992-.299.006c-3.63.175-6.518 3.127-6.678 6.775L5 21h13.998l-.009-.268a7.157 7.157 0 0 0-1.97-4.573l-.214-.213A6.967 6.967 0 0 0 11.984 14Z"></path></symbol><symbol id="icon-eds-i-warning-circle-medium" viewBox="0 0 24 24"><path d="M12 1c6.075 0 11 4.925 11 11s-4.925 11-11 11S1 18.075 1 12 5.925 1 12 1Zm0 2a9 9 0 1 0 0 18 9 9 0 0 0 0-18Zm0 11.5a1.5 1.5 0 0 1 .144 2.993L12 17.5a1.5 1.5 0 0 1 0-3ZM12 6a1 1 0 0 1 1 1v5a1 1 0 0 1-2 0V7a1 1 0 0 1 1-1Z"></path></symbol><symbol id="icon-eds-i-warning-filled-medium" viewBox="0 0 24 24"><path d="M12 1c6.075 0 11 4.925 11 11s-4.925 11-11 11S1 18.075 1 12 5.925 1 12 1Zm0 13.5a1.5 1.5 0 0 0 0 3l.144-.007A1.5 1.5 0 0 0 12 14.5ZM12 6a1 1 0 0 0-1 1v5a1 1 0 0 0 2 0V7a1 1 0 0 0-1-1Z"></path></symbol><symbol id="icon-ai" viewBox="-4.807 394.815 109.546 109.923"><g transform="translate(-1321.07 -292.074) scale(2.96829)"><a><path stroke="url(#a)" stroke-width="2" d="m469.451 241.994-4.593-7.217c-1.376-2.161-4.53-2.161-5.906 0l-4.593 7.217a1.507 1.507 0 0 1-.46.46l-7.217 4.593c-2.141 1.534-2.084 4.397 0 5.906l7.217 4.593c.185.118.342.275.46.46l4.593 7.217c1.376 2.161 4.438 2.119 5.906 0l4.593-7.217c.116-.19.275-.342.46-.46l7.217-4.593c2.166-1.451 2.161-4.53 0-5.906l-7.217-4.593a1.507 1.507 0 0 1-.46-.46Z" style="transform-origin:461.906px 249.992px"></path><path d="m468.608 242.533-4.302-6.801c-1.258-2.183-3.581-2.168-4.729-.124l-4.463 7.013a2.187 2.187 0 0 1-.489.53l-6.748 4.303c-2.467 1.36-2.325 3.704-.132 5.031l6.831 4.385c.174.113.472.357.582.557l4.356 6.858c1.251 2.333 3.693 2.026 4.894-.205l4.375-6.854c.15-.205.513-.477.705-.596l6.703-4.26c2.288-1.321 2.176-3.548.038-4.714l-6.836-4.347a2.584 2.584 0 0 1-.647-.577" style="stroke-width:0;paint-order:stroke;transform-origin:461.906px 249.992px;fill:#f0f7fc"></path></a><g style="transform-origin:461.359px 249.886px"><path fill-rule="evenodd" d="M456.406 254.106h2.145l.43-1.529h2.844l.455 1.53h2.167l-2.87-8.229h-2.322l-2.849 8.228Zm4.994-2.959-.759-2.552-.264-1.067-.275 1.067-.718 2.552h2.016Z" clip-rule="evenodd"></path><path d="M465.337 245.932v8.173h2.068v-8.173h-2.068Z"></path></g></g></symbol><symbol id="icon-chevron-left-medium" viewBox="0 0 24 24"><path d="M15.7194 3.3054C15.3358 2.90809 14.7027 2.89699 14.3054 3.28061L6.54342 10.7757C6.19804 11.09 6 11.5335 6 12C6 12.4665 6.19804 12.91 6.5218 13.204L14.3054 20.7194C14.7027 21.103 15.3358 21.0919 15.7194 20.6946C16.103 20.2973 16.0919 19.6642 15.6946 19.2806L8.155 12L15.6946 4.71939C16.0614 4.36528 16.099 3.79863 15.8009 3.40105L15.7194 3.3054Z"></path></symbol><symbol id="icon-chevron-right-medium" viewBox="0 0 24 24"><path d="M8.28061 3.3054C8.66423 2.90809 9.29729 2.89699 9.6946 3.28061L17.4566 10.7757C17.802 11.09 18 11.5335 18 12C18 12.4665 17.802 12.91 17.4782 13.204L9.6946 20.7194C9.29729 21.103 8.66423 21.0919 8.28061 20.6946C7.89699 20.2973 7.90809 19.6642 8.3054 19.2806L15.845 12L8.3054 4.71939C7.93865 4.36528 7.90098 3.79863 8.19908 3.40105L8.28061 3.3054Z"></path></symbol><symbol id="icon-eds-alerts" viewBox="0 0 32 32"><path d="M28 12.667c.736 0 1.333.597 1.333 1.333v13.333A3.333 3.333 0 0 1 26 30.667H6a3.333 3.333 0 0 1-3.333-3.334V14a1.333 1.333 0 1 1 2.666 0v1.252L16 21.769l10.667-6.518V14c0-.736.597-1.333 1.333-1.333Zm-1.333 5.71-9.972 6.094c-.427.26-.963.26-1.39 0l-9.972-6.094v8.956c0 .368.299.667.667.667h20a.667.667 0 0 0 .667-.667v-8.956ZM19.333 12a1.333 1.333 0 1 1 0 2.667h-6.666a1.333 1.333 0 1 1 0-2.667h6.666Zm4-10.667a3.333 3.333 0 0 1 3.334 3.334v6.666a1.333 1.333 0 1 1-2.667 0V4.667A.667.667 0 0 0 23.333 4H8.667A.667.667 0 0 0 8 4.667v6.666a1.333 1.333 0 1 1-2.667 0V4.667a3.333 3.333 0 0 1 3.334-3.334h14.666Zm-4 5.334a1.333 1.333 0 0 1 0 2.666h-6.666a1.333 1.333 0 1 1 0-2.666h6.666Z"></path></symbol><symbol id="icon-eds-arrow-up" viewBox="0 0 24 24"><path fill-rule="evenodd" d="m13.002 7.408 4.88 4.88a.99.99 0 0 0 1.32.08l.09-.08c.39-.39.39-1.03 0-1.42l-6.58-6.58a1.01 1.01 0 0 0-1.42 0l-6.58 6.58a1 1 0 0 0-.09 1.32l.08.1a1 1 0 0 0 1.42-.01l4.88-4.87v11.59a.99.99 0 0 0 .88.99l.12.01c.55 0 1-.45 1-1V7.408z" class="layer"></path></symbol><symbol id="icon-eds-checklist" viewBox="0 0 32 32"><path d="M19.2 1.333a3.468 3.468 0 0 1 3.381 2.699L24.667 4C26.515 4 28 5.52 28 7.38v19.906c0 1.86-1.485 3.38-3.333 3.38H7.333c-1.848 0-3.333-1.52-3.333-3.38V7.38C4 5.52 5.485 4 7.333 4h2.093A3.468 3.468 0 0 1 12.8 1.333h6.4ZM9.426 6.667H7.333c-.36 0-.666.312-.666.713v19.906c0 .401.305.714.666.714h17.334c.36 0 .666-.313.666-.714V7.38c0-.4-.305-.713-.646-.714l-2.121.033A3.468 3.468 0 0 1 19.2 9.333h-6.4a3.468 3.468 0 0 1-3.374-2.666Zm12.715 5.606c.586.446.7 1.283.253 1.868l-7.111 9.334a1.333 1.333 0 0 1-1.792.306l-3.556-2.333a1.333 1.333 0 1 1 1.463-2.23l2.517 1.651 6.358-8.344a1.333 1.333 0 0 1 1.868-.252ZM19.2 4h-6.4a.8.8 0 0 0-.8.8v1.067a.8.8 0 0 0 .8.8h6.4a.8.8 0 0 0 .8-.8V4.8a.8.8 0 0 0-.8-.8Z"></path></symbol><symbol id="icon-eds-citation" viewBox="0 0 36 36"><path d="M23.25 1.5a1.5 1.5 0 0 1 1.06.44l8.25 8.25a1.5 1.5 0 0 1 .44 1.06v19.5c0 2.105-1.645 3.75-3.75 3.75H18a1.5 1.5 0 0 1 0-3h11.25c.448 0 .75-.302.75-.75V11.873L22.628 4.5H8.31a.811.811 0 0 0-.8.68l-.011.13V16.5a1.5 1.5 0 0 1-3 0V5.31A3.81 3.81 0 0 1 8.31 1.5h14.94ZM8.223 20.358a.984.984 0 0 1-.192 1.378l-.048.034c-.54.36-.942.676-1.206.951-.59.614-.885 1.395-.885 2.343.115-.028.288-.042.518-.042.662 0 1.26.237 1.791.711.533.474.799 1.074.799 1.799 0 .753-.259 1.352-.777 1.799-.518.446-1.151.669-1.9.669-1.006 0-1.812-.293-2.417-.878C3.302 28.536 3 27.657 3 26.486c0-1.115.165-2.085.496-2.907.331-.823.734-1.513 1.209-2.071.475-.558.971-.997 1.49-1.318a6.01 6.01 0 0 1 .347-.2 1.321 1.321 0 0 1 1.681.368Zm7.5 0a.984.984 0 0 1-.192 1.378l-.048.034c-.54.36-.942.676-1.206.951-.59.614-.885 1.395-.885 2.343.115-.028.288-.042.518-.042.662 0 1.26.237 1.791.711.533.474.799 1.074.799 1.799 0 .753-.259 1.352-.777 1.799-.518.446-1.151.669-1.9.669-1.006 0-1.812-.293-2.417-.878-.604-.586-.906-1.465-.906-2.636 0-1.115.165-2.085.496-2.907.331-.823.734-1.513 1.209-2.071.475-.558.971-.997 1.49-1.318a6.01 6.01 0 0 1 .347-.2 1.321 1.321 0 0 1 1.681.368Z"></path></symbol><symbol id="icon-eds-i-access-indicator" viewBox="0 0 16 16"><circle cx="4.5" cy="11.5" r="3.5" style="fill:currentColor"></circle><path fill-rule="evenodd" d="M4 3v3a1 1 0 0 1-2 0V2.923C2 1.875 2.84 1 3.909 1h5.909a1 1 0 0 1 .713.298l3.181 3.231a1 1 0 0 1 .288.702v7.846c0 .505-.197.993-.554 1.354a1.902 1.902 0 0 1-1.355.569H10a1 1 0 1 1 0-2h2V5.64L9.4 3H4Z" clip-rule="evenodd" style="fill:#222"></path></symbol><symbol id="icon-eds-i-copy-link" viewBox="0 0 24 24"><path fill-rule="evenodd" clip-rule="evenodd" d="M19.4594 8.57015C19.0689 8.17963 19.0689 7.54646 19.4594 7.15594L20.2927 6.32261C20.2927 6.32261 20.2927 6.32261 20.2927 6.32261C21.0528 5.56252 21.0528 4.33019 20.2928 3.57014C19.5327 2.81007 18.3004 2.81007 17.5404 3.57014L16.7071 4.40347C16.3165 4.794 15.6834 4.794 15.2928 4.40348C14.9023 4.01296 14.9023 3.3798 15.2928 2.98927L16.1262 2.15594C17.6673 0.614803 20.1659 0.614803 21.707 2.15593C23.2481 3.69705 23.248 6.19569 21.707 7.7368L20.8737 8.57014C20.4831 8.96067 19.85 8.96067 19.4594 8.57015Z"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M18.0944 5.90592C18.4849 6.29643 18.4849 6.9296 18.0944 7.32013L16.4278 8.9868C16.0373 9.37733 15.4041 9.37734 15.0136 8.98682C14.6231 8.59631 14.6231 7.96314 15.0136 7.57261L16.6802 5.90594C17.0707 5.51541 17.7039 5.5154 18.0944 5.90592Z"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M13.5113 6.32243C13.9018 6.71295 13.9018 7.34611 13.5113 7.73664L12.678 8.56997C12.678 8.56997 12.678 8.56997 12.678 8.56997C11.9179 9.33006 11.9179 10.5624 12.6779 11.3224C13.438 12.0825 14.6703 12.0825 15.4303 11.3224L16.2636 10.4891C16.6542 10.0986 17.2873 10.0986 17.6779 10.4891C18.0684 10.8796 18.0684 11.5128 17.6779 11.9033L16.8445 12.7366C15.3034 14.2778 12.8048 14.2778 11.2637 12.7366C9.72262 11.1955 9.72266 8.69689 11.2637 7.15578L12.097 6.32244C12.4876 5.93191 13.1207 5.93191 13.5113 6.32243Z"></path><path d="M8 20V22H19.4619C20.136 22 20.7822 21.7311 21.2582 21.2529C21.7333 20.7757 22 20.1289 22 19.4549V15C22 14.4477 21.5523 14 21 14C20.4477 14 20 14.4477 20 15V19.4549C20 19.6004 19.9426 19.7397 19.8408 19.842C19.7399 19.9433 19.6037 20 19.4619 20H8Z"></path><path d="M4 13H2V19.4619C2 20.136 2.26889 20.7822 2.74705 21.2582C3.22434 21.7333 3.87105 22 4.5451 22H9C9.55228 22 10 21.5523 10 21C10 20.4477 9.55228 20 9 20H4.5451C4.39957 20 4.26028 19.9426 4.15804 19.8408C4.05668 19.7399 4 19.6037 4 19.4619V13Z"></path><path d="M4 13H2V4.53808C2 3.86398 2.26889 3.21777 2.74705 2.74178C3.22434 2.26666 3.87105 2 4.5451 2H9C9.55228 2 10 2.44772 10 3C10 3.55228 9.55228 4 9 4H4.5451C4.39957 4 4.26028 4.05743 4.15804 4.15921C4.05668 4.26011 4 4.39633 4 4.53808V13Z"></path></symbol><symbol id="icon-eds-i-github-medium" viewBox="0 0 24 24"><path d="M 11.964844 0 C 5.347656 0 0 5.269531 0 11.792969 C 0 17.003906 3.425781 21.417969 8.179688 22.976562 C 8.773438 23.09375 8.992188 22.722656 8.992188 22.410156 C 8.992188 22.136719 8.972656 21.203125 8.972656 20.226562 C 5.644531 20.929688 4.953125 18.820312 4.953125 18.820312 C 4.417969 17.453125 3.625 17.101562 3.625 17.101562 C 2.535156 16.378906 3.703125 16.378906 3.703125 16.378906 C 4.914062 16.457031 5.546875 17.589844 5.546875 17.589844 C 6.617188 19.386719 8.339844 18.878906 9.03125 18.566406 C 9.132812 17.804688 9.449219 17.277344 9.785156 16.984375 C 7.132812 16.710938 4.339844 15.695312 4.339844 11.167969 C 4.339844 9.878906 4.8125 8.824219 5.566406 8.003906 C 5.445312 7.710938 5.03125 6.5 5.683594 4.878906 C 5.683594 4.878906 6.695312 4.566406 8.972656 6.089844 C 9.949219 5.832031 10.953125 5.703125 11.964844 5.699219 C 12.972656 5.699219 14.003906 5.835938 14.957031 6.089844 C 17.234375 4.566406 18.242188 4.878906 18.242188 4.878906 C 18.898438 6.5 18.480469 7.710938 18.363281 8.003906 C 19.136719 8.824219 19.589844 9.878906 19.589844 11.167969 C 19.589844 15.695312 16.796875 16.691406 14.125 16.984375 C 14.558594 17.355469 14.933594 18.058594 14.933594 19.171875 C 14.933594 20.753906 14.914062 22.019531 14.914062 22.410156 C 14.914062 22.722656 15.132812 23.09375 15.726562 22.976562 C 20.480469 21.414062 23.910156 17.003906 23.910156 11.792969 C 23.929688 5.269531 18.558594 0 11.964844 0 Z M 11.964844 0 "></path></symbol><symbol id="icon-eds-i-institution-medium" viewBox="0 0 24 24"><g><path fill-rule="evenodd" clip-rule="evenodd" d="M11.9967 1C11.6364 1 11.279 1.0898 10.961 1.2646C10.9318 1.28061 10.9035 1.29806 10.8761 1.31689L2.79765 6.87C2.46776 7.08001 2.20618 7.38466 2.07836 7.76668C1.94823 8.15561 1.98027 8.55648 2.12665 8.90067C2.42086 9.59246 3.12798 10 3.90107 10H4.99994V16H4.49994C3.11923 16 1.99994 17.1193 1.99994 18.5V19.5C1.99994 20.8807 3.11923 22 4.49994 22H19.4999C20.8807 22 21.9999 20.8807 21.9999 19.5V18.5C21.9999 17.1193 20.8807 16 19.4999 16H18.9999V10H20.0922C20.8653 10 21.5725 9.59252 21.8667 8.90065C22.0131 8.55642 22.0451 8.15553 21.9149 7.7666C21.7871 7.38459 21.5255 7.07997 21.1956 6.86998L13.1172 1.31689C13.0898 1.29806 13.0615 1.28061 13.0324 1.2646C12.7143 1.0898 12.357 1 11.9967 1ZM4.6844 8L11.9472 3.00755C11.9616 3.00295 11.9783 3 11.9967 3C12.015 3 12.0318 3.00295 12.0461 3.00755L19.3089 8H4.6844ZM16.9999 16V10H14.9999V16H16.9999ZM12.9999 16V10H10.9999V16H12.9999ZM8.99994 16V10H6.99994V16H8.99994ZM3.99994 18.5C3.99994 18.2239 4.2238 18 4.49994 18H19.4999C19.7761 18 19.9999 18.2239 19.9999 18.5V19.5C19.9999 19.7761 19.7761 20 19.4999 20H4.49994C4.2238 20 3.99994 19.7761 3.99994 19.5V18.5Z"></path></g></symbol><symbol id="icon-eds-i-limited-access" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 3v3a1 1 0 0 1-2 0V2.923C2 1.875 2.84 1 3.909 1h5.909a1 1 0 0 1 .713.298l3.181 3.231a1 1 0 0 1 .288.702V6a1 1 0 1 1-2 0v-.36L9.4 3H4ZM3 8a1 1 0 0 1 1 1v1a1 1 0 1 1-2 0V9a1 1 0 0 1 1-1Zm10 0a1 1 0 0 1 1 1v1a1 1 0 1 1-2 0V9a1 1 0 0 1 1-1Zm-3.5 6a1 1 0 0 1-1 1h-1a1 1 0 1 1 0-2h1a1 1 0 0 1 1 1Zm2.441-1a1 1 0 0 1 2 0c0 .73-.246 1.306-.706 1.664a1.61 1.61 0 0 1-.876.334l-.032.002H11.5a1 1 0 1 1 0-2h.441ZM4 13a1 1 0 0 0-2 0c0 .73.247 1.306.706 1.664a1.609 1.609 0 0 0 .876.334l.032.002H4.5a1 1 0 1 0 0-2H4Z" clip-rule="evenodd"></path></symbol><symbol id="icon-eds-i-rss" viewBox="0 0 22 22"><path d="M1.96094 1C1.96094 0.447715 2.40865 0 2.96094 0C5.46109 0 7.93678 0.492038 10.2467 1.44806C12.5565 2.40407 14.6554 3.80534 16.4234 5.57189C18.1913 7.33843 19.5939 9.4357 20.5508 11.744C21.5077 14.0522 22.0001 16.5263 22.0001 19.0247C22.0001 19.577 21.5524 20.0247 21.0001 20.0247C20.4478 20.0247 20.0001 19.577 20.0001 19.0247C20.0001 16.7891 19.5595 14.5753 18.7033 12.5098C17.8471 10.4444 16.5919 8.56762 15.0097 6.98666C13.4275 5.40575 11.5492 4.15167 9.48182 3.29604C7.41447 2.4404 5.19868 2 2.96094 2C2.40865 2 1.96094 1.55228 1.96094 1Z"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M0 18.649C0 16.7974 1.50196 15.298 3.35294 15.298C5.20392 15.298 6.70588 16.7974 6.70588 18.649C6.70588 20.5003 5.20397 22 3.35294 22C1.50191 22 0 20.5003 0 18.649ZM3.35294 17.298C2.60493 17.298 2 17.9036 2 18.649C2 19.3943 2.60498 20 3.35294 20C4.1009 20 4.70588 19.3943 4.70588 18.649C4.70588 17.9036 4.10095 17.298 3.35294 17.298Z"></path><path d="M3.3374 7.46115C2.78512 7.46115 2.3374 7.90887 2.3374 8.46115C2.3374 9.01344 2.78512 9.46115 3.3374 9.46115C4.54515 9.46115 5.74107 9.69885 6.85684 10.1606C7.97262 10.6224 8.98639 11.2993 9.84028 12.1525C10.6942 13.0057 11.3715 14.0185 11.8336 15.1332C12.2956 16.2478 12.5335 17.4424 12.5335 18.649C12.5335 19.2013 12.9812 19.649 13.5335 19.649C14.0858 19.649 14.5335 19.2013 14.5335 18.649C14.5335 17.1796 14.2438 15.7247 13.6811 14.3673C13.1184 13.0099 12.2936 11.7765 11.2539 10.7377C10.2142 9.69885 8.97999 8.87484 7.62168 8.31266C6.26337 7.75049 4.80757 7.46115 3.3374 7.46115Z"></path></symbol><symbol id="icon-eds-i-search-category-medium" viewBox="0 0 32 32"><path fill-rule="evenodd" d="M2 5.306A3.306 3.306 0 0 1 5.306 2h5.833a3.306 3.306 0 0 1 3.306 3.306v5.833a3.306 3.306 0 0 1-3.306 3.305H5.306A3.306 3.306 0 0 1 2 11.14V5.306Zm3.306-.584a.583.583 0 0 0-.584.584v5.833c0 .322.261.583.584.583h5.833a.583.583 0 0 0 .583-.583V5.306a.583.583 0 0 0-.583-.584H5.306Zm15.555 8.945a7.194 7.194 0 1 0 4.034 13.153l2.781 2.781a1.361 1.361 0 1 0 1.925-1.925l-2.781-2.781a7.194 7.194 0 0 0-5.958-11.228Zm3.173 10.346a4.472 4.472 0 1 0-.021.021l.01-.01.011-.011Zm-5.117-19.29a.583.583 0 0 0-.584.583v5.833a1.361 1.361 0 0 1-2.722 0V5.306A3.306 3.306 0 0 1 18.917 2h5.833a3.306 3.306 0 0 1 3.306 3.306v5.833c0 .6-.161 1.166-.443 1.654a1.361 1.361 0 1 1-2.357-1.363.575.575 0 0 0 .078-.291V5.306a.583.583 0 0 0-.584-.584h-5.833ZM2 18.916a3.306 3.306 0 0 1 3.306-3.306h5.833a1.361 1.361 0 1 1 0 2.722H5.306a.583.583 0 0 0-.584.584v5.833c0 .322.261.583.584.583h5.833a.574.574 0 0 0 .29-.077 1.361 1.361 0 1 1 1.364 2.356 3.296 3.296 0 0 1-1.654.444H5.306A3.306 3.306 0 0 1 2 24.75v-5.833Z" clip-rule="evenodd"></path></symbol><symbol id="icon-eds-i-subjects-medium" viewBox="0 0 24 24"><g id="icon-subjects-copy" stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M13.3846154,2 C14.7015971,2 15.7692308,3.06762994 15.7692308,4.38461538 L15.7692308,7.15384615 C15.7692308,8.47082629 14.7015955,9.53846154 13.3846154,9.53846154 L13.1038388,9.53925278 C13.2061091,9.85347965 13.3815528,10.1423885 13.6195822,10.3804178 C13.9722182,10.7330539 14.436524,10.9483278 14.9293854,10.9918129 L15.1153846,11 C16.2068332,11 17.2535347,11.433562 18.0254647,12.2054189 C18.6411944,12.8212361 19.0416785,13.6120766 19.1784166,14.4609738 L19.6153846,14.4615385 C20.932386,14.4615385 22,15.5291672 22,16.8461538 L22,19.6153846 C22,20.9323924 20.9323924,22 19.6153846,22 L16.8461538,22 C15.5291672,22 14.4615385,20.932386 14.4615385,19.6153846 L14.4615385,16.8461538 C14.4615385,15.5291737 15.5291737,14.4615385 16.8461538,14.4615385 L17.126925,14.460779 C17.0246537,14.1465537 16.8492179,13.857633 16.6112344,13.6196157 C16.2144418,13.2228606 15.6764136,13 15.1153846,13 C14.0239122,13 12.9771569,12.5664197 12.2053686,11.7946314 C12.1335167,11.7227795 12.0645962,11.6485444 11.9986839,11.5721119 C11.9354038,11.6485444 11.8664833,11.7227795 11.7946314,11.7946314 C11.0228431,12.5664197 9.97608778,13 8.88461538,13 C8.323576,13 7.78552852,13.2228666 7.38881294,13.6195822 C7.15078359,13.8576115 6.97533988,14.1465203 6.8730696,14.4607472 L7.15384615,14.4615385 C8.47082629,14.4615385 9.53846154,15.5291737 9.53846154,16.8461538 L9.53846154,19.6153846 C9.53846154,20.932386 8.47083276,22 7.15384615,22 L4.38461538,22 C3.06762347,22 2,20.9323876 2,19.6153846 L2,16.8461538 C2,15.5291721 3.06762994,14.4615385 4.38461538,14.4615385 L4.8215823,14.4609378 C4.95831893,13.6120029 5.3588057,12.8211623 5.97459937,12.2053686 C6.69125996,11.488708 7.64500941,11.0636656 8.6514968,11.0066017 L8.88461538,11 C9.44565477,11 9.98370225,10.7771334 10.3804178,10.3804178 C10.6184472,10.1423885 10.7938909,9.85347965 10.8961612,9.53925278 L10.6153846,9.53846154 C9.29840448,9.53846154 8.23076923,8.47082629 8.23076923,7.15384615 L8.23076923,4.38461538 C8.23076923,3.06762994 9.29840286,2 10.6153846,2 L13.3846154,2 Z M7.15384615,16.4615385 L4.38461538,16.4615385 C4.17220099,16.4615385 4,16.63374 4,16.8461538 L4,19.6153846 C4,19.8278134 4.17218833,20 4.38461538,20 L7.15384615,20 C7.36626945,20 7.53846154,19.8278103 7.53846154,19.6153846 L7.53846154,16.8461538 C7.53846154,16.6337432 7.36625679,16.4615385 7.15384615,16.4615385 Z M19.6153846,16.4615385 L16.8461538,16.4615385 C16.6337432,16.4615385 16.4615385,16.6337432 16.4615385,16.8461538 L16.4615385,19.6153846 C16.4615385,19.8278103 16.6337306,20 16.8461538,20 L19.6153846,20 C19.8278229,20 20,19.8278229 20,19.6153846 L20,16.8461538 C20,16.6337306 19.8278103,16.4615385 19.6153846,16.4615385 Z M13.3846154,4 L10.6153846,4 C10.4029708,4 10.2307692,4.17220099 10.2307692,4.38461538 L10.2307692,7.15384615 C10.2307692,7.36625679 10.402974,7.53846154 10.6153846,7.53846154 L13.3846154,7.53846154 C13.597026,7.53846154 13.7692308,7.36625679 13.7692308,7.15384615 L13.7692308,4.38461538 C13.7692308,4.17220099 13.5970292,4 13.3846154,4 Z" id="Shape" fill-rule="nonzero"></path></g></symbol><symbol id="icon-eds-small-arrow-left" viewBox="0 0 16 17"><path stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M14 8.092H2m0 0L8 2M2 8.092l6 6.035"></path></symbol><symbol id="icon-eds-small-arrow-right" viewBox="0 0 16 16"><g fill-rule="evenodd" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2"><path d="M2 8.092h12M8 2l6 6.092M8 14.127l6-6.035"></path></g></symbol><symbol id="icon-orcid-logo" viewBox="0 0 40 40"><path fill-rule="evenodd" d="M12.281 10.453c.875 0 1.578-.719 1.578-1.578 0-.86-.703-1.578-1.578-1.578-.875 0-1.578.703-1.578 1.578 0 .86.703 1.578 1.578 1.578Zm-1.203 18.641h2.406V12.359h-2.406v16.735Z"></path><path fill-rule="evenodd" d="M17.016 12.36h6.5c6.187 0 8.906 4.421 8.906 8.374 0 4.297-3.36 8.375-8.875 8.375h-6.531V12.36Zm6.234 14.578h-3.828V14.53h3.703c4.688 0 6.828 2.844 6.828 6.203 0 2.063-1.25 6.203-6.703 6.203Z" clip-rule="evenodd"></path></symbol></svg>
</div>


        

        
        
    <a class="c-skip-link" href="#main">Skip to main content</a>

    

    <header class="eds-c-header" data-eds-c-header="">
    <div class="eds-c-header__container" data-eds-c-header-expander-anchor="">
        <div class="eds-c-header__brand">
            
                
                    <a href="https://link.springer.com" data-test="springerlink-logo" data-track="click_imprint_logo" data-track-context="unified header" data-track-action="click logo link" data-track-category="unified header" data-track-label="link">
                        <img src="/oscar-static/images/darwin/header/img/logo-springer-nature-link-3149409f62.svg" alt="Springer Nature Link">
                    </a>
                
            
        </div>

        
            
                
    
        <a class="c-header__link eds-c-header__link" id="identity-account-widget" data-track="click_login" data-track-context="header" href="https://idp.springer.com/auth/personal/springernature?redirect_uri=https://link.springer.com/article/10.1186/s12880-022-00818-1"><span class="eds-c-header__widget-fragment-title">Log in</span></a>
    


            
        
    </div>

    
        <nav class="eds-c-header__nav" aria-label="header navigation">
            <div class="eds-c-header__nav-container">
                <div class="eds-c-header__item eds-c-header__item--menu">
                   <a href="javascript:;" class="eds-c-header__link" data-eds-c-header-expander="" role="button" aria-controls="eds-c-header-nav" aria-haspopup="true" aria-expanded="false">
                        <svg class="eds-c-header__icon" width="24" height="24" aria-hidden="true" focusable="false">
                            <use xlink:href="#icon-eds-i-menu-medium"></use>
                        </svg><span>Menu</span>
                    </a>
                </div>

                <div class="eds-c-header__item eds-c-header__item--inline-links">
                    
                        <a class="eds-c-header__link" href="https://link.springer.com/journals/" data-track="nav_find_a_journal" data-track-context="unified header" data-track-action="click find a journal" data-track-category="unified header" data-track-label="link">
                            Find a journal
                        </a>
                    
                        <a class="eds-c-header__link" href="https://www.springernature.com/gp/authors" data-track="nav_how_to_publish" data-track-context="unified header" data-track-action="click publish with us link" data-track-category="unified header" data-track-label="link">
                            Publish with us
                        </a>
                    
                        <a class="eds-c-header__link" href="https://link.springernature.com/home/" data-track="nav_track_your_research" data-track-context="unified header" data-track-action="click track your research" data-track-category="unified header" data-track-label="link">
                            Track your research
                        </a>
                    
                </div>

                <div class="eds-c-header__link-container">
                    
                        <div class="eds-c-header__item eds-c-header__item--divider">
                            <a href="javascript:;" class="eds-c-header__link" data-eds-c-header-expander="" data-eds-c-header-test-search-btn="" role="button" aria-controls="eds-c-header-popup-search" aria-haspopup="true" aria-expanded="false">
                                <svg class="eds-c-header__icon" width="24" height="24" aria-hidden="true" focusable="false">
                                    <use xlink:href="#icon-eds-i-search-medium"></use>
                                </svg><span>Search</span>
                            </a>
                        </div>
                    
                    
                        <div id="ecommerce-header-cart-icon-link" class="eds-c-header__item ecommerce-cart" style="display:inline-block">
 <a class="eds-c-header__link" href="https://order.springer.com/public/cart" style="appearance:none;border:none;background:none;color:inherit;position:relative">
  <svg id="eds-i-cart" class="eds-c-header__icon" xmlns="http://www.w3.org/2000/svg" height="24" width="24" viewBox="0 0 24 24" aria-hidden="true" focusable="false">
   <path fill="currentColor" fill-rule="nonzero" d="M2 1a1 1 0 0 0 0 2l1.659.001 2.257 12.808a2.599 2.599 0 0 0 2.435 2.185l.167.004 9.976-.001a2.613 2.613 0 0 0 2.61-1.748l.03-.106 1.755-7.82.032-.107a2.546 2.546 0 0 0-.311-1.986l-.108-.157a2.604 2.604 0 0 0-2.197-1.076L6.042 5l-.56-3.17a1 1 0 0 0-.864-.82l-.12-.007L2.001 1ZM20.35 6.996a.63.63 0 0 1 .54.26.55.55 0 0 1 .082.505l-.028.1L19.2 15.63l-.022.05c-.094.177-.282.299-.526.317l-10.145.002a.61.61 0 0 1-.618-.515L6.394 6.999l13.955-.003ZM18 19a2 2 0 1 0 0 4 2 2 0 0 0 0-4ZM8 19a2 2 0 1 0 0 4 2 2 0 0 0 0-4Z"></path>
  </svg><span>Cart</span><span class="cart-info" style="display:none;position:absolute;top:10px;right:45px;background-color:#C65301;color:#fff;width:18px;height:18px;font-size:11px;border-radius:50%;line-height:17.5px;text-align:center"></span></a>
 <script>(function () { var exports = {}; if (window.fetch) {
            
            "use strict";
Object.defineProperty(exports, "__esModule", { value: true });
exports.headerWidgetClientInit = void 0;
var headerWidgetClientInit = function (getCartInfo) {
    document.body.addEventListener("updatedCart", function () {
        updateCartIcon();
    }, false);
    return updateCartIcon();
    function updateCartIcon() {
        return getCartInfo()
            .then(function (res) { return res.json(); })
            .then(refreshCartState)
            .catch(function (_) { });
    }
    function refreshCartState(json) {
        var indicator = document.querySelector("#ecommerce-header-cart-icon-link .cart-info");
        /* istanbul ignore else */
        if (indicator && json.itemCount) {
            indicator.style.display = 'block';
            indicator.textContent = json.itemCount > 9 ? '9+' : json.itemCount.toString();
            var moreThanOneItem = json.itemCount > 1;
            indicator.setAttribute('title', "there ".concat(moreThanOneItem ? "are" : "is", " ").concat(json.itemCount, " item").concat(moreThanOneItem ? "s" : "", " in your cart"));
        }
        return json;
    }
};
exports.headerWidgetClientInit = headerWidgetClientInit;

            
            headerWidgetClientInit(
              function () {
                return window.fetch("https://cart.springer.com/cart-info", {
                  credentials: "include",
                  headers: { Accept: "application/json" }
                })
              }
            )
        }})()</script>
</div>
                    
                </div>
            </div>
        </nav>
    
</header><div class="eds-c-header__expander eds-c-header__expander--search has-tethered u-js-hide" id="eds-c-header-popup-search" hidden="">
            <h2 class="eds-c-header__heading">Search</h2>
            <div class="u-container">
                <search class="eds-c-header__search" role="search" aria-label="Search from the header">
                    <form method="GET" action="//link.springer.com/search" data-test="header-search" data-track="search" data-track-context="search from header" data-track-action="submit search form" data-track-category="unified header" data-track-label="form">
                        <label for="eds-c-header-search" class="eds-c-header__search-label">Search by keyword or author</label>
                        <div class="eds-c-header__search-container">
                            <input id="eds-c-header-search" class="eds-c-header__search-input" autocomplete="off" name="query" type="search" value="" required="">
                            <button class="eds-c-header__search-button" type="submit">
                                <svg class="eds-c-header__icon" aria-hidden="true" focusable="false">
                                    <use xlink:href="#icon-eds-i-search-medium"></use>
                                </svg>
                                <span class="u-visually-hidden">Search</span>
                            </button>
                        </div>
                    </form>
                </search>
            </div>
        </div><div class="eds-c-header__expander eds-c-header__expander--menu has-tethered u-js-hide" id="eds-c-header-nav" hidden="">
    
        <h2 class="eds-c-header__heading">Navigation</h2>
        <ul class="eds-c-header__list">
            
                <li class="eds-c-header__list-item">
                   <a class="eds-c-header__link" href="https://link.springer.com/journals/" data-track="nav_find_a_journal" data-track-context="unified header" data-track-action="click find a journal" data-track-category="unified header" data-track-label="link">
                        Find a journal
                    </a>
                </li>
            
                <li class="eds-c-header__list-item">
                   <a class="eds-c-header__link" href="https://www.springernature.com/gp/authors" data-track="nav_how_to_publish" data-track-context="unified header" data-track-action="click publish with us link" data-track-category="unified header" data-track-label="link">
                        Publish with us
                    </a>
                </li>
            
                <li class="eds-c-header__list-item">
                   <a class="eds-c-header__link" href="https://link.springernature.com/home/" data-track="nav_track_your_research" data-track-context="unified header" data-track-action="click track your research" data-track-category="unified header" data-track-label="link">
                        Track your research
                    </a>
                </li>
            
        </ul>
    
</div>



    <article lang="en" id="main" class="app-masthead__colour-default">
        <section class="app-masthead " aria-label="article masthead">
    <div class="app-masthead__container">
        
            <div class="app-article-masthead u-sans-serif js-context-bar-sticky-point-masthead" data-track-component="article" data-test="masthead-component">
                <div class="app-article-masthead__info">
                    
    
        <nav aria-label="breadcrumbs" data-test="breadcrumbs">
            <ol class="c-breadcrumbs c-breadcrumbs--contrast" itemscope="" itemtype="https://schema.org/BreadcrumbList">
                
                    <li class="c-breadcrumbs__item" id="breadcrumb0" itemprop="itemListElement" itemscope="" itemtype="https://schema.org/ListItem">
                        <a href="/" class="c-breadcrumbs__link" itemprop="item" data-track="click_breadcrumb" data-track-context="article page" data-track-category="article" data-track-action="breadcrumbs" data-track-label="breadcrumb1"><span itemprop="name">Home</span></a><meta itemprop="position" content="1">
                            <svg class="c-breadcrumbs__chevron" role="img" aria-hidden="true" focusable="false" width="10" height="10" viewBox="0 0 10 10">
                                <path d="m5.96738168 4.70639573 2.39518594-2.41447274c.37913917-.38219212.98637524-.38972225 1.35419292-.01894278.37750606.38054586.37784436.99719163-.00013556 1.37821513l-4.03074001 4.06319683c-.37758093.38062133-.98937525.38100976-1.367372-.00003075l-4.03091981-4.06337806c-.37759778-.38063832-.38381821-.99150444-.01600053-1.3622839.37750607-.38054587.98772445-.38240057 1.37006824.00302197l2.39538588 2.4146743.96295325.98624457z" fill-rule="evenodd" transform="matrix(0 -1 1 0 0 10)"></path>
                            </svg>
                    </li>
                
                    <li class="c-breadcrumbs__item" id="breadcrumb1" itemprop="itemListElement" itemscope="" itemtype="https://schema.org/ListItem">
                        <a href="/journal/12880" class="c-breadcrumbs__link" itemprop="item" data-track="click_breadcrumb" data-track-context="article page" data-track-category="article" data-track-action="breadcrumbs" data-track-label="breadcrumb2"><span itemprop="name">BMC Medical Imaging</span></a><meta itemprop="position" content="2">
                            <svg class="c-breadcrumbs__chevron" role="img" aria-hidden="true" focusable="false" width="10" height="10" viewBox="0 0 10 10">
                                <path d="m5.96738168 4.70639573 2.39518594-2.41447274c.37913917-.38219212.98637524-.38972225 1.35419292-.01894278.37750606.38054586.37784436.99719163-.00013556 1.37821513l-4.03074001 4.06319683c-.37758093.38062133-.98937525.38100976-1.367372-.00003075l-4.03091981-4.06337806c-.37759778-.38063832-.38381821-.99150444-.01600053-1.3622839.37750607-.38054587.98772445-.38240057 1.37006824.00302197l2.39538588 2.4146743.96295325.98624457z" fill-rule="evenodd" transform="matrix(0 -1 1 0 0 10)"></path>
                            </svg>
                    </li>
                
                    <li class="c-breadcrumbs__item" id="breadcrumb2" itemprop="itemListElement" itemscope="" itemtype="https://schema.org/ListItem">
                        <span itemprop="name">Article</span><meta itemprop="position" content="3">
                    </li>
                
            </ol>
        </nav>
    

                    <h1 class="c-article-title" data-test="article-title" data-article-title="">WBC image classification and generative models based on convolutional neural network</h1>

                    <ul class="c-article-identifiers">
                        
        <li class="c-article-identifiers__item" data-test="article-category">Research</li>
    
        <li class="c-article-identifiers__item">
            <a href="https://www.springernature.com/gp/open-research/about/the-fundamentals-of-open-access-and-open-research" data-track="click" data-track-action="open access" data-track-label="link" class="u-color-open-access" data-test="open-access">Open access</a>
        </li>
    
    

                        <li class="c-article-identifiers__item">
                            Published: <time datetime="2022-05-20">20 May 2022</time>
                        </li>
                    </ul>
                    <ul class="c-article-identifiers c-article-identifiers--cite-list">
                        <li class="c-article-identifiers__item">
                            <span data-test="journal-volume">Volume&nbsp;22</span>, article&nbsp;number&nbsp;<span data-test="article-number">94</span>, (<span data-test="article-publication-year">2022</span>)
            
                        </li>
                        <li class="c-article-identifiers__item c-article-identifiers__item--cite">
                            <a href="#citeas" data-track="click" data-track-action="cite this article" data-track-category="article body" data-track-label="link">Cite this article</a>
                        </li>
                    </ul>

                    <div class="app-article-masthead__buttons" data-test="download-article-link-wrapper" data-track-context="masthead">
                        
        <div class="c-pdf-container">
            <div class="c-pdf-download u-clear-both u-mb-16">
                <a href="/content/pdf/10.1186/s12880-022-00818-1.pdf" class="u-button u-button--full-width u-button--primary u-justify-content-space-between c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="content_download" data-track-type="article pdf download" data-track-action="download pdf" data-track-label="button" data-track-external="" download="">
                    
                        <span class="c-pdf-download__text">Download PDF</span>
                        <svg aria-hidden="true" focusable="false" width="16" height="16" class="u-icon"><use xlink:href="#icon-eds-i-download-medium"></use></svg>
                    
                </a>
            </div>
        </div>
    

                        <p class="app-article-masthead__access">
                            <svg width="16" height="16" focusable="false" role="img" aria-hidden="true"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-check-filled-medium"></use></svg>
                            You have full access to this <a href="https://www.springernature.com/gp/open-research/about/the-fundamentals-of-open-access-and-open-research" data-track="click" data-track-action="open access" data-track-label="link">open access</a> article</p>
                        
                    </div>
                </div>
                <div class="app-article-masthead__brand">
                    
                        
                            <a href="/journal/12880" class="app-article-masthead__journal-link" data-track="click_journal_home" data-track-action="journal homepage" data-track-context="article page" data-track-label="link">
                            <picture>
                                <source type="image/webp" media="(min-width: 768px)" width="120" height="159" srcset="https://media.springernature.com/w120/springer-static/cover-hires/journal/12880?as=webp,
                                                    https://media.springernature.com/w316/springer-static/cover-hires/journal/12880?as=webp 2x">
                                <img width="72" height="95" src="https://media.springernature.com/w72/springer-static/cover-hires/journal/12880?as=webp" srcset="https://media.springernature.com/w144/springer-static/cover-hires/journal/12880?as=webp 2x" alt="">
                            </picture>
                            <span class="app-article-masthead__journal-title">BMC Medical Imaging</span>
                        </a>
                        
                            <a href="https://bmcmedimaging.biomedcentral.com/about" class="app-article-masthead__submission-link" data-track="click_aims_and_scope" data-track-action="aims and scope" data-track-context="article page" data-track-label="link">
                                Aims and scope
                                <svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-arrow-right-medium"></use></svg>
                            </a>
                        
                        
                            <a href="https://submission.nature.com/new-submission/12880/3" class="app-article-masthead__submission-link" data-track="click_submit_manuscript" data-track-context="article masthead on springerlink article page" data-track-action="submit manuscript" data-track-label="link">
                                Submit manuscript
                                <svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-arrow-right-medium"></use></svg>
                            </a>
                        
                    
                </div>
            </div>
        
    </div>
</section>

        <div class="c-article-main u-container u-mt-24 u-mb-32 l-with-sidebar" id="main-content" data-component="article-container">
            <main class="u-serif js-main-column" data-track-component="article body">
                
                
                    <div class="c-context-bar u-hide" data-test="context-bar" data-context-bar="" aria-hidden="true">
                        <div class="c-context-bar__container u-container">
                            <div class="c-context-bar__title">
                                WBC image classification and generative models based on convolutional neural network
                            </div>
                            
                                <div data-test="inCoD" data-track-context="sticky banner">
                                    
        <div class="c-pdf-container">
            <div class="c-pdf-download u-clear-both u-mb-16">
                <a href="/content/pdf/10.1186/s12880-022-00818-1.pdf" class="u-button u-button--full-width u-button--primary u-justify-content-space-between c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="content_download" data-track-type="article pdf download" data-track-action="download pdf" data-track-label="button" data-track-external="" download="">
                    
                        <span class="c-pdf-download__text">Download PDF</span>
                        <svg aria-hidden="true" focusable="false" width="16" height="16" class="u-icon"><use xlink:href="#icon-eds-i-download-medium"></use></svg>
                    
                </a>
            </div>
        </div>
    

                                </div>
                            
                        </div>
                    </div>
                

                

                <div class="c-article-header">
                    <header>
                        <ul class="c-article-author-list c-article-author-list--short js-no-scroll" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-article-author-list__item"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Changhun-Jung-Aff1" data-author-popup="auth-Changhun-Jung-Aff1" data-author-search="Jung, Changhun" data-track-context="researcher popup with no profile" data-track-index="1_5">Changhun Jung</a><sup class="u-js-hide"><a href="#Aff1" tabindex="-1">1</a></sup>, </li><li class="c-article-author-list__item"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Mohammed-Abuhamad-Aff2" data-author-popup="auth-Mohammed-Abuhamad-Aff2" data-author-search="Abuhamad, Mohammed" data-track-context="researcher popup with no profile" data-track-index="2_5">Mohammed Abuhamad</a><sup class="u-js-hide"><a href="#Aff2" tabindex="-1">2</a></sup>, </li><li class="c-article-author-list__item c-article-author-list__item--hide-small-screen"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-David-Mohaisen-Aff3" data-author-popup="auth-David-Mohaisen-Aff3" data-author-search="Mohaisen, David" data-track-context="researcher popup with no profile" data-track-index="3_5">David Mohaisen</a><sup class="u-js-hide"><a href="#Aff3" tabindex="-1">3</a></sup>, </li><li class="c-article-author-list__item c-article-author-list__item--hide-small-screen"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Kyungja-Han-Aff4" data-author-popup="auth-Kyungja-Han-Aff4" data-author-search="Han, Kyungja" data-track-context="researcher popup with no profile" data-track-index="4_5">Kyungja Han</a><sup class="u-js-hide"><a href="#Aff4" tabindex="-1">4</a></sup> &amp; </li><li class="c-article-author-list__show-more" aria-label="Show all 5 authors for this article" title="Show all 5 authors for this article">…</li><li class="c-article-author-list__item"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-DaeHun-Nyang-Aff1" data-author-popup="auth-DaeHun-Nyang-Aff1" data-author-search="Nyang, DaeHun" data-corresp-id="c1" data-track-context="researcher popup with no profile" data-track-index="5_5">DaeHun Nyang<svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-mail-medium"></use></svg></a><sup class="u-js-hide"><a href="#Aff1" tabindex="-1">1</a></sup>&nbsp;</li></ul><button aria-expanded="false" class="c-article-author-list__button"><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-down-medium"></use></svg><span>Show authors</span></button>
                        
    

                        <div data-test="article-metrics">
                            <ul class="app-article-metrics-bar u-list-reset">
                                
    
        <li class="app-article-metrics-bar__item">
            <p class="app-article-metrics-bar__count"><svg class="u-icon app-article-metrics-bar__icon" width="24" height="24" aria-hidden="true" focusable="false">
                <use xlink:href="#icon-eds-i-accesses-medium"></use>
            </svg>9716 <span class="app-article-metrics-bar__label">Accesses</span></p>
        </li>
    
    
    
    
    
        <li class="app-article-metrics-bar__item app-article-metrics-bar__item--metrics">
            <p class="app-article-metrics-bar__details"><a href="/article/10.1186/s12880-022-00818-1/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Explore all metrics <svg class="u-icon app-article-metrics-bar__arrow-icon" width="24" height="24" aria-hidden="true" focusable="false">
                <use xlink:href="#icon-eds-i-arrow-right-medium"></use>
            </svg></a></p>
        </li>
    




                            </ul>
                        </div>
                        
    <div class="u-mt-32">
    

    
    </div>

                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    
                    <section aria-labelledby="Abs1" data-title="Abstract" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><h3 class="c-article__sub-heading" data-test="abstract-sub-heading">Background</h3><p>Computer-aided methods for analyzing white blood cells (WBC) are popular due to the complexity of the manual alternatives. Recent works have shown highly accurate segmentation and detection of white blood cells from microscopic blood images. However, the classification of the observed cells is still a challenge, in part due to the distribution of the five types that affect the condition of the immune system.</p><h3 class="c-article__sub-heading" data-test="abstract-sub-heading">Methods</h3><p>(i) This work proposes W-Net, a CNN-based method for WBC classification. We evaluate W-Net on a real-world large-scale dataset that includes 6562 real images of the five WBC types. (ii) For further benefits, we generate synthetic WBC images using Generative Adversarial Network to be used for education and research purposes through sharing.</p><h3 class="c-article__sub-heading" data-test="abstract-sub-heading">Results</h3><p>(i) W-Net achieves an average accuracy of 97%. In comparison to state-of-the-art methods in the field of WBC classification, we show that W-Net outperforms other CNN- and RNN-based model architectures. Moreover, we show the benefits of using pre-trained W-Net in a transfer learning context when fine-tuned to specific task or accommodating another dataset. (ii) The synthetic WBC images are confirmed by experiments and a domain expert to have a high degree of similarity to the original images. The pre-trained W-Net and the generated WBC dataset are available for the community to facilitate reproducibility and follow up research work.</p><h3 class="c-article__sub-heading" data-test="abstract-sub-heading">Conclusion</h3><p>This work proposed W-Net, a CNN-based architecture with a small number of layers, to accurately classify the five WBC types. We evaluated W-Net on a real-world large-scale dataset and addressed several challenges such as the transfer learning property and the class imbalance. W-Net achieved an average classification accuracy of 97%. We synthesized a dataset of new WBC image samples using DCGAN, which we released to the public for education and research purposes.</p></div></div></section>
                    
    
        <div class="app-peer-review">
            <a class="app-peer-review__link" href="/article/10.1186/s12880-022-00818-1/peer-review" data-track="click" data-track-category="article body" data-track-action="open peer review reports" data-track-label="10.1186/s12880-022-00818-1">
                <svg class="u-icon app-peer-review__icon" aria-hidden="true" focusable="false">
                    <use xlink:href="#icon-eds-i-file-report-medium"></use>
                </svg>
                View this article's peer review reports
            </a>
        </div>
    


                    

                    <div data-test="cobranding-download">
                        
                    </div>

                    
                        
        
            <section aria-labelledby="inline-recommendations" data-title="Inline Recommendations" class="c-article-recommendations" data-track-component="inline-recommendations">
                <h3 class="c-article-recommendations-title" id="inline-recommendations">Similar content being viewed by others</h3>
                <div class="c-article-recommendations-list">
                    
                        <div class="c-article-recommendations-list__item">
                            <article class="c-article-recommendations-card" itemscope="" itemtype="http://schema.org/ScholarlyArticle">
                                
                                    <div class="c-article-recommendations-card__img"><img src="https://media.springernature.com/w215h120/springer-static/image/art%3A10.1007%2Fs11517-020-02163-3/MediaObjects/11517_2020_2163_Figf_HTML.png" loading="lazy" alt=""></div>
                                
                                <div class="c-article-recommendations-card__main">
                                    <h3 class="c-article-recommendations-card__heading" itemprop="name headline">
                                        <a class="c-article-recommendations-card__link" itemprop="url" href="https://link.springer.com/10.1007/s11517-020-02163-3?fromPaywallRec=false" data-track="select_recommendations_1" data-track-context="inline recommendations" data-track-action="click recommendations inline - 1" data-track-label="10.1007/s11517-020-02163-3">Combining DC-GAN with ResNet for blood cell image classification
                                        </a>
                                    </h3>
                                    <div class="c-article-meta-recommendations" data-test="recommendation-info">
                                        <span class="c-article-meta-recommendations__item-type">Article</span>
                                        
                                         <span class="c-article-meta-recommendations__date">27 March 2020</span>
                                    </div>
                                </div>
                            </article>
                        </div>
                    
                        <div class="c-article-recommendations-list__item">
                            <article class="c-article-recommendations-card" itemscope="" itemtype="http://schema.org/ScholarlyArticle">
                                
                                    <div class="c-article-recommendations-card__img"><img src="https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs41598-024-52880-0/MediaObjects/41598_2024_52880_Fig1_HTML.png" loading="lazy" alt=""></div>
                                
                                <div class="c-article-recommendations-card__main">
                                    <h3 class="c-article-recommendations-card__heading" itemprop="name headline">
                                        <a class="c-article-recommendations-card__link" itemprop="url" href="https://link.springer.com/10.1038/s41598-024-52880-0?fromPaywallRec=false" data-track="select_recommendations_2" data-track-context="inline recommendations" data-track-action="click recommendations inline - 2" data-track-label="10.1038/s41598-024-52880-0">White blood cells classification using multi-fold pre-processing and optimized CNN model
                                        </a>
                                    </h3>
                                    <div class="c-article-meta-recommendations" data-test="recommendation-info">
                                        <span class="c-article-meta-recommendations__item-type">Article</span>
                                         <span class="c-article-meta-recommendations__access-type">Open access</span>
                                         <span class="c-article-meta-recommendations__date">12 February 2024</span>
                                    </div>
                                </div>
                            </article>
                        </div>
                    
                        <div class="c-article-recommendations-list__item">
                            <article class="c-article-recommendations-card" itemscope="" itemtype="http://schema.org/ScholarlyArticle">
                                
                                    <div class="c-article-recommendations-card__img"><img src="https://media.springernature.com/w215h120/springer-static/image/art%3A10.1007%2Fs11227-024-06405-1/MediaObjects/11227_2024_6405_Fig1_HTML.png" loading="lazy" alt=""></div>
                                
                                <div class="c-article-recommendations-card__main">
                                    <h3 class="c-article-recommendations-card__heading" itemprop="name headline">
                                        <a class="c-article-recommendations-card__link" itemprop="url" href="https://link.springer.com/10.1007/s11227-024-06405-1?fromPaywallRec=false" data-track="select_recommendations_3" data-track-context="inline recommendations" data-track-action="click recommendations inline - 3" data-track-label="10.1007/s11227-024-06405-1">Efficient white blood cell identification with hybrid inception-xception network
                                        </a>
                                    </h3>
                                    <div class="c-article-meta-recommendations" data-test="recommendation-info">
                                        <span class="c-article-meta-recommendations__item-type">Article</span>
                                        
                                         <span class="c-article-meta-recommendations__date">07 August 2024</span>
                                    </div>
                                </div>
                            </article>
                        </div>
                    
                </div>
            </section>
        
            <script>
                window.dataLayer = window.dataLayer || [];
                window.dataLayer.push({
                    recommendations: {
                        recommender: 'semantic',
                        model: 'specter',
                        policy_id: 'NA',
                        timestamp: 1744107146,
                        embedded_user: 'null'
                    }
                });
            </script>
        
    
                    

                    
                        
                    

                    
                        
                    

                    
                        
                                <div class="main-content">
                                    <section data-title="Background"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Background</h2><div class="c-article-section__content" id="Sec1-content"><p>White blood cells (WBCs) are one type of blood cells, besides red blood cell and platelet, and are responsible for the immune system, defending against foreign substances and bacteria. WBCs are typically categorized into five major types: neutrophils, eosinophils, basophils, lymphocytes and monocytes. Neutrophils consist of two functionally unequal subgroups: neutrophil-killers and neutrophil-cagers, and they defend against bacterial or fungal infections [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2" title="Pillay J, et al. In vivo labeling with 2H2O reveals a human neutrophil lifespan of 5.4 days. Blood. 2010;116(4):625–7." href="/article/10.1186/s12880-022-00818-1#ref-CR2" id="ref-link-section-d107567642e478">2</a>]. The number of eosinophils increase in response to allergies, parasitic infections, collagen diseases, and disease of the spleen and central nervous system [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 3" title="Rothenberg ME, Hogan SP. The eosinophil. Annu Rev Immunol. 2006;24:147–74." href="/article/10.1186/s12880-022-00818-1#ref-CR3" id="ref-link-section-d107567642e481">3</a>]. Basophils are mainly responsible for allergic and antigen response by releasing chemical histamine causing the dilation of blood vessels [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 4" title="Falcone FH, Haas H, Gibbs BF. The human basophil: a new appreciation of its role in immune responses. Blood. 2000;13:4028–38." href="/article/10.1186/s12880-022-00818-1#ref-CR4" id="ref-link-section-d107567642e484">4</a>]. Lymphocytes help immune cells to combine with other foreign invasive organisms such as microorganisms and antigens, in order to remove them out of the body [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 5" title="Butcher EC, Picker LJ. Lymphocyte homing and homeostasis. Science. 1996;272(5258):60–7." href="/article/10.1186/s12880-022-00818-1#ref-CR5" id="ref-link-section-d107567642e487">5</a>]. Monocytes phagocytose foreign substances in the tissues [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 6" title="Gordon S, Taylor PR. Monocyte and macrophage heterogeneity. Nat Rev Immunol. 2005;5:953–64." href="/article/10.1186/s12880-022-00818-1#ref-CR6" id="ref-link-section-d107567642e490">6</a>]. The usual distribution of these five classes is 62%, 2.3%, 0.4%, 30% and 5.3% among WBCs in the body [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 7" title="WBC (White Blood Cell) Count. 
                  https://bit.ly/3cDg58c
                  
                . Accessed: 2020-06-09." href="/article/10.1186/s12880-022-00818-1#ref-CR7" id="ref-link-section-d107567642e494">7</a>]. This distribution of WBC describes the condition of the immune system. Considering the complexity of manually estimating the distribution of WBC, <i>e.g.,</i> by consulting a human expert, many studies have introduced methods for automating the process through WBC segmentation, detection, and classification. Despite these numerous studies, which are greatly focused on the segmentation and detection tasks, less attention has been given to the WBC classification task and factors impacting the accuracy and performance of the task.</p><p>Accurate WBC classification is also beneficial for diagnosing leukemia, a type of blood cancer in which abnormal WBCs in the blood rapidly proliferate, decreasing the number of normal blood cells making the immune system vulnerable to infections In the US, around 60,000 people are diagnosed with leukemia every year, and around 20,000 people die of leukemia annually. From 2011 to 2015, leukemia was the sixth most common cause of cancer-caused death in the US [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 8" title="Statistics. 
                  https://bit.ly/30esTwt
                  
                . Accessed: 2019-06-27." href="/article/10.1186/s12880-022-00818-1#ref-CR8" id="ref-link-section-d107567642e503">8</a>]. There are various types of leukemia, including ALL (Acute lymphocytic leukemia), AML (Acute myelogenous leukemia), CLL (Chronic lymphocytic leukemia), CML (Chronic myelogenous leukemia). Chronic leukemia progresses more slowly than acute leukemia which requires immediate medical care. Acute leukemia is characterized by proliferation of blasts, CLL is characterized by increased lymphocytes while CML shows markedly increased neutrophils and some basophils in the blood [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 9" title="Leukemia. 
                  https://bit.ly/32WFwOn
                  
                . Accessed: 2019-06-29." href="/article/10.1186/s12880-022-00818-1#ref-CR9" id="ref-link-section-d107567642e506">9</a>]. Therefore, accurate classification of WBCs contributes to the diagnosis of leukemia.</p><p>Recent advancements in the field of computer vision and computer-aided diagnosis show a promising direction for the applicability of deep learning-based technologies to assist accurate classification and counting of WBC. Convolutional neural network (CNN) is one of the most common and successful deep learning architectures that have been utilized for analyzing and classifying medical imagery data [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Shin H, et al. Deep convolutional neural networks for computer-aided detection: CNN architectures, dataset characteristics and transfer learning. IEEE Trans Med Imaging. 2016;35(5):1285–98." href="#ref-CR10" id="ref-link-section-d107567642e512">10</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Tajbakhsh N, et al. Convolutional neural networks for medical image analysis: full training or fine tuning? IEEE Trans Med Imaging. 2016;35(5):1299–312." href="#ref-CR11" id="ref-link-section-d107567642e512_1">11</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Gupta H, Jin KH, Nguyen HQ, McCann MT, Unser M. CNN-based projected gradient descent for consistent CT image reconstruction. IEEE Trans Med Imaging. 2018;37(6):1440–53." href="#ref-CR12" id="ref-link-section-d107567642e512_2">12</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 13" title="Wolterink JM, Leiner T, Viergever MA, Isgum I. Generative adversarial networks for noise reduction in low-dose CT. IEEE Trans Med Imaging. 2017;36(12):2536–45." href="/article/10.1186/s12880-022-00818-1#ref-CR13" id="ref-link-section-d107567642e515">13</a>]. In this paper, we propose W-Net, a CNN-based network for WBC images classification. W-Net consists of three convolutional layers and two fully-connected layers, and they are responsible for extracting and learning features from WBC images and classifying them into five classes using a softmax classifier. In comparison to state-of-the-art methods, W-Net shows outstanding results in terms of accuracy. Further, we investigate the performance of several deep learning architectures in performing the WBC classification task. We applied and compared the performance of several architectures including W-Net, AlexNet [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="Krizhevsky A, Sutskever I, Hinton GE. Imagenet classification with deep convolutional neural networks. In: Advances in NIPS; 2012." href="/article/10.1186/s12880-022-00818-1#ref-CR14" id="ref-link-section-d107567642e518">14</a>], VGGNet [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 15" title="Simonyan K, Zisserman A. Very deep convolutional networks for large-scale image recognition. arXiv preprint 
                  arXiv:1409.1556
                  
                 (2014)." href="/article/10.1186/s12880-022-00818-1#ref-CR15" id="ref-link-section-d107567642e521">15</a>], ResNet [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 16" title="He K, Zhang X, Ren S, Sun J. Deep residual learning for image recognition. In: IEEE CVPR; 2016. p. 770–8." href="/article/10.1186/s12880-022-00818-1#ref-CR16" id="ref-link-section-d107567642e524">16</a>], and Recurrent Neural Network (RNN). Moreover, we compared the utilization of different classifiers such as softmax classifier and Support Vector Machine (SVM) on top of the adopted models. Moreover, we explore the effects of pre-training W-Net using public datasets, such as the LISC public [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 17" title="Rezatofighi SH, Soltanian-Zadeh H. Automatic recognition of five types of white blood cells in peripheral blood. Comput Med Imaging Graph. 2011;35(4):333–43." href="/article/10.1186/s12880-022-00818-1#ref-CR17" id="ref-link-section-d107567642e528">17</a>], on its performance. Understanding the importance of large-scale datasets on the models’ performance, we generate new WBC images using GAN [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 18" title="Goodfellow I, et al. Generative adversarial nets. In: Advances in NIPS; 2014. p. 2672–80." href="/article/10.1186/s12880-022-00818-1#ref-CR18" id="ref-link-section-d107567642e531">18</a>] to augment current educational and research datasets.
</p><h3 class="c-article__sub-heading" id="Sec2">Contributions</h3><p>The contributions of this paper are as follows. 1 We propose ❶ W-Net, a CNN-based network, designed to accurately classify WBCs while maintaining a high efficiency through minimal depth of the CNN architecture. ❷ We evaluate the performance of W-Net using a real-world large-scale dataset that consist of 6562 real images. ❸ We address and handle the problem of imbalanced classes of WBCs and achieve an average classification accuracy of 97% for all classes. ❹ We show how W-Net which consists of three convolutional layers stands among most popular CNN-based architectures, in the field of image classification and computer vision, in performing the WBCs classification task. ❺ Serving the purpose of advancing the task, we studied the applicability of transfer learning and generating larger datasets of WBC images using GAN for the public release. ❻ We generate and publicize synthetic WBC images using Generative Adversarial Network to be used for education and research purposes. The synthetic WBC images are verified by experiments and a domain expert to have a high degree of similarity to the original images. The pre-trained W-Net and the generated WBC dataset are available for the public.
</p><h3 class="c-article__sub-heading" id="Sec3">Organization</h3><p>The rest of the paper is organized as follows: in “<a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1186/s12880-022-00818-1#Sec4">Related works</a>” section, we review literature. We introduce our model W-Net in “<a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1186/s12880-022-00818-1#Sec7">Methods</a>” section. We evaluate W-Net through various experiments on WBC images in “<a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1186/s12880-022-00818-1#Sec11">Experiments</a>” section. Our design choices and the experiment result are discussed in “<a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1186/s12880-022-00818-1#Sec20">Design considerations for W-Net</a>” section. We release a new WBC dataset using GAN in “<a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1186/s12880-022-00818-1#Sec24">Dataset sharing</a>” section. Finally, we conclude our study in “<a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1186/s12880-022-00818-1#Sec27">Conclusion</a>” section.</p></div></div></section><section data-title="Related works"><div class="c-article-section" id="Sec4-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec4">Related works</h2><div class="c-article-section__content" id="Sec4-content"><h3 class="c-article__sub-heading" id="Sec5">Previous works</h3><p>Analysis of white blood cells (WBC) has vital importance in diagnosing diseases. Distribution of the five WBC types, (basophils, eosinophils, lymphocytes, monocytes and neutrophils) reflects highly on the condition of the immune system. Analyzing the components of WBCs requires performing segmentation and classification processes. The traditional analysis of WBC includes observing a blood smear on a microscope and using the visible properties, such as shapes and colors, to classifing the blood cells. However, the accuracy of the WBCs analysis depends significantly on the knowledge and experience of the medical operator [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 19" title="Wang Q, Chang L, Zhou M, Li Q, Liu H, Guo F. A spectral and morphologic method for white blood cell classification. Opt Laser Technol. 2016;84:144–8." href="/article/10.1186/s12880-022-00818-1#ref-CR19" id="ref-link-section-d107567642e582">19</a>]. This makes the process of analyzing of WBCs using conventional methods time-consuming and labor-intensive [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Wang Q, Chang L, Zhou M, Li Q, Liu H, Guo F. A spectral and morphologic method for white blood cell classification. Opt Laser Technol. 2016;84:144–8." href="#ref-CR19" id="ref-link-section-d107567642e585">19</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Dorini LB, Minetto R, Leite NJ. Semiautomatic white blood cell segmentation based on multiscale analysis. IEEE J Biomed Health Inform. 2012;17(1):250–6." href="#ref-CR20" id="ref-link-section-d107567642e585_1">20</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 21" title="Prinyakupt J, Pluempitiwiriyawej C. Segmentation of white blood cells and comparison of cell morphology by linear and naïve bayes classifiers. Biomed Eng Online. 2015;14(1):63." href="/article/10.1186/s12880-022-00818-1#ref-CR21" id="ref-link-section-d107567642e588">21</a>]. Therefore, many studies have proposed computer-aided technologies to facilitate the WBC analysis through accurate cell detection and segmentation to reduce the manual efforts needed by human experts. For instance, Shitong and Min [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 22" title="Shitong W, Min W. A new detection algorithm (NDA) based on fuzzy cellular neural networks for white blood cell detection. IEEE Trans Inf Technol Biomed Publ Inf. 2006;10(1):5–10." href="/article/10.1186/s12880-022-00818-1#ref-CR22" id="ref-link-section-d107567642e591">22</a>] have proposed an algorithm based on fuzzy cellular neural networks to detect WBCs in microscopic blood images as the first key step for automatic WBC recognition. Using mathematical morphology and fuzzy cellular neural networks, the authors achieved a detection accuracy of 99%. The detection of WBCs is followed by a segmentation process, which segments the image into nucleus and cytoplasm regions. This task has been pursued by several studies providing accurate segmentation using a variety of methods. The most common approach for nuclei segmentation is the clustering based on extracted features from pixels values [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 23" title="Andrade AR, Vogado LH, de Veras MS, Silva RRV, Araujo FH, Medeiros FN. Recent computational methods for white blood cell nuclei segmentation: a comparative study. Comput Methods Prog Biomed. 2019;173:1–14." href="/article/10.1186/s12880-022-00818-1#ref-CR23" id="ref-link-section-d107567642e594">23</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 24" title="Viswanathan P. Fuzzy c means detection of leukemia based on morphological contour segmentation. Procedia Comput Sci. 2015;58:84–90." href="/article/10.1186/s12880-022-00818-1#ref-CR24" id="ref-link-section-d107567642e598">24</a>]. The literature shows a successful nuclei segmentation using different clustering techniques, such as K-means [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 25" title="Gautam A, Bhadauria H. White blood nucleus extraction using k-mean clustering and mathematical morphing. In: 2014 5th International conference-confluence the next generation information technology summit (confluence). IEEE; 2014. p. 549–554." href="/article/10.1186/s12880-022-00818-1#ref-CR25" id="ref-link-section-d107567642e601">25</a>], fuzzy K-means [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 24" title="Viswanathan P. Fuzzy c means detection of leukemia based on morphological contour segmentation. Procedia Comput Sci. 2015;58:84–90." href="/article/10.1186/s12880-022-00818-1#ref-CR24" id="ref-link-section-d107567642e604">24</a>], C-means [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 24" title="Viswanathan P. Fuzzy c means detection of leukemia based on morphological contour segmentation. Procedia Comput Sci. 2015;58:84–90." href="/article/10.1186/s12880-022-00818-1#ref-CR24" id="ref-link-section-d107567642e607">24</a>], and GK-means [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 26" title="Mohapatra S, Samanta SS, Patra D, Satpathi S. Fuzzy based blood image segmentation for automated leukemia detection. In: 2011 ICDeCom. IEEE; 2011. p. 1–5." href="/article/10.1186/s12880-022-00818-1#ref-CR26" id="ref-link-section-d107567642e610">26</a>]. Among other unsupervised techniques for nuclei segmentation beside clustering, many studies utilized thresholding [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 21" title="Prinyakupt J, Pluempitiwiriyawej C. Segmentation of white blood cells and comparison of cell morphology by linear and naïve bayes classifiers. Biomed Eng Online. 2015;14(1):63." href="/article/10.1186/s12880-022-00818-1#ref-CR21" id="ref-link-section-d107567642e613">21</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Nazlibilek S, Karacor D, Ercan T, Sazli MH, Kalender O, Ege Y. Automatic segmentation, counting, size determination and classification of white blood cells. Measurement. 2014;55:58–65." href="#ref-CR27" id="ref-link-section-d107567642e617">27</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Abdeldaim AM, Sahlol AT, Elhoseny M, Hassanien AE. Computer-aided acute lymphoblastic leukemia diagnosis system based on image analysis. Berlin: Springer; 2018. p. 131–47." href="#ref-CR28" id="ref-link-section-d107567642e617_1">28</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Tosta TAA, De Abreu AF, Travençolo BAN, do Nascimento MZ, Neves LA. Unsupervised segmentation of leukocytes images using thresholding neighborhood valley-emphasis. In: 2015 IEEE CBMS. IEEE; 2015. p. 93–94." href="#ref-CR29" id="ref-link-section-d107567642e617_2">29</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Cao H, Liu H, Song E. A novel algorithm for segmentation of leukocytes in peripheral blood. Biomed Signal Process Control. 2018;45:10–21." href="#ref-CR30" id="ref-link-section-d107567642e617_3">30</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 31" title="Mohammed EA, Mohamed MM, Naugler C, Far BH. Chronic lymphocytic leukemia cell segmentation from microscopic blood images using watershed algorithm and optimal thresholding. In: 2013 26th IEEE CCECE. IEEE; 2013. p. 1–5." href="/article/10.1186/s12880-022-00818-1#ref-CR31" id="ref-link-section-d107567642e620">31</a>], arithmetical operations [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 32" title="Hegde RB, Prasad K, Hebbar H, Singh BMK. Development of a robust algorithm for detection of nuclei and classification of white blood cells in peripheral blood smear images. J Med Syst. 2018;42(6):110." href="/article/10.1186/s12880-022-00818-1#ref-CR32" id="ref-link-section-d107567642e623">32</a>], edge-based detection [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 24" title="Viswanathan P. Fuzzy c means detection of leukemia based on morphological contour segmentation. Procedia Comput Sci. 2015;58:84–90." href="/article/10.1186/s12880-022-00818-1#ref-CR24" id="ref-link-section-d107567642e626">24</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 31" title="Mohammed EA, Mohamed MM, Naugler C, Far BH. Chronic lymphocytic leukemia cell segmentation from microscopic blood images using watershed algorithm and optimal thresholding. In: 2013 26th IEEE CCECE. IEEE; 2013. p. 1–5." href="/article/10.1186/s12880-022-00818-1#ref-CR31" id="ref-link-section-d107567642e629">31</a>], region-based detection [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 31" title="Mohammed EA, Mohamed MM, Naugler C, Far BH. Chronic lymphocytic leukemia cell segmentation from microscopic blood images using watershed algorithm and optimal thresholding. In: 2013 26th IEEE CCECE. IEEE; 2013. p. 1–5." href="/article/10.1186/s12880-022-00818-1#ref-CR31" id="ref-link-section-d107567642e632">31</a>], genetic algorithm [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 33" title="Chan Y-K, Tsai M-H, Huang D-C, Zheng Z-H, Hung K-D. Leukocyte nucleus segmentation and nucleus lobe counting. BMC Bioinform. 2010;11(1):558." href="/article/10.1186/s12880-022-00818-1#ref-CR33" id="ref-link-section-d107567642e636">33</a>], watershed algorithm [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 31" title="Mohammed EA, Mohamed MM, Naugler C, Far BH. Chronic lymphocytic leukemia cell segmentation from microscopic blood images using watershed algorithm and optimal thresholding. In: 2013 26th IEEE CCECE. IEEE; 2013. p. 1–5." href="/article/10.1186/s12880-022-00818-1#ref-CR31" id="ref-link-section-d107567642e639">31</a>], and Gram-Schmidt orthogonalization [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 17" title="Rezatofighi SH, Soltanian-Zadeh H. Automatic recognition of five types of white blood cells in peripheral blood. Comput Med Imaging Graph. 2011;35(4):333–43." href="/article/10.1186/s12880-022-00818-1#ref-CR17" id="ref-link-section-d107567642e642">17</a>].</p><p>The literature on WBC segmentation process is very rich and provides valuable insights for the WBC identification. Andrade et al. [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 23" title="Andrade AR, Vogado LH, de Veras MS, Silva RRV, Araujo FH, Medeiros FN. Recent computational methods for white blood cell nuclei segmentation: a comparative study. Comput Methods Prog Biomed. 2019;173:1–14." href="/article/10.1186/s12880-022-00818-1#ref-CR23" id="ref-link-section-d107567642e648">23</a>] provides a survey and a comparative study on the performance of 15 segmentation methods using five public WBC databases. Some of these works are dedicated to the separation of adjacent cells, while many others addressed particularly the separation of overlapping cells. After the segmentation process, the WBC image classification or identification process is conducted. The distinction between the task of WBC identification and WBC image classification is the identification process aims to detect and identify leucocytes in an image, while the classification process aims to distinguish the different types of WBC. Even though many studies are dedicated to segmentation and identification task, fewer researches are addressed the classification of the WBCs. The literature shows that classification methods used for this purpose include the K-Nearest Neighbor (KNN) classifier [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 20" title="Dorini LB, Minetto R, Leite NJ. Semiautomatic white blood cell segmentation based on multiscale analysis. IEEE J Biomed Health Inform. 2012;17(1):250–6." href="/article/10.1186/s12880-022-00818-1#ref-CR20" id="ref-link-section-d107567642e651">20</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 28" title="Abdeldaim AM, Sahlol AT, Elhoseny M, Hassanien AE. Computer-aided acute lymphoblastic leukemia diagnosis system based on image analysis. Berlin: Springer; 2018. p. 131–47." href="/article/10.1186/s12880-022-00818-1#ref-CR28" id="ref-link-section-d107567642e654">28</a>], Bayesian classifier [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 21" title="Prinyakupt J, Pluempitiwiriyawej C. Segmentation of white blood cells and comparison of cell morphology by linear and naïve bayes classifiers. Biomed Eng Online. 2015;14(1):63." href="/article/10.1186/s12880-022-00818-1#ref-CR21" id="ref-link-section-d107567642e657">21</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 28" title="Abdeldaim AM, Sahlol AT, Elhoseny M, Hassanien AE. Computer-aided acute lymphoblastic leukemia diagnosis system based on image analysis. Berlin: Springer; 2018. p. 131–47." href="/article/10.1186/s12880-022-00818-1#ref-CR28" id="ref-link-section-d107567642e660">28</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 34" title="Mathur A, Tripathi AS, Kuse M. Scalable system for classification of white blood cells from Leishman stained blood stain images. J Pathol Inform. 2013;4(Suppl):15." href="/article/10.1186/s12880-022-00818-1#ref-CR34" id="ref-link-section-d107567642e664">34</a>], SVM classifier [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 17" title="Rezatofighi SH, Soltanian-Zadeh H. Automatic recognition of five types of white blood cells in peripheral blood. Comput Med Imaging Graph. 2011;35(4):333–43." href="/article/10.1186/s12880-022-00818-1#ref-CR17" id="ref-link-section-d107567642e667">17</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 19" title="Wang Q, Chang L, Zhou M, Li Q, Liu H, Guo F. A spectral and morphologic method for white blood cell classification. Opt Laser Technol. 2016;84:144–8." href="/article/10.1186/s12880-022-00818-1#ref-CR19" id="ref-link-section-d107567642e670">19</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 26" title="Mohapatra S, Samanta SS, Patra D, Satpathi S. Fuzzy based blood image segmentation for automated leukemia detection. In: 2011 ICDeCom. IEEE; 2011. p. 1–5." href="/article/10.1186/s12880-022-00818-1#ref-CR26" id="ref-link-section-d107567642e673">26</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 28" title="Abdeldaim AM, Sahlol AT, Elhoseny M, Hassanien AE. Computer-aided acute lymphoblastic leukemia diagnosis system based on image analysis. Berlin: Springer; 2018. p. 131–47." href="/article/10.1186/s12880-022-00818-1#ref-CR28" id="ref-link-section-d107567642e676">28</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 35" title="Su M-C, Cheng C-Y, Wang P-C. A neural-network-based approach to WBC classification. Sci World J. 2014;2014:1–9." href="/article/10.1186/s12880-022-00818-1#ref-CR35" id="ref-link-section-d107567642e679">35</a>], Linear Discriminant Analysis (LDA) [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 36" title="Ramesh N, Dangott B, Salama ME, Tasdizen T. Isolation and two-step classification of normal white blood cells in peripheral blood smears. J Pathol Inform. 2012;3:179–91." href="/article/10.1186/s12880-022-00818-1#ref-CR36" id="ref-link-section-d107567642e683">36</a>], decision trees and random forest classifier [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 28" title="Abdeldaim AM, Sahlol AT, Elhoseny M, Hassanien AE. Computer-aided acute lymphoblastic leukemia diagnosis system based on image analysis. Berlin: Springer; 2018. p. 131–47." href="/article/10.1186/s12880-022-00818-1#ref-CR28" id="ref-link-section-d107567642e686">28</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 37" title="Ghosh P, Bhattacharjee D, Nasipuri M. Blood smear analyzer for white blood cell counting: a hybrid microscopic image analyzing technique. Appl Soft Comput. 2016;46:629–38." href="/article/10.1186/s12880-022-00818-1#ref-CR37" id="ref-link-section-d107567642e689">37</a>], and deep learning [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 17" title="Rezatofighi SH, Soltanian-Zadeh H. Automatic recognition of five types of white blood cells in peripheral blood. Comput Med Imaging Graph. 2011;35(4):333–43." href="/article/10.1186/s12880-022-00818-1#ref-CR17" id="ref-link-section-d107567642e692">17</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 27" title="Nazlibilek S, Karacor D, Ercan T, Sazli MH, Kalender O, Ege Y. Automatic segmentation, counting, size determination and classification of white blood cells. Measurement. 2014;55:58–65." href="/article/10.1186/s12880-022-00818-1#ref-CR27" id="ref-link-section-d107567642e695">27</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 32" title="Hegde RB, Prasad K, Hebbar H, Singh BMK. Development of a robust algorithm for detection of nuclei and classification of white blood cells in peripheral blood smear images. J Med Syst. 2018;42(6):110." href="/article/10.1186/s12880-022-00818-1#ref-CR32" id="ref-link-section-d107567642e698">32</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 35" title="Su M-C, Cheng C-Y, Wang P-C. A neural-network-based approach to WBC classification. Sci World J. 2014;2014:1–9." href="/article/10.1186/s12880-022-00818-1#ref-CR35" id="ref-link-section-d107567642e702">35</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 38" title="Habibzadeh M, Jannesari M, Rezaei Z, Baharvand H, Totonchi M. Automatic white blood cell classification using pre-trained deep learning models: Resnet and inception. In: Tenth ICMV 2017, vol 10696. International Society for Optics and Photonics; 2018. p. 1069612." href="/article/10.1186/s12880-022-00818-1#ref-CR38" id="ref-link-section-d107567642e705">38</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 39" title="Rawat J, Singh A, Bhadauria H, Virmani J, Devgun JS. Application of ensemble artificial neural network for the classification of white blood cells using microscopic blood images. Int J Comput Syst Eng. 2018;4(2–3):202–16." href="/article/10.1186/s12880-022-00818-1#ref-CR39" id="ref-link-section-d107567642e708">39</a>].</p><p>Recently, deep learning-based methods have been utilized for WBC classification and segmentation tasks [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Patil AM, Patil MD, Birajdar GK. White blood cells image classification using deep learning with canonical correlation analysis. IRBM. 2021;42(5):378–89." href="#ref-CR40" id="ref-link-section-d107567642e714">40</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Toğaçar M, Ergen B, Cömert Z. Classification of white blood cells using deep features obtained from convolutional neural network models based on the combination of feature selection methods. Appl Soft Comput. 2020;97:106810." href="#ref-CR41" id="ref-link-section-d107567642e714_1">41</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 42" title="Khan S, Sajjad M, Hussain T, Ullah A, Imran AS. A review on traditional machine learning and deep learning models for WBCs classification in blood smear images. IEEE Access. 2020;9:10657–73." href="/article/10.1186/s12880-022-00818-1#ref-CR42" id="ref-link-section-d107567642e717">42</a>]. Patil et al. [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 40" title="Patil AM, Patil MD, Birajdar GK. White blood cells image classification using deep learning with canonical correlation analysis. IRBM. 2021;42(5):378–89." href="/article/10.1186/s12880-022-00818-1#ref-CR40" id="ref-link-section-d107567642e720">40</a>] incorporated canonical correlation analysis with CNN to extract and train on multiple nuclei patches with overlapping nuclei for WBC classification. Toğaçar et al. [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 41" title="Toğaçar M, Ergen B, Cömert Z. Classification of white blood cells using deep features obtained from convolutional neural network models based on the combination of feature selection methods. Appl Soft Comput. 2020;97:106810." href="/article/10.1186/s12880-022-00818-1#ref-CR41" id="ref-link-section-d107567642e723">41</a>] have utilized multiple CNN-based models, namely, AlexNet, GoogLeNet, and ResNet-50, for feature extraction and adopted quadratic discriminant analysis for classifying WBCs. Their method achieved an accuracy of 97.95% on a dataset of four categories: Neutrophil, Eosinophil, Monocyte, and Lymphocyte. Mohamed et al. [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 43" title="Mohamed EH, El-Behaidy WH, Khoriba G, Li J. Improved white blood cells classification based on pre-trained deep learning models. J Commun Softw Syst. 2020;16(1):37–45." href="/article/10.1186/s12880-022-00818-1#ref-CR43" id="ref-link-section-d107567642e726">43</a>] have investigated the use of deep CNN models over different shallow classifiers for WBC classification. For example, using a logistic regression classifier, extracting features using MobileNet-22 enabled a classification accuracy of 97.03%. Banik et al. [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 44" title="Banik PP, Saha R, Kim KD. An automatic nucleus segmentation and CNN model based classification method of white blood cell. Expert Syst Appl. 2020;149:113211." href="/article/10.1186/s12880-022-00818-1#ref-CR44" id="ref-link-section-d107567642e730">44</a>] explored the use of combining features from different layers of CNN model to classify WBC in the BCCD dataset. Karthikeyan et al. [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 45" title="Karthikeyan MP, Venkatesan R. Interpolative Leishman-stained transformation invariant deep pattern classification for white blood cells. Soft Comput. 2020;24(16):12215–25." href="/article/10.1186/s12880-022-00818-1#ref-CR45" id="ref-link-section-d107567642e733">45</a>] proposed the LSM-TIDC approach to classify WBCs in blood smear images where a multi-directional model is used to extract texture and geometrical features that are then fed to a CNN model. Kutlu et al. [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 46" title="Kutlu H, Avci E, Özyurt F. White blood cells detection and classification based on regional convolutional neural networks. Med Hypotheses. 2020;135:109472." href="/article/10.1186/s12880-022-00818-1#ref-CR46" id="ref-link-section-d107567642e736">46</a>] proposed using Regional-Based CNN model for WBC classification in blood smear images. Many other approaches have been proposed to tackle various challenges in the field of WBC using traditional machine learning and deep learning-based methods. Khan et al. [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 42" title="Khan S, Sajjad M, Hussain T, Ullah A, Imran AS. A review on traditional machine learning and deep learning models for WBCs classification in blood smear images. IEEE Access. 2020;9:10657–73." href="/article/10.1186/s12880-022-00818-1#ref-CR42" id="ref-link-section-d107567642e739">42</a>] provided a comprehensive review of such practices and their impact on the field. Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1186/s12880-022-00818-1#Tab1">1</a> shows an overview of the performance and methods of the related works.
</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-1"><figure><figcaption class="c-article-table__figcaption"><b id="Tab1" data-test="table-caption">Table 1 Related work highlighting the used datasets, their size, number of classes (C), employed methods, and accuracy</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1186/s12880-022-00818-1/tables/1" aria-label="Full size table 1"><span>Full size table</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><h3 class="c-article__sub-heading" id="Sec6">CNN with medical images</h3><p>Due to the vast success in a variety of applications, CNN has been adopted in several medical applications where imagery inputs are analyzed for diagnosis or classification. In the field of medical imaging, CNN has been successfully utilized for histological microscopic image [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 47" title="Rakhlin A, Shvets A, Iglovikov V, Kalinin AA. Deep convolutional neural networks for breast cancer histology image analysis. In: ICIAR. Springer; 2018. p. 737–44." href="/article/10.1186/s12880-022-00818-1#ref-CR47" id="ref-link-section-d107567642e1980">47</a>], pediatric pneumonia [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 48" title="Kermany DS, et al. Identifying medical diagnoses and treatable diseases by image-based deep learning. Cell. 2018;172(5):1122–31." href="/article/10.1186/s12880-022-00818-1#ref-CR48" id="ref-link-section-d107567642e1983">48</a>], diabetic macular edema [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 48" title="Kermany DS, et al. Identifying medical diagnoses and treatable diseases by image-based deep learning. Cell. 2018;172(5):1122–31." href="/article/10.1186/s12880-022-00818-1#ref-CR48" id="ref-link-section-d107567642e1986">48</a>], ventricular arrhythmias [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 49" title="Acharya UR, et al. Automated identification of shockable and non-shockable life-threatening ventricular arrhythmias using convolutional neural network. Futur Gener Comput Syst. 2018;79:952–9." href="/article/10.1186/s12880-022-00818-1#ref-CR49" id="ref-link-section-d107567642e1989">49</a>], thyroid anomalies, mitotic nuclei estimation [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 50" title="Moran MB, et al. Identification of thyroid nodules in infrared images by convolutional neural networks. In: 2018 IJCNN. IEEE; 2018. p. 1–7." href="/article/10.1186/s12880-022-00818-1#ref-CR50" id="ref-link-section-d107567642e1992">50</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 51" title="Sohail A, Khan A, Nisar H, Tabassum S, Zameer A. Mitotic nuclei analysis in breast cancer histopathology images using deep ensemble classifier. Med Image Anal. 2021;72:102121." href="/article/10.1186/s12880-022-00818-1#ref-CR51" id="ref-link-section-d107567642e1996">51</a>], neuroanatomy [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 52" title="Wachinger C, Reuter M, Klein T. DeepNAT: deep convolutional neural network for segmenting neuroanatomy. Neuroimage. 2018;170:434–45." href="/article/10.1186/s12880-022-00818-1#ref-CR52" id="ref-link-section-d107567642e1999">52</a>], and others [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Shin H, et al. Deep convolutional neural networks for computer-aided detection: CNN architectures, dataset characteristics and transfer learning. IEEE Trans Med Imaging. 2016;35(5):1285–98." href="#ref-CR10" id="ref-link-section-d107567642e2002">10</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Tajbakhsh N, et al. Convolutional neural networks for medical image analysis: full training or fine tuning? IEEE Trans Med Imaging. 2016;35(5):1299–312." href="#ref-CR11" id="ref-link-section-d107567642e2002_1">11</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Gupta H, Jin KH, Nguyen HQ, McCann MT, Unser M. CNN-based projected gradient descent for consistent CT image reconstruction. IEEE Trans Med Imaging. 2018;37(6):1440–53." href="#ref-CR12" id="ref-link-section-d107567642e2002_2">12</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 13" title="Wolterink JM, Leiner T, Viergever MA, Isgum I. Generative adversarial networks for noise reduction in low-dose CT. IEEE Trans Med Imaging. 2017;36(12):2536–45." href="/article/10.1186/s12880-022-00818-1#ref-CR13" id="ref-link-section-d107567642e2005">13</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Zhang B, Zhou J. Multi-feature representation for burn depth classification via burn images. Artif Intell Med. 2021;118:102128." href="#ref-CR53" id="ref-link-section-d107567642e2008">53</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Karimi D, Warfield SK, Gholipour A. Transfer learning in medical image segmentation: new insights from analysis of the dynamics of model parameters and learned representations. Artif Intell Med. 2021;116:102078." href="#ref-CR54" id="ref-link-section-d107567642e2008_1">54</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Nateghi R, Danyali H, Helfroush MS. A deep learning approach for mitosis detection: application in tumor proliferation prediction from whole slide images. Artif Intell Med. 2021;114:102048." href="#ref-CR55" id="ref-link-section-d107567642e2008_2">55</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Yoon H, Kim J, Lim HJ, Lee M-J. Image quality assessment of pediatric chest and abdomen ct by deep learning reconstruction. BMC Med Imaging. 2021;21:1–11." href="#ref-CR56" id="ref-link-section-d107567642e2008_3">56</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Wang T, Song N, Liu L, Zhu Z, Chen B, Yang W, Chen Z. Efficiency of a deep learning-based artificial intelligence diagnostic system in spontaneous intracerebral hemorrhage volume measurement. BMC Med Imaging. 2021;21(1):1–9." href="#ref-CR57" id="ref-link-section-d107567642e2008_4">57</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Guo K, Li X, Hu X, Liu J, Fan T. Hahn-PCNN-CNN: an end-to-end multi-modal brain medical image fusion framework useful for clinical diagnosis. BMC Med Imaging. 2021;21(1):1–22." href="#ref-CR58" id="ref-link-section-d107567642e2008_5">58</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 59" title="Sun J, Li H, Wang B, Li J, Li M, Zhou Z, Peng Y. Application of a deep learning image reconstruction (DLIR) algorithm in head CT imaging for children to improve image quality and lesion detection. BMC Med Imaging. 2021;21(1):1–9." href="/article/10.1186/s12880-022-00818-1#ref-CR59" id="ref-link-section-d107567642e2011">59</a>]. Kermany et al. [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 48" title="Kermany DS, et al. Identifying medical diagnoses and treatable diseases by image-based deep learning. Cell. 2018;172(5):1122–31." href="/article/10.1186/s12880-022-00818-1#ref-CR48" id="ref-link-section-d107567642e2015">48</a>] showed that CNN can detect diabetic macular edema and age-related macular degeneration with high accuracy and with a comparable performance of human experts. The authors also demonstrated the applicability of CNN in diagnosing pediatric pneumonia from chest X-ray images. Alexander et al. [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 47" title="Rakhlin A, Shvets A, Iglovikov V, Kalinin AA. Deep convolutional neural networks for breast cancer histology image analysis. In: ICIAR. Springer; 2018. p. 737–44." href="/article/10.1186/s12880-022-00818-1#ref-CR47" id="ref-link-section-d107567642e2018">47</a>] have provided the state-of-the-art performance (by the publication date) using CNN for histopathological image classification on the dataset of the ICIAR 2018 Grand Challenge on Breast Cancer Histology Images. Acharya [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 49" title="Acharya UR, et al. Automated identification of shockable and non-shockable life-threatening ventricular arrhythmias using convolutional neural network. Futur Gener Comput Syst. 2018;79:952–9." href="/article/10.1186/s12880-022-00818-1#ref-CR49" id="ref-link-section-d107567642e2021">49</a>] have shown that CNN can be used to accurately detect shockable and non-shockable life-threatening ventricular arrhythmias. Wachinger et al. [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 52" title="Wachinger C, Reuter M, Klein T. DeepNAT: deep convolutional neural network for segmenting neuroanatomy. Neuroimage. 2018;170:434–45." href="/article/10.1186/s12880-022-00818-1#ref-CR52" id="ref-link-section-d107567642e2024">52</a>] proposed DeepNAT, a CNN-based approach for automatic segmentation of NeuroAnaTomy in magnetic resonance images. The authors showed that their approach provided comparable results to those of state-of-the-art methods.</p></div></div></section><section data-title="Methods"><div class="c-article-section" id="Sec7-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec7">Methods</h2><div class="c-article-section__content" id="Sec7-content"><p>This section provides a description of the dataset used in this study, the pre-processing steps for the WBC images, and the proposed CNN-based architecture for WBC classification. The dataset was provided by The Catholic University of Korea (The CUK), and approved by the Institutional Review Board (IRB) of The CUK [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 60" title="The Catholic University of Korea Institutional Review Board. 
                  https://bit.ly/2YrlQPl
                  
                . Accessed: 2019-07-17." href="/article/10.1186/s12880-022-00818-1#ref-CR60" id="ref-link-section-d107567642e2036">60</a>]. The experimental protocols and informed consent were approved by the Institutional Review Board (IRB) of The CUK [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 60" title="The Catholic University of Korea Institutional Review Board. 
                  https://bit.ly/2YrlQPl
                  
                . Accessed: 2019-07-17." href="/article/10.1186/s12880-022-00818-1#ref-CR60" id="ref-link-section-d107567642e2039">60</a>].</p><h3 class="c-article__sub-heading" id="Sec8">Dataset</h3><p>We use a real-world dataset of 6562 images that belong to five WBC types, namely, neutrophil, eosinophil, basophil, lymphocyte, and monocyte. The dataset was provided by The Catholic University of Korea (The CUK), and approved by the Institutional Review Board (IRB) of The CUK [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 60" title="The Catholic University of Korea Institutional Review Board. 
                  https://bit.ly/2YrlQPl
                  
                . Accessed: 2019-07-17." href="/article/10.1186/s12880-022-00818-1#ref-CR60" id="ref-link-section-d107567642e2049">60</a>]. The images were captured by Sysmex DI-60 machine [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 61" title="Sysmex DI-60. 
                  https://bit.ly/313v6L3
                  
                . Accessed: 2019-07-17." href="/article/10.1186/s12880-022-00818-1#ref-CR61" id="ref-link-section-d107567642e2052">61</a>], and provided with 360 × 361 × 3 (3 channels, RGB) image size. Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1186/s12880-022-00818-1#Tab2">2</a> shows the number of images per class: 2006 neutrophils (NE) images, 1310 eosinophils (EO) images, 377 basophils (BA) images, 1676 lymphocytes (LY) images and 1193 monocytes (MO) images. The class distribution in our dataset is 30%, 20%, 6%, 26% and 18% for the five classes.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-2"><figure><figcaption class="c-article-table__figcaption"><b id="Tab2" data-test="table-caption">Table 2 The number of five type samples in the dataset</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1186/s12880-022-00818-1/tables/2" aria-label="Full size table 2"><span>Full size table</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><h3 class="c-article__sub-heading" id="Sec9">Pre-processing of WBC images</h3><p>Prior to the model creation and training, WBC images are pre-processed using three steps: ❶ image border cropping, ❷ image re-sizing, and ❸ image normalization. To eliminate the external borders of the image and to focus on the WBC, we remove the top 80 pixels, the bottom 81 pixels, the left 80 pixels, and the right 80 pixels of the image. The resulting cropped images, <i>i.e.,</i> images with a size of 200 × 200 × 3, are then re-sized to 128 × 128 × 3 for properly fitting them into a GPU memory and for efficient processing. Samples of the processed images are shown in Fig.&nbsp;<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1186/s12880-022-00818-1#Fig1">1</a>. The image normalization process was applied to reduce the heterogeneity of the RGB distribution in the images and to prevent over/underflow. This step is shown in Fig.&nbsp;<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1186/s12880-022-00818-1#Fig2">2</a>.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1" data-title="Fig. 1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1186/s12880-022-00818-1/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1186%2Fs12880-022-00818-1/MediaObjects/12880_2022_818_Fig1_HTML.png?as=webp"><img aria-describedby="Fig1" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1186%2Fs12880-022-00818-1/MediaObjects/12880_2022_818_Fig1_HTML.png" alt="figure 1" loading="lazy" width="685" height="144"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>Neutrophil, eosinophil, basophil, lymphocyte and monocyte from the left. These were cropped and rescaled with 128 × 128 × 3 for efficient training</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1186/s12880-022-00818-1/figures/1" data-track-dest="link:Figure1 Full size image" aria-label="Full size image figure 1" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2" data-title="Fig. 2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1186/s12880-022-00818-1/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1186%2Fs12880-022-00818-1/MediaObjects/12880_2022_818_Fig2_HTML.png?as=webp"><img aria-describedby="Fig2" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1186%2Fs12880-022-00818-1/MediaObjects/12880_2022_818_Fig2_HTML.png" alt="figure 2" loading="lazy" width="685" height="153"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>An overview of the pre-processing and the proposed CNN-based architecture for WBC image classification. The pre-processing consists of cropping, re-sizing and normalizing. Three convolutional layers (including three pooling layers) are in charge of extracting and learning features, and two fully connected layers are in charge of classification</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1186/s12880-022-00818-1/figures/2" data-track-dest="link:Figure2 Full size image" aria-label="Full size image figure 2" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><h3 class="c-article__sub-heading" id="Sec10">W-Net: architecture and design</h3><p>We introduce our CNN-based model architecture for WBC image classification. As illustrated in Fig.&nbsp;<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1186/s12880-022-00818-1#Fig2">2</a>, W-Net consists of three convolutional layers and two fully-connected layers, and they are responsible for extracting and learning features from WBC images to accurately classifying them into five classes using a softmax classifier. Each convolutional layer has a kernel size of 3 × 3 with stride of size 1 and uses ReLU activation function <i>f</i>(<i>x</i>) = <i>max</i>(0, <i>x</i>). The first convolutional layer has 16 filters, the second has 32 filters and third has 64 filters. After each convolutional layer, there is a max-pooling layer of size 2 × 2 with stride of size 2 and zero padding. We also use dropout regularization with <i>p</i> = 0.6 [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 62" title="Srivastava N, Hinton G, Krizhevsky A, Sutskever I, Salakhutdinov R. Dropout: a simple way to prevent neural networks from overfitting. J Mach Learn Res. 2014;15(1):1929–58." href="/article/10.1186/s12880-022-00818-1#ref-CR62" id="ref-link-section-d107567642e2269">62</a>] to prevent overfitting in each convolutional layer. The output of the third convolutional layer is flattened and fed into the first fully connected layer which has 1024 units. ReLU activation, and dropout with <i>p</i> = 0.6 are followed. The second fully connected layer has five units (five classes of WBC) and is followed by softmax classifier to map the output (features) to one of the five classes. The network has a total size of 16,806,949 trainable parameters. The model parameters were initialized using Xavier uniform initializer W ~ U[− x, x], where <i>x</i> = <i>sqrt</i>(6/(<i>in</i> + <i>out</i>)). The training of models is guided by minimizing the softmax-cross-entropy loss function <span class="mathjax-tex">\(- \sum\nolimits_{x} {p(x)log q(x)}\)</span>, where <span class="mathjax-tex">\(q(x_{i} ) = exp (x_{i} )/\sum\nolimits_{j} {exp (x_{j} ))}\)</span>  using Adam optimizer <span class="mathjax-tex">\(\Delta \theta_{t} = - n \cdot \hat{m}_{t} /\sqrt {\hat{v}_{t} + \epsilon }\)</span> [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 63" title="Kingma DP, Ba J. Adam: a method for stochastic optimization. arXiv preprint 
                  arXiv:1412.6980
                  
                 (2014)." href="/article/10.1186/s12880-022-00818-1#ref-CR63" id="ref-link-section-d107567642e2518">63</a>] with a learning rate of 0.0001. The training process is conducted with different batch sizes and terminated by the conclusion of 500 training epochs. The evaluation of the models is conducted using a tenfold cross-validation approach [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 64" title="Arlot S, Celisse A, et al. A survey of cross-validation procedures for model selection. Stat Surv. 2010;4:40–79." href="/article/10.1186/s12880-022-00818-1#ref-CR64" id="ref-link-section-d107567642e2521">64</a>]. The structure is illustrated in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1186/s12880-022-00818-1#Tab3">3</a>. The hyperparameters are described in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1186/s12880-022-00818-1#Tab4">4</a>. Design choices for W-Net are discussed in “<a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1186/s12880-022-00818-1#Sec20">Design considerations for W-Net</a>” section.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-3"><figure><figcaption class="c-article-table__figcaption"><b id="Tab3" data-test="table-caption">Table 3 The structure of five layers (Conv. and FC.) for W-Net</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1186/s12880-022-00818-1/tables/3" aria-label="Full size table 3"><span>Full size table</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-4"><figure><figcaption class="c-article-table__figcaption"><b id="Tab4" data-test="table-caption">Table 4 Hyperparameters for all the models</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1186/s12880-022-00818-1/tables/4" aria-label="Full size table 4"><span>Full size table</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div></div></div></section><section data-title="Experiments"><div class="c-article-section" id="Sec11-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec11">Experiments</h2><div class="c-article-section__content" id="Sec11-content"><p>We show the performance of W-Net for WBC classification and compare the softmax classifier of W-Net with SVM. We show that W-Net provides remarkable results in the WBC classification by comparing it to the prior work. We also show how the number of layers affects performance. The comparison includes AlexNet, VGGNet, ResNet and RNN models. For transfer learning, we provide insights on adopting pre-trained W-Net to gain higher WBC classification performance on public datasets. ROC curve and AUC are a useful method for evaluating a system in medical area and are usually used to classify a binary task such as a diagnosis. However, we remark that our results are only based on an accuracy, because the output of our model is multiple-class not the binary.</p><h3 class="c-article__sub-heading" id="Sec12">W-Net performance</h3><p>Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1186/s12880-022-00818-1#Tab10">10</a> in “Appendix <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1186/s12880-022-00818-1#Sec28">1</a>” shows the accuracy achieved by W-Net using tenfold cross-validation approach. Conducting the experiments required 33.87&nbsp;h of model’s training time. For the neutrophil, 1800 images were used for training and 206 images were used for testing in each fold, and the average accuracy was 98%. For the eosinophil, 1179 images were used for training and 131 images were used for testing in each fold, and the average accuracy was 97%. For the basophil, 340 images were used for training and 37 images were used for testing in each fold, and the average accuracy was 95%. For the lymphocyte, 1509 images were used for training and 167 images were used for testing in each fold, and the average accuracy was 97%. For the monocyte, 1074 images were used for training and 119 images were used for testing in each fold, and the average accuracy was 97%. The average overall accuracy of the five WBC classes was 97%. As shown in Fig.&nbsp;<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1186/s12880-022-00818-1#Fig3">3</a>, it provides ROC curve and Precision-Recall (PR) curve in (a) and (b) respectively, based on the idea of one vs rest for multi-class classification. Each line in (a) represents each class of the five WBC classes, and our W-Net model achieved an AUC of 0.97 on average on ROC curve. On the one hand, it achieved an AUC of 0.98 on average on PR curve.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3" data-title="Fig. 3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1186/s12880-022-00818-1/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1186%2Fs12880-022-00818-1/MediaObjects/12880_2022_818_Fig3_HTML.png?as=webp"><img aria-describedby="Fig3" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1186%2Fs12880-022-00818-1/MediaObjects/12880_2022_818_Fig3_HTML.png" alt="figure 3" loading="lazy" width="685" height="267"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p><b>a</b> Provides ROC curve of our W-Net model based on the idea of one versus rest for multi-class classification, and <b>b</b> shows Precision–Recall curve. In <b>a</b>, each class achieves an AUC of 0.97 on average and achieves an AUC of 0.98 on average in <b>b</b></p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1186/s12880-022-00818-1/figures/3" data-track-dest="link:Figure3 Full size image" aria-label="Full size image figure 3" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><h3 class="c-article__sub-heading" id="Sec13">W-Net versus W-Net-SVM performance</h3><p>We compared softmax classifier of W-Net with SVM to demonstrate classifier’s abilities in performing the WBC classification task. We trained a W-Net model with SVM classifier (W-Net-SVM) using hinge loss function [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 65" title="Rosasco L, Vito ED, Caponnetto A, Piana M, Verri A. Are loss functions all the same? Neural Comput. 2004;16(5):1063–76." href="/article/10.1186/s12880-022-00818-1#ref-CR65" id="ref-link-section-d107567642e3104">65</a>] <i>l</i>(<i>y</i>) = <i>max</i>(0, 1 −<i> t</i>ㆍ<i>y</i>) instead of softmax (W-Net). We followed the same experimental settings adopted in previous experiment including the training parameters, dataset, pre-processing steps, workstation environment, and tenfold cross-validation approach for the evaluation. The network has a total of 16,806,949 trainable parameters. Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1186/s12880-022-00818-1#Tab11">11</a> in “Appendix <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1186/s12880-022-00818-1#Sec28">1</a>” shows the performance of W-Net-SVM using tenfold cross-validation in the WBC classification task. The training time of W-Net-SVM was 33.79 h. The achieved results for the neutrophil, eosinophil, basophil, lymphocyte, and monocyte classes are 98%, 97%, 87%, 98%, and 97%, respectively. The overall average accuracy of the five classes was 95%.</p><h3 class="c-article__sub-heading" id="Sec14">WBC classification with AlexNet</h3><p>This experiment adopts AlexNet architecture in the WBC classification task. AlexNet network consists of five convolutional layers and three fully-connected layers which apply ReLU activation function (in all layers except the last (softmax) layer). The training of AlexNet model is conducted by minimizing the softmax-cross-entropy loss function using the momentum optimizer <span class="mathjax-tex">\(\theta_{t} = - \gamma \nu_{t - 1} - \eta g_{t}\)</span> [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 66" title="Sutskever I, Martens J, Dahl G, Hinton G. On the importance of initialization and momentum in deep learning. In: International conference on ML; 2013. p. 1139–47." href="/article/10.1186/s12880-022-00818-1#ref-CR66" id="ref-link-section-d107567642e3194">66</a>]. Using a cross-validation approach, the best training hyperparameters that achieved the best WBC classification accuracy are described in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1186/s12880-022-00818-1#Tab4">4</a>. We follow the same experimental settings adopted in previous experiments by using same dataset, pre-processing steps (except for the image size, we re-sized the images to 224 × 224 × 3 for AlexNet), workstation environment, and the tenfold cross-validation evaluation approach. The AlexNet-based network has a total of 46,767,493 trainable parameters. Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1186/s12880-022-00818-1#Tab12">12</a> in “Appendix <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1186/s12880-022-00818-1#Sec28">1</a>” shows the performance of AlexNet using a tenfold cross-validation approach in the WBC classification task. The overall average accuracy is 84% (see Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1186/s12880-022-00818-1#Tab12">12</a> for details).</p><h3 class="c-article__sub-heading" id="Sec15">WBC classification with VGGNet</h3><p>We compared W-Net with VGGNet to demonstrate the effectiveness of W-Net in the WBC image classification. We trained a VGGNet-based model that consists of 16 convolutional layers and three full-connected layers, which followed with ReLU activation function. The model training is conducted using the minimization of the softmax-cross-entropy loss though Adam optimizer. Using a cross-validation method, the best training hyperparameters are described in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1186/s12880-022-00818-1#Tab4">4</a>. This experiment followed the same experimental settings adopted in previous experiments. The VGGNet-based model includes a total of 121,796,165 trainable parameters. The training of the VGGNet-based model required 510.59 h of training time. Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1186/s12880-022-00818-1#Tab13">13</a> in “Appendix <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1186/s12880-022-00818-1#Sec28">1</a>” shows the results of the tenfold cross-validation of VGGNet-based model in the WBC classification. The overall average accuracy of the five classes is 44% (see Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1186/s12880-022-00818-1#Tab13">13</a> for details).</p><h3 class="c-article__sub-heading" id="Sec16">WBC classification with ResNet</h3><p>We adopt ResNet50 and ResNet18 networks for WBC classification, which consists of 50 and 18 convolutional layers, respectively. Both models are trained by minimizing the softmax-cross-entropy loss using momentum optimizer. Using a cross-validation approach, the best training hyperparameters to achieve the highest accuracy in the WBC classification task are described in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1186/s12880-022-00818-1#Tab4">4</a>. The training and evaluation of the models are in compliance with experimental settings adopted in previous experiments. The ResNet50 and ResNet18 models include a total of 23,544,837 and 14,722,931 trainable parameters, respectively. It required a training time of 8.38&nbsp;h for ResNet50, and 3.51&nbsp;h for ResNet18. Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1186/s12880-022-00818-1#Tab14">14</a> in “Appendix <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1186/s12880-022-00818-1#Sec28">1</a>” shows the classification accuracy obtained by of ResNet50 model using the tenfold cross-validation approach, while Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1186/s12880-022-00818-1#Tab15">15</a> shows the results of ResNet18. The overall average accuracy of the five classes for ResNet50 is 51%. On the one hand, ResNet18 achieved the overall average accuracy of the five classes is 79% (see Tables <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1186/s12880-022-00818-1#Tab14">14</a> and <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1186/s12880-022-00818-1#Tab15">15</a> for details, respectively).</p><h3 class="c-article__sub-heading" id="Sec17">WBC classification with RNN</h3><p>We explore the capabilities of RNN in the WBC classification task. Using RNN for WBC image classification, we adopted the common approach by considering the image rows as sequences and the columns as timesteps. Since we the WBC images are 128 × 128 × 3 images, we feed the model with batches of 128 sequences of size 128 × 3. The RNN model adopted in this experiment consists of only one single hidden layer. The experimental settings for the training process are set with the following search space: learning rate = 0.0001, 0.001, 0.003, 0.01, 0.1, 0.3, batch size = 16, 32, 64, 128 and hidden units = 16, 32, 128, 256, 512, 1024. For hyper-parameter selection, we split 6562 images into train/test/validation sets by 5504/512/546 ratio. The best test accuracy was achieved when using a learning rate of 0.01, batch size of 64, and 32 hidden LSTM units, as described in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1186/s12880-022-00818-1#Tab4">4</a>. Once hyperparameters are selected, we conducted a new training process using a tenfold evaluation approach, where 10 different models are trained and evaluated using ten fold splits (each time a model is trained on nine folds and tested on one fold). The achieved accuracy for the individual classes are as follows: neutrophil 89%, eosinophil 88%, basophil 57%, lymphocyte 93%, and monocyte 90%. The results are shown in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1186/s12880-022-00818-1#Tab16">16</a>, “Appendix <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1186/s12880-022-00818-1#Sec28">1</a>”. The average accuracy of the five classes is 83%.</p><h3 class="c-article__sub-heading" id="Sec18">Models comparison for WBC classification</h3><p>Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1186/s12880-022-00818-1#Tab5">5</a> shows a summary of the results achieved by the different models, namely, W-Net, W-Net with SVM, AlexNet, VGGNet, ResNet, and RNN, using our dataset. The reported results are the average score of different accuracy metrics, accuracy, precision, recall, and F1-score. For W-Net, the accuracy, precision, recall, and F1-score are all 97%. For W-Net-SVM, they are 95%, 97%, 95% and 96% respectively. For Alexnet, they are 84%, 94%, 84% and 85% respectively. For VGGNet, they are 44%, 67%, 44% and 42% respectively. For ResNet, they are 51%, 60%, 51% and 43% respectively. For the RNN model, they are 83%, 86%, 85% and 85% respectively. The results show that W-Net outperforms other RNN- and CNN-based model’s architectures and an architecture with a small number of layers is also better than an architecture with many layers. The detailed results of tenfold cross validation for all experiments are in “Appendix <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1186/s12880-022-00818-1#Sec28">1</a>”.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-5"><figure><figcaption class="c-article-table__figcaption"><b id="Tab5" data-test="table-caption">Table 5 The result of accuracy, precision, recall, F1-score on average and the number of layers for all experiments</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1186/s12880-022-00818-1/tables/5" aria-label="Full size table 5"><span>Full size table</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><h3 class="c-article__sub-heading" id="Sec19">Further training with public data</h3><p>The LISC public dataset [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 17" title="Rezatofighi SH, Soltanian-Zadeh H. Automatic recognition of five types of white blood cells in peripheral blood. Comput Med Imaging Graph. 2011;35(4):333–43." href="/article/10.1186/s12880-022-00818-1#ref-CR17" id="ref-link-section-d107567642e3621">17</a>] includes WBC images of size 720 × 576 × 3 that were collected from peripheral blood of eight normal people. The images are classified by a hematologist into five types of WBC: neutrophils, eosinophils, basophils, lymphocytes and monocytes. For pre-processing the public dataset, we cropped the WBC images (nucleus and cytoplasm regions) in the original images, and then re-sized the images to 128 × 128 × 3 for training. We used a total of 254 WBC images as our dataset: 56, 39, 55, 56 and 48 images for neutrophil, eosinophil, basophil, lymphocyte and monocyte, respectively. Using the LISC public data, this experiment shows the performance of W-Net when adopted for different datasets. Moreover, we show the performance of W-Net using transfer learning when a pre-trained W-Net is fine-tuned to classify WBCs from different dataset or used for different WBC-related tasks. To this end, we conducted two experiments as follows: ❶ W-Net architecture is used for building a WBC classifier trained using only the LISC public data, ❷ a pre-trained W-Net with softmax classifier from “<a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1186/s12880-022-00818-1#Sec12">W-Net performance</a>” section is fine-tuned to classify WBCs from LISC public data. Except the training epochs, the training hyperparameters are set to be identical in both experiments. In the first experiment, W-Net-based model was trained from scratch using 4000 training epochs (254 × 4000/5 iterations) on the LISC public data. The training process was concluded after 10.33&nbsp;h. In the second experiment, we establish a pre-trained W-Net-based model (trained on our dataset for 500 training epochs.) to be used on the LISC public data. The pre-trained W-Net-based model was fine-tuned for 4000 epochs (254 × 4000/5 iterations) on the public data. The training process was concluded after 10.83&nbsp;h. Table  <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1186/s12880-022-00818-1#Tab16">16</a> in “Appendix <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1186/s12880-022-00818-1#Sec28">1</a>” shows the result of the first experiment where W-Net is used to classify WBCs from the LISC public data. The achieved results is an average accuracy of 91%. Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1186/s12880-022-00818-1#Tab17">17</a> in “Appendix <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1186/s12880-022-00818-1#Sec28">1</a>” shows the result of the experiment. The average accuracy achieved using a pre-trained W-Net model is 96%.</p><p>In the results, the second experiment shows a better performance. This result shows that training a model in large-scale dataset (such as the one used for this study) can benefit other transfer learning tasks, where the model is fine-tuned to other dataset or performing other WBC-related tasks. We share our pre-trained model on GitHub [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 67" title="Jung, C. W-Net model and generated WBC images. 2022. 
                  https://bit.ly/2KAffwM
                  
                . Accessed: 2022-3-24." href="/article/10.1186/s12880-022-00818-1#ref-CR67" id="ref-link-section-d107567642e3643">67</a>] and believe that using the transfer learning property (transfer learning in the same domain) of deep learning models can help other researchers in the field.</p></div></div></section><section data-title="Design considerations for W-Net"><div class="c-article-section" id="Sec20-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec20">Design considerations for W-Net</h2><div class="c-article-section__content" id="Sec20-content"><p>Design choices for our deep learning architecture are described in this section. There are two challenging issues to consider in choosing a specific architecture in the large design space for WBC classification problem: One is how to figure out the data imbalance problem, and the other is to classify similar-looking images into the relatively small number of classes. In many datasets in real world, data imbalance is quite common and WBC images resembles way more each other compared to objects in traditional image classification problems. Also, the number of classes is quite limited compared with the traditional object identification problems such as ImageNet challenge. Therefore, it is necessary to take a different approach to the classification problem.</p><h3 class="c-article__sub-heading" id="Sec21">Handling data imbalance: large batch and sampling</h3><p>The results show that W-Net performs well despite the dataset’s imbalance, which is observed by the number of samples for each class. Even though the least-represented class in the dataset (basophil with 6% of the dataset) show the least accuracy of 95% in comparison to other classes, this accuracy is still higher than the results achieved by other methods, <i>e.g.,</i> CNN-based and RNN-based models, for the same class. This performance can be due to several reasons. For instance, the evaluation of all experiments follows a stratified k-folds cross-validation approach, which preserves the percentage of samples across all folds. Using this approach allows the sampling from all classes in different ratios in each fold, which dictates the inclusion of all classes in both the training and testing phases. When using a small batch size, <i>e.g.,</i> five samples as adopted during the training of W-Net, the error resulting from misclassifying one class, especially from underrepresented classes, highly impacts the average cost of the learning epoch and contributes in an effective learning process for these classes. In contrast, using a large batch size and considering a random sampling scheme for batching could result in minimizing the effect of misclassification of underrepresented classes since performing well on other classes could out-weigh the misclassification of small, if any at all, samples from classes with small ratios in the dataset.</p><p>Having different distributions of image samples per class is a hard part to classify WBC images. W-Net achieves an accuracy of 95% for identifying the basophil class which are represented with the least number of samples (377 samples and a ratio of 6% of the dataset). This result is remarkable knowing that all other CNN-based and RNN-based models achieved an accuracy below 56% and 57%, respectively, for the same class. The overall average accuracy of W-Net is 97%, which is the highest among other methods for WBC classification. Considering the results for this large-scale dataset, W-Net presents a state-of-the-art performance.</p><p>Furthermore, the result of W-Net with softmax classifier is 97%, the result of W-Net with SVM classifier is 95% and they seem similar. However, for the basophil class that has 6% distribution of our dataset, the accuracy of W-Net with SVM is only 87% and it is lower than 95% the result of softmax. The W-Net-SVM uses the hinge loss function, while W-Net uses the softmax cross-entropy loss function. The nature of optimization under these functions differs since the optimization using the hinge loss concludes when finding parameters that satisfy the classification with the predefined margin. However, using softmax cross-entropy loss keeps the optimization going beyond a specific margin pushing the decision boundaries further. This allows the model to maintain robust generalization capabilities, hence the better performance of W-Net over W-Net-SVM. AlexNet has many layers than our W-Net, however, the average accuracy is 84%, and especially the average accuracy of the basophil class that has 6% distribution of our dataset is 33%. This means the SVM classifier and the network of AlexNet are not appropriate to address the unbalanced dataset. As a result, we can claim that W-Net with softmax classifier is more effective than AlexNet and W-Net with SVM classifier in WBC image classification area.</p><h3 class="c-article__sub-heading" id="Sec22">WBC dedicated architecture with shallow depth</h3><p>In the tenfold cross-validation evaluation of W-Net, the minimum average accuracy is 91% (basophil, Fold-9) and maximum average accuracy is 100%. However, in the case of VGGNet and ResNet50 architectures which have more depth (considering the number of layers), the variance between the folds is from 0 to 100% resulting in 44% tenfold average accuracy, and from 0 to 100% resulting in 51% tenfold average accuracy, respectively. In a comparison between ResNet50 and ResNet18, since ResNet18 consists of a shallower layer than ResNet50, the overfitting problem seems to occur less. It leads that ResNet18 shows better performance with 79% on average than ResNet50. This means that very deep networks may not be the optimal choice for WBC image classification. Most of the state-of-the-art CNN-based models (e.g., AlexNet, VGGNet, and ResNet) use larger receptive fields, (e.g., 7 × 7 in case ResNet and 11 × 11 in the case of AlexNet), which seem to work better on larger images with larger objects (classes). However, handling the WBC classification task requires adopting smaller filters to bring attention to finer receptive fields that hold relevant features.</p><p>The results of this research show that architectures such as W-Net’s, which has five layers (three convolutional and two fully-connected.), can be sufficient and more effective in the WBC classification task in comparison to other deeper networks such as VGGNet, ResNet50 and ResNet18. In general, deep networks are known to perform well for the image classification, the VGGNet and ResNet with deep networks show good performance in ILSVRC. However, they did not show good performance in WBC image dataset. We claim that our dataset to be classified is different from the dataset aimed by those deep networks in two aspects: (1) the ILSVRC dataset has 1000 classes, but our WBC dataset has only five classes, and (2) The images of the ILSVRC dataset are very different from each other (For example, they are dog, bird, flower and food etc.), while our dataset has very high visual similarity.</p><p>To support this claim we conducted two simple experiments, ❶ the first experiment was to run W-Net on 200 classes (bird, ball and car etc.) of images from Tiny ImageNet dataset [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 68" title="Tiny ImageNet. 
                  https://bit.ly/36Qxvfp
                  
                . Accessed: 2020-01-13." href="/article/10.1186/s12880-022-00818-1#ref-CR68" id="ref-link-section-d107567642e3688">68</a>] and ❷ the second experiment was to run W-Net on five classes without visual similarity (fish, clothes, chair, car and teddy bear) from Tiny ImageNet dataset with the same (imbalance) distribution of our WBC dataset. In these two experiments, we only used different dataset with our WBC dataset, and used same network, parameters (learning rate and training epoch etc.) and tenfold cross-validation approach with our W-Net. In the first experiment, we used the dataset with 200 classes, and each class had 500 images. We used total 100,000 images. The result from the first experiment showed 100% accuracy for the 200th class, but 0% accuracy for the other 199 classes. The average accuracy was 0.5%, it showed that the model was not trained at all. In the second experiment, we used the dataset with 5 classes, and each class had 500, 333, 100, 433, 300 (making them have the same distribution with our dataset) images. We used total 1666 images. The result from the second experiment showed 34% accuracy for the third class (100 chair images), and 84%, 78%, 90% and 65% for other classes, respectively. The average accuracy was 79%, which was not as good as the results of W-Net using our dataset. Therefore, we claim that a simple network may be better to classify our WBC dataset with data distribution imbalance, small number of classes, and visual similarity.</p><h3 class="c-article__sub-heading" id="Sec23">Why not RNN?</h3><p>RNN-based models perform well in sequential data and show remarkable results in capturing temporal dependencies within the input data. There are different variations of RNN, and for our experiments we used LSTM models for their abilities to handle long-term dependencies (<i>e.g.,</i> 128 sequences in our application) and the vanishing gradient problem. The average achieved results when using one-layer LSTM model with 32 hidden units is 83%. This result is far from the results achieved by W-Net (97%).</p><p>However, it outperforms other CNN-based models such as VGGNet (44%) and ResNet50 (51%). Karol et al. [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 69" title="Gregor K, Danihelka I, Graves A, Rezende D, Wierstra D. Draw: a recurrent neural network for image generation. In: International conference on machine learning. PMLR (2015, June). p. 1462–71." href="/article/10.1186/s12880-022-00818-1#ref-CR69" id="ref-link-section-d107567642e3705">69</a>] have also shown that RNN can encode independent scenes within an image instead of processing the entire image as a single input. Adopting sequential processing of white blood images via LSTM, enables the model to extract/adapt to patterns/changes in the scene to build a more robust model than following single-shot processing.</p></div></div></section><section data-title="Dataset sharing"><div class="c-article-section" id="Sec24-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec24">Dataset sharing</h2><div class="c-article-section__content" id="Sec24-content"><p>Recent advances in big data have also led to advances in deep learning, accordingly having a good dataset has become important. In this section, we generate new WBC image samples using Generative Adversarial Networks (GAN) [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 18" title="Goodfellow I, et al. Generative adversarial nets. In: Advances in NIPS; 2014. p. 2672–80." href="/article/10.1186/s12880-022-00818-1#ref-CR18" id="ref-link-section-d107567642e3718">18</a>] then release them in public for education and research to help other researchers. GAN is a deep learning architecture for generating new artificial samples, it composes of two deep networks: ❶ the generator G, and ❷ the discriminator <i>D</i>. The <i>G</i> generates new samples from the domain, and the <i>D</i> classifies whether the samples are real or fake. The output of the <i>D</i> is used to update both the model weights of the <i>D</i> itself and the <i>G</i>. Accordingly, the performance of the <i>G</i> depends on how well the <i>D</i> performs. GAN can be expressed by: <span class="mathjax-tex">\(\mathop {min }\limits_{G} \mathop {max }\limits_{D} V(D,G) = {\mathbb{E}}_{{x \sim p_{data} (x)}} [log D(x)] + {\mathbb{E}}_{{z \sim p_{z} (z)}} [log (1 - D(G(z)))]\)</span>, where <i>x</i> ~ <i>p</i><sub><i>data</i></sub>(<i>x</i>) and <i>z</i> ~ <i>p</i><sub><i>z</i></sub>(<i>z</i>) indicate the distribution of a real data and a fake data respectively, the <i>D</i> aims to maximize <i>logD</i>(<i>x</i>) and <i>G</i> aims to minimize <i>log</i>(1 − <i>D</i>(<i>G</i>(<i>z</i>))), to maximize the chance to recognize real images as real and generated images as fake. This expression defines GAN as a minimax game.</p><h3 class="c-article__sub-heading" id="Sec25">Experimental settings</h3><p>We use the same dataset (6562 WBC images of size of 128 × 128 × 3), similar experimental settings of previous experiments, and Deep Convolutional Generative Adversarial Network (DCGAN) [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 70" title="Radford A, Metz L, Chintala S. Unsupervised representation learning with deep convolutional generative adversarial networks. arXiv preprint 
                  arXiv:1511.06434
                  
                 (2015)." href="/article/10.1186/s12880-022-00818-1#ref-CR70" id="ref-link-section-d107567642e4015">70</a>] to train (<i>G</i> and <i>D</i>) models for generating images. For the network of <i>D</i>, six convolutional layers, one fully connected layer, LeakyReLU [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 71" title="Maas AL, Hannun AY, Ng AY. Rectifier nonlinearities improve neural network acoustic models. In: Proceedings of ICML, vol 30; 2013. p. 3." href="/article/10.1186/s12880-022-00818-1#ref-CR71" id="ref-link-section-d107567642e4027">71</a>] activation, sigmoid activation and dropout are used. For the network of <i>G</i>, six convolutional layers, one fully connected layer, ReLU activation, sigmoid activation, dropout, and batch normalization [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 72" title="Ioffe S, Szegedy C. Batch normalization: accelerating deep network training by reducing internal covariate shift. arXiv preprint 
                  arXiv:1502.03167
                  
                 (2)." href="/article/10.1186/s12880-022-00818-1#ref-CR72" id="ref-link-section-d107567642e4034">72</a>] are used. The training hyperparameters are set as follows: alpha 0.2, momentum 0.9, batch size 1, learning rate 0.00001, dropout 0.6, and training epochs 10,000. The network of <i>G</i> and <i>D</i> have a total of 2,780,099 and 69,878,401 trainable parameters. It took 191.66, 120.13, 34.44, 158.33 and 91.66&nbsp;h to train <i>G</i> and <i>D</i> models for five WBC classes, and it took an average of 18&nbsp;min to generate 1000 images per each class. We generated 1000 plausible WBC images of size of 128 × 128 × 3 for each class (a total of 5000 images). Figure&nbsp;<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1186/s12880-022-00818-1#Fig4">4</a> shows the samples of both the original images (left side) for training DCGAN model and the generated images (right side) by trained DCGAN model. The first row of the Fig.&nbsp;<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1186/s12880-022-00818-1#Fig4">4</a> is the neutrophil class, followed by the eosinophil, the basophil, the lymphocyte, and the monocyte.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4" data-title="Fig. 4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1186/s12880-022-00818-1/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1186%2Fs12880-022-00818-1/MediaObjects/12880_2022_818_Fig4_HTML.png?as=webp"><img aria-describedby="Fig4" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1186%2Fs12880-022-00818-1/MediaObjects/12880_2022_818_Fig4_HTML.png" alt="figure 4" loading="lazy" width="685" height="300"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>Left side: the original images of size of 128 × 128 × 3 for training DCGAN model. Right side: the synthesized images of size of 128 × 128 × 3 by trained DCGAN model. The first row is the neutrophil class, followed by the eosinophil, the basophil, the lymphocyte, and the monocyte classes</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1186/s12880-022-00818-1/figures/4" data-track-dest="link:Figure4 Full size image" aria-label="Full size image figure 4" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><h3 class="c-article__sub-heading" id="Sec26">Generated image quality</h3><p>To see how similar images were generated from the original images, we verified the generated WBC images using ❶ baseline-W-Net, ❷ generative-W-Net (<i>i.e.,</i> W-Net trained on generated synthetic dataset), ❸ cosine similarity, and ❹ domain-expert experiment with a medical laboratory specialist. First, we experimented to classify the generated images using W-Net. Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1186/s12880-022-00818-1#Tab6">6</a> shows the confusion matrix for the results achieved for the classification of the generated WBC im-ages. The second column indicates true classes, the second row indicates predicted classes, and the images are well-classified with 100% accuracy by W-Net. Second, we trained W-Net model using the 5000 generated synthetic images. For the training, we follow the same experimental settings of creating the baseline-W-Net. Then, we evaluated the generative-W-Net for classifying the 6562 real WBC images. Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1186/s12880-022-00818-1#Tab7">7</a> shows the confusion matrix for the results achieved for the classification of real WBC images using generative-W-Net. The images are classified with an accuracy of 95%, precision of 93%, recall of 95%, and F1-score of 94%. Third, we measure the similarity between the original images and the generated images using cosine similarity. We first measure the cosine similarity between the original images and the original images for each class (<i>e.g.,</i> 377 vs. 377 for the basophil class), then we measure the cosine similarity between the original images and the 1000 generated WBC images for each class (<i>e.g.,</i> 377 vs. 1000 for the basophil class) and then we compare them. Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1186/s12880-022-00818-1#Tab8">8</a> shows the difference in the cosine similarity between the original images and generated images. It was 4% for the neutrophil, 3% for the eosinophil, 7% for the basophil, 6% for the lymphocyte, and 6% for the monocyte with average 5% for five classes. Fourth, we conducted a domain-expert experiment on how well a medical laboratory specialist could classify the generated WBC images. The dataset used in this experiment consists of 10 random original images and 10 random generated images for each class, <i>i.e.,</i> a total of 100 images. Without informing the medical laboratory specialist of the source of the WBC images in the dataset, we asked for the classification of provided images. Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1186/s12880-022-00818-1#Tab9">9</a> shows the confusion matrix for this experiment. The results show that the specialist well-classified the given WBC samples with an accuracy of 95%. Among the five misclassified images, there are three original images and only two generated images. The results of all verification methods for the generated images show that the generated images are similar to the original images. We released the generated (labeled) WBC images on GitHub [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 67" title="Jung, C. W-Net model and generated WBC images. 2022. 
                  https://bit.ly/2KAffwM
                  
                . Accessed: 2022-3-24." href="/article/10.1186/s12880-022-00818-1#ref-CR67" id="ref-link-section-d107567642e4106">67</a>] for the education and research purposes.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-6"><figure><figcaption class="c-article-table__figcaption"><b id="Tab6" data-test="table-caption">Table 6 The confusion matrix for classification experiment result with generated WBC images using W-Net model</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1186/s12880-022-00818-1/tables/6" aria-label="Full size table 6"><span>Full size table</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-7"><figure><figcaption class="c-article-table__figcaption"><b id="Tab7" data-test="table-caption">Table 7 The confusion matrix for classification experiment result with real WBC images using the fake W-Net model</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1186/s12880-022-00818-1/tables/7" aria-label="Full size table 7"><span>Full size table</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-8"><figure><figcaption class="c-article-table__figcaption"><b id="Tab8" data-test="table-caption">Table 8 The difference in the cosine similarity between the original images and generated images</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1186/s12880-022-00818-1/tables/8" aria-label="Full size table 8"><span>Full size table</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-9"><figure><figcaption class="c-article-table__figcaption"><b id="Tab9" data-test="table-caption">Table 9 The confusion matrix for the user experiment result with the medical laboratory technologist</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1186/s12880-022-00818-1/tables/9" aria-label="Full size table 9"><span>Full size table</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div></div></div></section><section data-title="Conclusion"><div class="c-article-section" id="Sec27-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec27">Conclusion</h2><div class="c-article-section__content" id="Sec27-content"><p>Analysis of WBC images is essential for diagnosing leukemia. Although there are several methods for detecting and counting WBCs from microscopic images of a blood smear, the classification of the five types of WBCs is still a challenge in real-life applications, which we addressed in this work. The rapid growth in the area of computer vision and machine/deep learning have provided feasible solutions to classification tasks in many domains. This work proposed W-Net, a CNN-based architecture with a small number of layers, to accurately classify the five WBC types. We evaluated W-Net on a real-world large-scale dataset and addressed several challenges such as the transfer learning property and the class imbalance. W-Net achieved an average classification accuracy of 97%. Moreover, we compared the result of W-Net with W-Net with SVM, AlexNet, VGGNet, ResNet and RNN architectures to show the superiority of W-Net which consists of three layers over other architecture. We synthesized a dataset of new WBC image samples using DCGAN, which we released to the public for education and research purposes.</p><p>Even though our W-Net model provides good performance with an average classification accuracy of 97%, it still remains an error of 3%. In the future work, we will conduct the dataset augmentation using our generative model based on DCGAN, to address the dataset imbalance. Then, we will carry out additional experiments to further increase the accuracy performance of the classification model with the balanced dataset.</p></div></div></section>
                                </div>
                        
                    

                    <section data-title="Availability of data and materials"><div class="c-article-section" id="availability-of-data-and-materials-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="availability-of-data-and-materials">Availability of data and materials</h2><div class="c-article-section__content" id="availability-of-data-and-materials-content">
              
              <p>Our pre-trained model and the generated WBC images is available for scientific and education purposes on on GitHub [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 67" title="Jung, C. W-Net model and generated WBC images. 2022. 
                  https://bit.ly/2KAffwM
                  
                . Accessed: 2022-3-24." href="/article/10.1186/s12880-022-00818-1#ref-CR67" id="ref-link-section-d107567642e5042">67</a>] (<a href="https://bit.ly/3jfB7yA">https://bit.ly/3jfB7yA</a>, <a href="https://bit.ly/3pM3Ptf">https://bit.ly/3pM3Ptf</a>).</p>
            </div></div></section><div id="MagazineFulltextArticleBodySuffix"><section aria-labelledby="Bib1" data-title="References"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references" data-track-component="outbound reference" data-track-context="references section"><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="1."><p class="c-article-references__text" id="ref-CR1">Changhun J, Mohammed A, Jumabek A, Aziz M, Kyungja H, DaeHun N. W-Net: a CNN-based architecture for white blood cells image classification. In: AAAI 2019 fall symposium on AI for social good; 2019.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="2."><p class="c-article-references__text" id="ref-CR2">Pillay J, et al. In vivo labeling with <sup>2</sup>H<sub>2</sub>O reveals a human neutrophil lifespan of 5.4 days. Blood. 2010;116(4):625–7.</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1182/blood-2010-01-259028" data-track-item_id="10.1182/blood-2010-01-259028" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1182%2Fblood-2010-01-259028" aria-label="Article reference 2" data-doi="10.1182/blood-2010-01-259028">Article</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="cas reference" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC3cXhtVClu7vJ" aria-label="CAS reference 2">CAS</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=20410504" aria-label="PubMed reference 2">PubMed</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 2" href="http://scholar.google.com/scholar_lookup?&amp;title=In%20vivo%20labeling%20with%202H2O%20reveals%20a%20human%20neutrophil%20lifespan%20of%205.4%20days&amp;journal=Blood&amp;doi=10.1182%2Fblood-2010-01-259028&amp;volume=116&amp;issue=4&amp;pages=625-627&amp;publication_year=2010&amp;author=Pillay%2CJ">
                    Google Scholar</a>&nbsp;
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="3."><p class="c-article-references__text" id="ref-CR3">Rothenberg ME, Hogan SP. The eosinophil. Annu Rev Immunol. 2006;24:147–74.</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1146/annurev.immunol.24.021605.090720" data-track-item_id="10.1146/annurev.immunol.24.021605.090720" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1146%2Fannurev.immunol.24.021605.090720" aria-label="Article reference 3" data-doi="10.1146/annurev.immunol.24.021605.090720">Article</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="cas reference" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BD28XkvFSqt70%3D" aria-label="CAS reference 3">CAS</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=16551246" aria-label="PubMed reference 3">PubMed</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 3" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20eosinophil&amp;journal=Annu%20Rev%20Immunol&amp;doi=10.1146%2Fannurev.immunol.24.021605.090720&amp;volume=24&amp;pages=147-174&amp;publication_year=2006&amp;author=Rothenberg%2CME&amp;author=Hogan%2CSP">
                    Google Scholar</a>&nbsp;
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="4."><p class="c-article-references__text" id="ref-CR4">Falcone FH, Haas H, Gibbs BF. The human basophil: a new appreciation of its role in immune responses. Blood. 2000;13:4028–38.</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1182/blood.V96.13.4028" data-track-item_id="10.1182/blood.V96.13.4028" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1182%2Fblood.V96.13.4028" aria-label="Article reference 4" data-doi="10.1182/blood.V96.13.4028">Article</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 4" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20human%20basophil%3A%20a%20new%20appreciation%20of%20its%20role%20in%20immune%20responses&amp;journal=Blood&amp;doi=10.1182%2Fblood.V96.13.4028&amp;volume=13&amp;pages=4028-4038&amp;publication_year=2000&amp;author=Falcone%2CFH&amp;author=Haas%2CH&amp;author=Gibbs%2CBF">
                    Google Scholar</a>&nbsp;
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="5."><p class="c-article-references__text" id="ref-CR5">Butcher EC, Picker LJ. Lymphocyte homing and homeostasis. Science. 1996;272(5258):60–7.</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1126/science.272.5258.60" data-track-item_id="10.1126/science.272.5258.60" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1126%2Fscience.272.5258.60" aria-label="Article reference 5" data-doi="10.1126/science.272.5258.60">Article</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="cas reference" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DyaK28XitVKkt70%3D" aria-label="CAS reference 5">CAS</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=8600538" aria-label="PubMed reference 5">PubMed</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 5" href="http://scholar.google.com/scholar_lookup?&amp;title=Lymphocyte%20homing%20and%20homeostasis&amp;journal=Science&amp;doi=10.1126%2Fscience.272.5258.60&amp;volume=272&amp;issue=5258&amp;pages=60-67&amp;publication_year=1996&amp;author=Butcher%2CEC&amp;author=Picker%2CLJ">
                    Google Scholar</a>&nbsp;
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="6."><p class="c-article-references__text" id="ref-CR6">Gordon S, Taylor PR. Monocyte and macrophage heterogeneity. Nat Rev Immunol. 2005;5:953–64.</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1038/nri1733" data-track-item_id="10.1038/nri1733" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1038%2Fnri1733" aria-label="Article reference 6" data-doi="10.1038/nri1733">Article</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="cas reference" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BD2MXht1Kntr3N" aria-label="CAS reference 6">CAS</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=16322748" aria-label="PubMed reference 6">PubMed</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 6" href="http://scholar.google.com/scholar_lookup?&amp;title=Monocyte%20and%20macrophage%20heterogeneity&amp;journal=Nat%20Rev%20Immunol&amp;doi=10.1038%2Fnri1733&amp;volume=5&amp;pages=953-964&amp;publication_year=2005&amp;author=Gordon%2CS&amp;author=Taylor%2CPR">
                    Google Scholar</a>&nbsp;
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="7."><p class="c-article-references__text" id="ref-CR7">WBC (White Blood Cell) Count. <a href="https://bit.ly/3cDg58c" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="https://bit.ly/3cDg58c">https://bit.ly/3cDg58c</a>. Accessed: 2020-06-09.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="8."><p class="c-article-references__text" id="ref-CR8">Statistics. <a href="https://bit.ly/30esTwt" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="https://bit.ly/30esTwt">https://bit.ly/30esTwt</a>. Accessed: 2019-06-27.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="9."><p class="c-article-references__text" id="ref-CR9">Leukemia. <a href="https://bit.ly/32WFwOn" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="https://bit.ly/32WFwOn">https://bit.ly/32WFwOn</a>. Accessed: 2019-06-29.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="10."><p class="c-article-references__text" id="ref-CR10">Shin H, et al. Deep convolutional neural networks for computer-aided detection: CNN architectures, dataset characteristics and transfer learning. IEEE Trans Med Imaging. 2016;35(5):1285–98.</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1109/TMI.2016.2528162" data-track-item_id="10.1109/TMI.2016.2528162" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1109%2FTMI.2016.2528162" aria-label="Article reference 10" data-doi="10.1109/TMI.2016.2528162">Article</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=26886976" aria-label="PubMed reference 10">PubMed</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 10" href="http://scholar.google.com/scholar_lookup?&amp;title=Deep%20convolutional%20neural%20networks%20for%20computer-aided%20detection%3A%20CNN%20architectures%2C%20dataset%20characteristics%20and%20transfer%20learning&amp;journal=IEEE%20Trans%20Med%20Imaging&amp;doi=10.1109%2FTMI.2016.2528162&amp;volume=35&amp;issue=5&amp;pages=1285-1298&amp;publication_year=2016&amp;author=Shin%2CH">
                    Google Scholar</a>&nbsp;
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="11."><p class="c-article-references__text" id="ref-CR11">Tajbakhsh N, et al. Convolutional neural networks for medical image analysis: full training or fine tuning? IEEE Trans Med Imaging. 2016;35(5):1299–312.</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1109/TMI.2016.2535302" data-track-item_id="10.1109/TMI.2016.2535302" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1109%2FTMI.2016.2535302" aria-label="Article reference 11" data-doi="10.1109/TMI.2016.2535302">Article</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=26978662" aria-label="PubMed reference 11">PubMed</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 11" href="http://scholar.google.com/scholar_lookup?&amp;title=Convolutional%20neural%20networks%20for%20medical%20image%20analysis%3A%20full%20training%20or%20fine%20tuning%3F&amp;journal=IEEE%20Trans%20Med%20Imaging&amp;doi=10.1109%2FTMI.2016.2535302&amp;volume=35&amp;issue=5&amp;pages=1299-1312&amp;publication_year=2016&amp;author=Tajbakhsh%2CN">
                    Google Scholar</a>&nbsp;
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="12."><p class="c-article-references__text" id="ref-CR12">Gupta H, Jin KH, Nguyen HQ, McCann MT, Unser M. CNN-based projected gradient descent for consistent CT image reconstruction. IEEE Trans Med Imaging. 2018;37(6):1440–53.</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1109/TMI.2018.2832656" data-track-item_id="10.1109/TMI.2018.2832656" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1109%2FTMI.2018.2832656" aria-label="Article reference 12" data-doi="10.1109/TMI.2018.2832656">Article</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=29870372" aria-label="PubMed reference 12">PubMed</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 12" href="http://scholar.google.com/scholar_lookup?&amp;title=CNN-based%20projected%20gradient%20descent%20for%20consistent%20CT%20image%20reconstruction&amp;journal=IEEE%20Trans%20Med%20Imaging&amp;doi=10.1109%2FTMI.2018.2832656&amp;volume=37&amp;issue=6&amp;pages=1440-1453&amp;publication_year=2018&amp;author=Gupta%2CH&amp;author=Jin%2CKH&amp;author=Nguyen%2CHQ&amp;author=McCann%2CMT&amp;author=Unser%2CM">
                    Google Scholar</a>&nbsp;
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="13."><p class="c-article-references__text" id="ref-CR13">Wolterink JM, Leiner T, Viergever MA, Isgum I. Generative adversarial networks for noise reduction in low-dose CT. IEEE Trans Med Imaging. 2017;36(12):2536–45.</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1109/TMI.2017.2708987" data-track-item_id="10.1109/TMI.2017.2708987" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1109%2FTMI.2017.2708987" aria-label="Article reference 13" data-doi="10.1109/TMI.2017.2708987">Article</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=28574346" aria-label="PubMed reference 13">PubMed</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 13" href="http://scholar.google.com/scholar_lookup?&amp;title=Generative%20adversarial%20networks%20for%20noise%20reduction%20in%20low-dose%20CT&amp;journal=IEEE%20Trans%20Med%20Imaging&amp;doi=10.1109%2FTMI.2017.2708987&amp;volume=36&amp;issue=12&amp;pages=2536-2545&amp;publication_year=2017&amp;author=Wolterink%2CJM&amp;author=Leiner%2CT&amp;author=Viergever%2CMA&amp;author=Isgum%2CI">
                    Google Scholar</a>&nbsp;
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="14."><p class="c-article-references__text" id="ref-CR14">Krizhevsky A, Sutskever I, Hinton GE. Imagenet classification with deep convolutional neural networks. In: Advances in NIPS; 2012.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="15."><p class="c-article-references__text" id="ref-CR15">Simonyan K, Zisserman A. Very deep convolutional networks for large-scale image recognition. arXiv preprint <a href="http://arxiv.org/abs/1409.1556" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="http://arxiv.org/abs/1409.1556">arXiv:1409.1556</a> (2014).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="16."><p class="c-article-references__text" id="ref-CR16">He K, Zhang X, Ren S, Sun J. Deep residual learning for image recognition. In: IEEE CVPR; 2016. p. 770–8.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="17."><p class="c-article-references__text" id="ref-CR17">Rezatofighi SH, Soltanian-Zadeh H. Automatic recognition of five types of white blood cells in peripheral blood. Comput Med Imaging Graph. 2011;35(4):333–43.</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1016/j.compmedimag.2011.01.003" data-track-item_id="10.1016/j.compmedimag.2011.01.003" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.compmedimag.2011.01.003" aria-label="Article reference 17" data-doi="10.1016/j.compmedimag.2011.01.003">Article</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=21300521" aria-label="PubMed reference 17">PubMed</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 17" href="http://scholar.google.com/scholar_lookup?&amp;title=Automatic%20recognition%20of%20five%20types%20of%20white%20blood%20cells%20in%20peripheral%20blood&amp;journal=Comput%20Med%20Imaging%20Graph&amp;doi=10.1016%2Fj.compmedimag.2011.01.003&amp;volume=35&amp;issue=4&amp;pages=333-343&amp;publication_year=2011&amp;author=Rezatofighi%2CSH&amp;author=Soltanian-Zadeh%2CH">
                    Google Scholar</a>&nbsp;
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="18."><p class="c-article-references__text" id="ref-CR18">Goodfellow I, et al. Generative adversarial nets. In: Advances in NIPS; 2014. p. 2672–80.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="19."><p class="c-article-references__text" id="ref-CR19">Wang Q, Chang L, Zhou M, Li Q, Liu H, Guo F. A spectral and morphologic method for white blood cell classification. Opt Laser Technol. 2016;84:144–8.</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1016/j.optlastec.2016.05.013" data-track-item_id="10.1016/j.optlastec.2016.05.013" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.optlastec.2016.05.013" aria-label="Article reference 19" data-doi="10.1016/j.optlastec.2016.05.013">Article</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="cas reference" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC28Xosl2htLY%3D" aria-label="CAS reference 19">CAS</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 19" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20spectral%20and%20morphologic%20method%20for%20white%20blood%20cell%20classification&amp;journal=Opt%20Laser%20Technol&amp;doi=10.1016%2Fj.optlastec.2016.05.013&amp;volume=84&amp;pages=144-148&amp;publication_year=2016&amp;author=Wang%2CQ&amp;author=Chang%2CL&amp;author=Zhou%2CM&amp;author=Li%2CQ&amp;author=Liu%2CH&amp;author=Guo%2CF">
                    Google Scholar</a>&nbsp;
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="20."><p class="c-article-references__text" id="ref-CR20">Dorini LB, Minetto R, Leite NJ. Semiautomatic white blood cell segmentation based on multiscale analysis. IEEE J Biomed Health Inform. 2012;17(1):250–6.</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1109/TITB.2012.2207398" data-track-item_id="10.1109/TITB.2012.2207398" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1109%2FTITB.2012.2207398" aria-label="Article reference 20" data-doi="10.1109/TITB.2012.2207398">Article</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=22855228" aria-label="PubMed reference 20">PubMed</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 20" href="http://scholar.google.com/scholar_lookup?&amp;title=Semiautomatic%20white%20blood%20cell%20segmentation%20based%20on%20multiscale%20analysis&amp;journal=IEEE%20J%20Biomed%20Health%20Inform&amp;doi=10.1109%2FTITB.2012.2207398&amp;volume=17&amp;issue=1&amp;pages=250-256&amp;publication_year=2012&amp;author=Dorini%2CLB&amp;author=Minetto%2CR&amp;author=Leite%2CNJ">
                    Google Scholar</a>&nbsp;
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="21."><p class="c-article-references__text" id="ref-CR21">Prinyakupt J, Pluempitiwiriyawej C. Segmentation of white blood cells and comparison of cell morphology by linear and naïve bayes classifiers. Biomed Eng Online. 2015;14(1):63.</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="noopener" data-track-label="10.1186/s12938-015-0037-1" data-track-item_id="10.1186/s12938-015-0037-1" data-track-value="article reference" data-track-action="article reference" href="https://link.springer.com/doi/10.1186/s12938-015-0037-1" aria-label="Article reference 21" data-doi="10.1186/s12938-015-0037-1">Article</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=26123131" aria-label="PubMed reference 21">PubMed</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed central reference" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4485641" aria-label="PubMed Central reference 21">PubMed Central</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 21" href="http://scholar.google.com/scholar_lookup?&amp;title=Segmentation%20of%20white%20blood%20cells%20and%20comparison%20of%20cell%20morphology%20by%20linear%20and%20na%C3%AFve%20bayes%20classifiers&amp;journal=Biomed%20Eng%20Online&amp;doi=10.1186%2Fs12938-015-0037-1&amp;volume=14&amp;issue=1&amp;publication_year=2015&amp;author=Prinyakupt%2CJ&amp;author=Pluempitiwiriyawej%2CC">
                    Google Scholar</a>&nbsp;
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="22."><p class="c-article-references__text" id="ref-CR22">Shitong W, Min W. A new detection algorithm (NDA) based on fuzzy cellular neural networks for white blood cell detection. IEEE Trans Inf Technol Biomed Publ Inf. 2006;10(1):5–10.</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1109/TITB.2005.855545" data-track-item_id="10.1109/TITB.2005.855545" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1109%2FTITB.2005.855545" aria-label="Article reference 22" data-doi="10.1109/TITB.2005.855545">Article</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 22" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20new%20detection%20algorithm%20%28NDA%29%20based%20on%20fuzzy%20cellular%20neural%20networks%20for%20white%20blood%20cell%20detection&amp;journal=IEEE%20Trans%20Inf%20Technol%20Biomed%20Publ%20Inf&amp;doi=10.1109%2FTITB.2005.855545&amp;volume=10&amp;issue=1&amp;pages=5-10&amp;publication_year=2006&amp;author=Shitong%2CW&amp;author=Min%2CW">
                    Google Scholar</a>&nbsp;
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="23."><p class="c-article-references__text" id="ref-CR23">Andrade AR, Vogado LH, de Veras MS, Silva RRV, Araujo FH, Medeiros FN. Recent computational methods for white blood cell nuclei segmentation: a comparative study. Comput Methods Prog Biomed. 2019;173:1–14.</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1016/j.cmpb.2019.03.001" data-track-item_id="10.1016/j.cmpb.2019.03.001" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.cmpb.2019.03.001" aria-label="Article reference 23" data-doi="10.1016/j.cmpb.2019.03.001">Article</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 23" href="http://scholar.google.com/scholar_lookup?&amp;title=Recent%20computational%20methods%20for%20white%20blood%20cell%20nuclei%20segmentation%3A%20a%20comparative%20study&amp;journal=Comput%20Methods%20Prog%20Biomed&amp;doi=10.1016%2Fj.cmpb.2019.03.001&amp;volume=173&amp;pages=1-14&amp;publication_year=2019&amp;author=Andrade%2CAR&amp;author=Vogado%2CLH&amp;author=Veras%2CMS&amp;author=Silva%2CRRV&amp;author=Araujo%2CFH&amp;author=Medeiros%2CFN">
                    Google Scholar</a>&nbsp;
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="24."><p class="c-article-references__text" id="ref-CR24">Viswanathan P. Fuzzy c means detection of leukemia based on morphological contour segmentation. Procedia Comput Sci. 2015;58:84–90.</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1016/j.procs.2015.08.017" data-track-item_id="10.1016/j.procs.2015.08.017" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.procs.2015.08.017" aria-label="Article reference 24" data-doi="10.1016/j.procs.2015.08.017">Article</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 24" href="http://scholar.google.com/scholar_lookup?&amp;title=Fuzzy%20c%20means%20detection%20of%20leukemia%20based%20on%20morphological%20contour%20segmentation&amp;journal=Procedia%20Comput%20Sci&amp;doi=10.1016%2Fj.procs.2015.08.017&amp;volume=58&amp;pages=84-90&amp;publication_year=2015&amp;author=Viswanathan%2CP">
                    Google Scholar</a>&nbsp;
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="25."><p class="c-article-references__text" id="ref-CR25">Gautam A, Bhadauria H. White blood nucleus extraction using k-mean clustering and mathematical morphing. In: 2014 5th International conference-confluence the next generation information technology summit (confluence). IEEE; 2014. p. 549–554.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="26."><p class="c-article-references__text" id="ref-CR26">Mohapatra S, Samanta SS, Patra D, Satpathi S. Fuzzy based blood image segmentation for automated leukemia detection. In: 2011 ICDeCom. IEEE; 2011. p. 1–5.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="27."><p class="c-article-references__text" id="ref-CR27">Nazlibilek S, Karacor D, Ercan T, Sazli MH, Kalender O, Ege Y. Automatic segmentation, counting, size determination and classification of white blood cells. Measurement. 2014;55:58–65.</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1016/j.measurement.2014.04.008" data-track-item_id="10.1016/j.measurement.2014.04.008" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.measurement.2014.04.008" aria-label="Article reference 27" data-doi="10.1016/j.measurement.2014.04.008">Article</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 27" href="http://scholar.google.com/scholar_lookup?&amp;title=Automatic%20segmentation%2C%20counting%2C%20size%20determination%20and%20classification%20of%20white%20blood%20cells&amp;journal=Measurement&amp;doi=10.1016%2Fj.measurement.2014.04.008&amp;volume=55&amp;pages=58-65&amp;publication_year=2014&amp;author=Nazlibilek%2CS&amp;author=Karacor%2CD&amp;author=Ercan%2CT&amp;author=Sazli%2CMH&amp;author=Kalender%2CO&amp;author=Ege%2CY">
                    Google Scholar</a>&nbsp;
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="28."><p class="c-article-references__text" id="ref-CR28">Abdeldaim AM, Sahlol AT, Elhoseny M, Hassanien AE. Computer-aided acute lymphoblastic leukemia diagnosis system based on image analysis. Berlin: Springer; 2018. p. 131–47.</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 28" href="http://scholar.google.com/scholar_lookup?&amp;title=Computer-aided%20acute%20lymphoblastic%20leukemia%20diagnosis%20system%20based%20on%20image%20analysis&amp;pages=131-147&amp;publication_year=2018&amp;author=Abdeldaim%2CAM&amp;author=Sahlol%2CAT&amp;author=Elhoseny%2CM&amp;author=Hassanien%2CAE">
                    Google Scholar</a>&nbsp;
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="29."><p class="c-article-references__text" id="ref-CR29">Tosta TAA, De Abreu AF, Travençolo BAN, do Nascimento MZ, Neves LA. Unsupervised segmentation of leukocytes images using thresholding neighborhood valley-emphasis. In: 2015 IEEE CBMS. IEEE; 2015. p. 93–94.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="30."><p class="c-article-references__text" id="ref-CR30">Cao H, Liu H, Song E. A novel algorithm for segmentation of leukocytes in peripheral blood. Biomed Signal Process Control. 2018;45:10–21.</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1016/j.bspc.2018.05.010" data-track-item_id="10.1016/j.bspc.2018.05.010" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.bspc.2018.05.010" aria-label="Article reference 30" data-doi="10.1016/j.bspc.2018.05.010">Article</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 30" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20novel%20algorithm%20for%20segmentation%20of%20leukocytes%20in%20peripheral%20blood&amp;journal=Biomed%20Signal%20Process%20Control&amp;doi=10.1016%2Fj.bspc.2018.05.010&amp;volume=45&amp;pages=10-21&amp;publication_year=2018&amp;author=Cao%2CH&amp;author=Liu%2CH&amp;author=Song%2CE">
                    Google Scholar</a>&nbsp;
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="31."><p class="c-article-references__text" id="ref-CR31">Mohammed EA, Mohamed MM, Naugler C, Far BH. Chronic lymphocytic leukemia cell segmentation from microscopic blood images using watershed algorithm and optimal thresholding. In: 2013 26th IEEE CCECE. IEEE; 2013. p. 1–5.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="32."><p class="c-article-references__text" id="ref-CR32">Hegde RB, Prasad K, Hebbar H, Singh BMK. Development of a robust algorithm for detection of nuclei and classification of white blood cells in peripheral blood smear images. J Med Syst. 2018;42(6):110.</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="noopener" data-track-label="10.1007/s10916-018-0962-1" data-track-item_id="10.1007/s10916-018-0962-1" data-track-value="article reference" data-track-action="article reference" href="https://link.springer.com/doi/10.1007/s10916-018-0962-1" aria-label="Article reference 32" data-doi="10.1007/s10916-018-0962-1">Article</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=29721616" aria-label="PubMed reference 32">PubMed</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 32" href="http://scholar.google.com/scholar_lookup?&amp;title=Development%20of%20a%20robust%20algorithm%20for%20detection%20of%20nuclei%20and%20classification%20of%20white%20blood%20cells%20in%20peripheral%20blood%20smear%20images&amp;journal=J%20Med%20Syst&amp;doi=10.1007%2Fs10916-018-0962-1&amp;volume=42&amp;issue=6&amp;publication_year=2018&amp;author=Hegde%2CRB&amp;author=Prasad%2CK&amp;author=Hebbar%2CH&amp;author=Singh%2CBMK">
                    Google Scholar</a>&nbsp;
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="33."><p class="c-article-references__text" id="ref-CR33">Chan Y-K, Tsai M-H, Huang D-C, Zheng Z-H, Hung K-D. Leukocyte nucleus segmentation and nucleus lobe counting. BMC Bioinform. 2010;11(1):558.</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="noopener" data-track-label="10.1186/1471-2105-11-558" data-track-item_id="10.1186/1471-2105-11-558" data-track-value="article reference" data-track-action="article reference" href="https://link.springer.com/doi/10.1186/1471-2105-11-558" aria-label="Article reference 33" data-doi="10.1186/1471-2105-11-558">Article</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 33" href="http://scholar.google.com/scholar_lookup?&amp;title=Leukocyte%20nucleus%20segmentation%20and%20nucleus%20lobe%20counting&amp;journal=BMC%20Bioinform&amp;doi=10.1186%2F1471-2105-11-558&amp;volume=11&amp;issue=1&amp;publication_year=2010&amp;author=Chan%2CY-K&amp;author=Tsai%2CM-H&amp;author=Huang%2CD-C&amp;author=Zheng%2CZ-H&amp;author=Hung%2CK-D">
                    Google Scholar</a>&nbsp;
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="34."><p class="c-article-references__text" id="ref-CR34">Mathur A, Tripathi AS, Kuse M. Scalable system for classification of white blood cells from Leishman stained blood stain images. J Pathol Inform. 2013;4(Suppl):15.</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.4103/2153-3539.109883" data-track-item_id="10.4103/2153-3539.109883" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.4103%2F2153-3539.109883" aria-label="Article reference 34" data-doi="10.4103/2153-3539.109883">Article</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 34" href="http://scholar.google.com/scholar_lookup?&amp;title=Scalable%20system%20for%20classification%20of%20white%20blood%20cells%20from%20Leishman%20stained%20blood%20stain%20images&amp;journal=J%20Pathol%20Inform&amp;doi=10.4103%2F2153-3539.109883&amp;volume=4&amp;issue=Suppl&amp;publication_year=2013&amp;author=Mathur%2CA&amp;author=Tripathi%2CAS&amp;author=Kuse%2CM">
                    Google Scholar</a>&nbsp;
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="35."><p class="c-article-references__text" id="ref-CR35">Su M-C, Cheng C-Y, Wang P-C. A neural-network-based approach to WBC classification. Sci World J. 2014;2014:1–9.</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 35" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20neural-network-based%20approach%20to%20WBC%20classification&amp;journal=Sci%20World%20J&amp;volume=2014&amp;pages=1-9&amp;publication_year=2014&amp;author=Su%2CM-C&amp;author=Cheng%2CC-Y&amp;author=Wang%2CP-C">
                    Google Scholar</a>&nbsp;
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="36."><p class="c-article-references__text" id="ref-CR36">Ramesh N, Dangott B, Salama ME, Tasdizen T. Isolation and two-step classification of normal white blood cells in peripheral blood smears. J Pathol Inform. 2012;3:179–91.</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.4103/2153-3539.93895" data-track-item_id="10.4103/2153-3539.93895" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.4103%2F2153-3539.93895" aria-label="Article reference 36" data-doi="10.4103/2153-3539.93895">Article</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 36" href="http://scholar.google.com/scholar_lookup?&amp;title=Isolation%20and%20two-step%20classification%20of%20normal%20white%20blood%20cells%20in%20peripheral%20blood%20smears&amp;journal=J%20Pathol%20Inform&amp;doi=10.4103%2F2153-3539.93895&amp;volume=3&amp;pages=179-191&amp;publication_year=2012&amp;author=Ramesh%2CN&amp;author=Dangott%2CB&amp;author=Salama%2CME&amp;author=Tasdizen%2CT">
                    Google Scholar</a>&nbsp;
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="37."><p class="c-article-references__text" id="ref-CR37">Ghosh P, Bhattacharjee D, Nasipuri M. Blood smear analyzer for white blood cell counting: a hybrid microscopic image analyzing technique. Appl Soft Comput. 2016;46:629–38.</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1016/j.asoc.2015.12.038" data-track-item_id="10.1016/j.asoc.2015.12.038" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.asoc.2015.12.038" aria-label="Article reference 37" data-doi="10.1016/j.asoc.2015.12.038">Article</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 37" href="http://scholar.google.com/scholar_lookup?&amp;title=Blood%20smear%20analyzer%20for%20white%20blood%20cell%20counting%3A%20a%20hybrid%20microscopic%20image%20analyzing%20technique&amp;journal=Appl%20Soft%20Comput&amp;doi=10.1016%2Fj.asoc.2015.12.038&amp;volume=46&amp;pages=629-638&amp;publication_year=2016&amp;author=Ghosh%2CP&amp;author=Bhattacharjee%2CD&amp;author=Nasipuri%2CM">
                    Google Scholar</a>&nbsp;
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="38."><p class="c-article-references__text" id="ref-CR38">Habibzadeh M, Jannesari M, Rezaei Z, Baharvand H, Totonchi M. Automatic white blood cell classification using pre-trained deep learning models: Resnet and inception. In: Tenth ICMV 2017, vol 10696. International Society for Optics and Photonics; 2018. p. 1069612.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="39."><p class="c-article-references__text" id="ref-CR39">Rawat J, Singh A, Bhadauria H, Virmani J, Devgun JS. Application of ensemble artificial neural network for the classification of white blood cells using microscopic blood images. Int J Comput Syst Eng. 2018;4(2–3):202–16.</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1504/IJCSYSE.2018.091407" data-track-item_id="10.1504/IJCSYSE.2018.091407" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1504%2FIJCSYSE.2018.091407" aria-label="Article reference 39" data-doi="10.1504/IJCSYSE.2018.091407">Article</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 39" href="http://scholar.google.com/scholar_lookup?&amp;title=Application%20of%20ensemble%20artificial%20neural%20network%20for%20the%20classification%20of%20white%20blood%20cells%20using%20microscopic%20blood%20images&amp;journal=Int%20J%20Comput%20Syst%20Eng&amp;doi=10.1504%2FIJCSYSE.2018.091407&amp;volume=4&amp;issue=2%E2%80%933&amp;pages=202-216&amp;publication_year=2018&amp;author=Rawat%2CJ&amp;author=Singh%2CA&amp;author=Bhadauria%2CH&amp;author=Virmani%2CJ&amp;author=Devgun%2CJS">
                    Google Scholar</a>&nbsp;
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="40."><p class="c-article-references__text" id="ref-CR40">Patil AM, Patil MD, Birajdar GK. White blood cells image classification using deep learning with canonical correlation analysis. IRBM. 2021;42(5):378–89.</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1016/j.irbm.2020.08.005" data-track-item_id="10.1016/j.irbm.2020.08.005" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.irbm.2020.08.005" aria-label="Article reference 40" data-doi="10.1016/j.irbm.2020.08.005">Article</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 40" href="http://scholar.google.com/scholar_lookup?&amp;title=White%20blood%20cells%20image%20classification%20using%20deep%20learning%20with%20canonical%20correlation%20analysis&amp;journal=IRBM&amp;doi=10.1016%2Fj.irbm.2020.08.005&amp;volume=42&amp;issue=5&amp;pages=378-389&amp;publication_year=2021&amp;author=Patil%2CAM&amp;author=Patil%2CMD&amp;author=Birajdar%2CGK">
                    Google Scholar</a>&nbsp;
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="41."><p class="c-article-references__text" id="ref-CR41">Toğaçar M, Ergen B, Cömert Z. Classification of white blood cells using deep features obtained from convolutional neural network models based on the combination of feature selection methods. Appl Soft Comput. 2020;97:106810.</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1016/j.asoc.2020.106810" data-track-item_id="10.1016/j.asoc.2020.106810" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.asoc.2020.106810" aria-label="Article reference 41" data-doi="10.1016/j.asoc.2020.106810">Article</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 41" href="http://scholar.google.com/scholar_lookup?&amp;title=Classification%20of%20white%20blood%20cells%20using%20deep%20features%20obtained%20from%20convolutional%20neural%20network%20models%20based%20on%20the%20combination%20of%20feature%20selection%20methods&amp;journal=Appl%20Soft%20Comput&amp;doi=10.1016%2Fj.asoc.2020.106810&amp;volume=97&amp;publication_year=2020&amp;author=To%C4%9Fa%C3%A7ar%2CM&amp;author=Ergen%2CB&amp;author=C%C3%B6mert%2CZ">
                    Google Scholar</a>&nbsp;
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="42."><p class="c-article-references__text" id="ref-CR42">Khan S, Sajjad M, Hussain T, Ullah A, Imran AS. A review on traditional machine learning and deep learning models for WBCs classification in blood smear images. IEEE Access. 2020;9:10657–73.</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1109/ACCESS.2020.3048172" data-track-item_id="10.1109/ACCESS.2020.3048172" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1109%2FACCESS.2020.3048172" aria-label="Article reference 42" data-doi="10.1109/ACCESS.2020.3048172">Article</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 42" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20review%20on%20traditional%20machine%20learning%20and%20deep%20learning%20models%20for%20WBCs%20classification%20in%20blood%20smear%20images&amp;journal=IEEE%20Access&amp;doi=10.1109%2FACCESS.2020.3048172&amp;volume=9&amp;pages=10657-10673&amp;publication_year=2020&amp;author=Khan%2CS&amp;author=Sajjad%2CM&amp;author=Hussain%2CT&amp;author=Ullah%2CA&amp;author=Imran%2CAS">
                    Google Scholar</a>&nbsp;
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="43."><p class="c-article-references__text" id="ref-CR43">Mohamed EH, El-Behaidy WH, Khoriba G, Li J. Improved white blood cells classification based on pre-trained deep learning models. J Commun Softw Syst. 2020;16(1):37–45.</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.24138/jcomss.v16i1.818" data-track-item_id="10.24138/jcomss.v16i1.818" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.24138%2Fjcomss.v16i1.818" aria-label="Article reference 43" data-doi="10.24138/jcomss.v16i1.818">Article</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 43" href="http://scholar.google.com/scholar_lookup?&amp;title=Improved%20white%20blood%20cells%20classification%20based%20on%20pre-trained%20deep%20learning%20models&amp;journal=J%20Commun%20Softw%20Syst&amp;doi=10.24138%2Fjcomss.v16i1.818&amp;volume=16&amp;issue=1&amp;pages=37-45&amp;publication_year=2020&amp;author=Mohamed%2CEH&amp;author=El-Behaidy%2CWH&amp;author=Khoriba%2CG&amp;author=Li%2CJ">
                    Google Scholar</a>&nbsp;
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="44."><p class="c-article-references__text" id="ref-CR44">Banik PP, Saha R, Kim KD. An automatic nucleus segmentation and CNN model based classification method of white blood cell. Expert Syst Appl. 2020;149:113211.</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1016/j.eswa.2020.113211" data-track-item_id="10.1016/j.eswa.2020.113211" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.eswa.2020.113211" aria-label="Article reference 44" data-doi="10.1016/j.eswa.2020.113211">Article</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 44" href="http://scholar.google.com/scholar_lookup?&amp;title=An%20automatic%20nucleus%20segmentation%20and%20CNN%20model%20based%20classification%20method%20of%20white%20blood%20cell&amp;journal=Expert%20Syst%20Appl&amp;doi=10.1016%2Fj.eswa.2020.113211&amp;volume=149&amp;publication_year=2020&amp;author=Banik%2CPP&amp;author=Saha%2CR&amp;author=Kim%2CKD">
                    Google Scholar</a>&nbsp;
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="45."><p class="c-article-references__text" id="ref-CR45">Karthikeyan MP, Venkatesan R. Interpolative Leishman-stained transformation invariant deep pattern classification for white blood cells. Soft Comput. 2020;24(16):12215–25.</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="noopener" data-track-label="10.1007/s00500-019-04662-4" data-track-item_id="10.1007/s00500-019-04662-4" data-track-value="article reference" data-track-action="article reference" href="https://link.springer.com/doi/10.1007/s00500-019-04662-4" aria-label="Article reference 45" data-doi="10.1007/s00500-019-04662-4">Article</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 45" href="http://scholar.google.com/scholar_lookup?&amp;title=Interpolative%20Leishman-stained%20transformation%20invariant%20deep%20pattern%20classification%20for%20white%20blood%20cells&amp;journal=Soft%20Comput&amp;doi=10.1007%2Fs00500-019-04662-4&amp;volume=24&amp;issue=16&amp;pages=12215-12225&amp;publication_year=2020&amp;author=Karthikeyan%2CMP&amp;author=Venkatesan%2CR">
                    Google Scholar</a>&nbsp;
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="46."><p class="c-article-references__text" id="ref-CR46">Kutlu H, Avci E, Özyurt F. White blood cells detection and classification based on regional convolutional neural networks. Med Hypotheses. 2020;135:109472.</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1016/j.mehy.2019.109472" data-track-item_id="10.1016/j.mehy.2019.109472" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.mehy.2019.109472" aria-label="Article reference 46" data-doi="10.1016/j.mehy.2019.109472">Article</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="cas reference" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC1MXit1Smu7zE" aria-label="CAS reference 46">CAS</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=31760248" aria-label="PubMed reference 46">PubMed</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 46" href="http://scholar.google.com/scholar_lookup?&amp;title=White%20blood%20cells%20detection%20and%20classification%20based%20on%20regional%20convolutional%20neural%20networks&amp;journal=Med%20Hypotheses&amp;doi=10.1016%2Fj.mehy.2019.109472&amp;volume=135&amp;publication_year=2020&amp;author=Kutlu%2CH&amp;author=Avci%2CE&amp;author=%C3%96zyurt%2CF">
                    Google Scholar</a>&nbsp;
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="47."><p class="c-article-references__text" id="ref-CR47">Rakhlin A, Shvets A, Iglovikov V, Kalinin AA. Deep convolutional neural networks for breast cancer histology image analysis. In: ICIAR. Springer; 2018. p. 737–44.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="48."><p class="c-article-references__text" id="ref-CR48">Kermany DS, et al. Identifying medical diagnoses and treatable diseases by image-based deep learning. Cell. 2018;172(5):1122–31.</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1016/j.cell.2018.02.010" data-track-item_id="10.1016/j.cell.2018.02.010" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.cell.2018.02.010" aria-label="Article reference 48" data-doi="10.1016/j.cell.2018.02.010">Article</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="cas reference" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BC1cXjt12ltr0%3D" aria-label="CAS reference 48">CAS</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=29474911" aria-label="PubMed reference 48">PubMed</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 48" href="http://scholar.google.com/scholar_lookup?&amp;title=Identifying%20medical%20diagnoses%20and%20treatable%20diseases%20by%20image-based%20deep%20learning&amp;journal=Cell&amp;doi=10.1016%2Fj.cell.2018.02.010&amp;volume=172&amp;issue=5&amp;pages=1122-1131&amp;publication_year=2018&amp;author=Kermany%2CDS">
                    Google Scholar</a>&nbsp;
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="49."><p class="c-article-references__text" id="ref-CR49">Acharya UR, et al. Automated identification of shockable and non-shockable life-threatening ventricular arrhythmias using convolutional neural network. Futur Gener Comput Syst. 2018;79:952–9.</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1016/j.future.2017.08.039" data-track-item_id="10.1016/j.future.2017.08.039" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.future.2017.08.039" aria-label="Article reference 49" data-doi="10.1016/j.future.2017.08.039">Article</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 49" href="http://scholar.google.com/scholar_lookup?&amp;title=Automated%20identification%20of%20shockable%20and%20non-shockable%20life-threatening%20ventricular%20arrhythmias%20using%20convolutional%20neural%20network&amp;journal=Futur%20Gener%20Comput%20Syst&amp;doi=10.1016%2Fj.future.2017.08.039&amp;volume=79&amp;pages=952-959&amp;publication_year=2018&amp;author=Acharya%2CUR">
                    Google Scholar</a>&nbsp;
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="50."><p class="c-article-references__text" id="ref-CR50">Moran MB, et al. Identification of thyroid nodules in infrared images by convolutional neural networks. In: 2018 IJCNN. IEEE; 2018. p. 1–7.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="51."><p class="c-article-references__text" id="ref-CR51">Sohail A, Khan A, Nisar H, Tabassum S, Zameer A. Mitotic nuclei analysis in breast cancer histopathology images using deep ensemble classifier. Med Image Anal. 2021;72:102121.</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1016/j.media.2021.102121" data-track-item_id="10.1016/j.media.2021.102121" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.media.2021.102121" aria-label="Article reference 51" data-doi="10.1016/j.media.2021.102121">Article</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=34139665" aria-label="PubMed reference 51">PubMed</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 51" href="http://scholar.google.com/scholar_lookup?&amp;title=Mitotic%20nuclei%20analysis%20in%20breast%20cancer%20histopathology%20images%20using%20deep%20ensemble%20classifier&amp;journal=Med%20Image%20Anal&amp;doi=10.1016%2Fj.media.2021.102121&amp;volume=72&amp;publication_year=2021&amp;author=Sohail%2CA&amp;author=Khan%2CA&amp;author=Nisar%2CH&amp;author=Tabassum%2CS&amp;author=Zameer%2CA">
                    Google Scholar</a>&nbsp;
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="52."><p class="c-article-references__text" id="ref-CR52">Wachinger C, Reuter M, Klein T. DeepNAT: deep convolutional neural network for segmenting neuroanatomy. Neuroimage. 2018;170:434–45.</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1016/j.neuroimage.2017.02.035" data-track-item_id="10.1016/j.neuroimage.2017.02.035" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.neuroimage.2017.02.035" aria-label="Article reference 52" data-doi="10.1016/j.neuroimage.2017.02.035">Article</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=28223187" aria-label="PubMed reference 52">PubMed</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 52" href="http://scholar.google.com/scholar_lookup?&amp;title=DeepNAT%3A%20deep%20convolutional%20neural%20network%20for%20segmenting%20neuroanatomy&amp;journal=Neuroimage&amp;doi=10.1016%2Fj.neuroimage.2017.02.035&amp;volume=170&amp;pages=434-445&amp;publication_year=2018&amp;author=Wachinger%2CC&amp;author=Reuter%2CM&amp;author=Klein%2CT">
                    Google Scholar</a>&nbsp;
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="53."><p class="c-article-references__text" id="ref-CR53">Zhang B, Zhou J. Multi-feature representation for burn depth classification via burn images. Artif Intell Med. 2021;118:102128.</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1016/j.artmed.2021.102128" data-track-item_id="10.1016/j.artmed.2021.102128" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.artmed.2021.102128" aria-label="Article reference 53" data-doi="10.1016/j.artmed.2021.102128">Article</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=34412845" aria-label="PubMed reference 53">PubMed</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 53" href="http://scholar.google.com/scholar_lookup?&amp;title=Multi-feature%20representation%20for%20burn%20depth%20classification%20via%20burn%20images&amp;journal=Artif%20Intell%20Med&amp;doi=10.1016%2Fj.artmed.2021.102128&amp;volume=118&amp;publication_year=2021&amp;author=Zhang%2CB&amp;author=Zhou%2CJ">
                    Google Scholar</a>&nbsp;
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="54."><p class="c-article-references__text" id="ref-CR54">Karimi D, Warfield SK, Gholipour A. Transfer learning in medical image segmentation: new insights from analysis of the dynamics of model parameters and learned representations. Artif Intell Med. 2021;116:102078.</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1016/j.artmed.2021.102078" data-track-item_id="10.1016/j.artmed.2021.102078" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.artmed.2021.102078" aria-label="Article reference 54" data-doi="10.1016/j.artmed.2021.102078">Article</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=34020754" aria-label="PubMed reference 54">PubMed</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 54" href="http://scholar.google.com/scholar_lookup?&amp;title=Transfer%20learning%20in%20medical%20image%20segmentation%3A%20new%20insights%20from%20analysis%20of%20the%20dynamics%20of%20model%20parameters%20and%20learned%20representations&amp;journal=Artif%20Intell%20Med&amp;doi=10.1016%2Fj.artmed.2021.102078&amp;volume=116&amp;publication_year=2021&amp;author=Karimi%2CD&amp;author=Warfield%2CSK&amp;author=Gholipour%2CA">
                    Google Scholar</a>&nbsp;
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="55."><p class="c-article-references__text" id="ref-CR55">Nateghi R, Danyali H, Helfroush MS. A deep learning approach for mitosis detection: application in tumor proliferation prediction from whole slide images. Artif Intell Med. 2021;114:102048.</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1016/j.artmed.2021.102048" data-track-item_id="10.1016/j.artmed.2021.102048" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.artmed.2021.102048" aria-label="Article reference 55" data-doi="10.1016/j.artmed.2021.102048">Article</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=33875159" aria-label="PubMed reference 55">PubMed</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 55" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20deep%20learning%20approach%20for%20mitosis%20detection%3A%20application%20in%20tumor%20proliferation%20prediction%20from%20whole%20slide%20images&amp;journal=Artif%20Intell%20Med&amp;doi=10.1016%2Fj.artmed.2021.102048&amp;volume=114&amp;publication_year=2021&amp;author=Nateghi%2CR&amp;author=Danyali%2CH&amp;author=Helfroush%2CMS">
                    Google Scholar</a>&nbsp;
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="56."><p class="c-article-references__text" id="ref-CR56">Yoon H, Kim J, Lim HJ, Lee M-J. Image quality assessment of pediatric chest and abdomen ct by deep learning reconstruction. BMC Med Imaging. 2021;21:1–11.</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="noopener" data-track-label="10.1186/s12880-021-00677-2" data-track-item_id="10.1186/s12880-021-00677-2" data-track-value="article reference" data-track-action="article reference" href="https://link.springer.com/doi/10.1186/s12880-021-00677-2" aria-label="Article reference 56" data-doi="10.1186/s12880-021-00677-2">Article</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="cas reference" data-track-action="cas reference" href="/articles/cas-redirect/1:CAS:528:DC%2BB3MXosVemuw%3D%3D" aria-label="CAS reference 56">CAS</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 56" href="http://scholar.google.com/scholar_lookup?&amp;title=Image%20quality%20assessment%20of%20pediatric%20chest%20and%20abdomen%20ct%20by%20deep%20learning%20reconstruction&amp;journal=BMC%20Med%20Imaging&amp;doi=10.1186%2Fs12880-021-00677-2&amp;volume=21&amp;pages=1-11&amp;publication_year=2021&amp;author=Yoon%2CH&amp;author=Kim%2CJ&amp;author=Lim%2CHJ&amp;author=Lee%2CM-J">
                    Google Scholar</a>&nbsp;
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="57."><p class="c-article-references__text" id="ref-CR57">Wang T, Song N, Liu L, Zhu Z, Chen B, Yang W, Chen Z. Efficiency of a deep learning-based artificial intelligence diagnostic system in spontaneous intracerebral hemorrhage volume measurement. BMC Med Imaging. 2021;21(1):1–9.</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="noopener" data-track-label="10.1186/s12880-021-00657-6" data-track-item_id="10.1186/s12880-021-00657-6" data-track-value="article reference" data-track-action="article reference" href="https://link.springer.com/doi/10.1186/s12880-021-00657-6" aria-label="Article reference 57" data-doi="10.1186/s12880-021-00657-6">Article</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 57" href="http://scholar.google.com/scholar_lookup?&amp;title=Efficiency%20of%20a%20deep%20learning-based%20artificial%20intelligence%20diagnostic%20system%20in%20spontaneous%20intracerebral%20hemorrhage%20volume%20measurement&amp;journal=BMC%20Med%20Imaging&amp;doi=10.1186%2Fs12880-021-00657-6&amp;volume=21&amp;issue=1&amp;pages=1-9&amp;publication_year=2021&amp;author=Wang%2CT&amp;author=Song%2CN&amp;author=Liu%2CL&amp;author=Zhu%2CZ&amp;author=Chen%2CB&amp;author=Yang%2CW&amp;author=Chen%2CZ">
                    Google Scholar</a>&nbsp;
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="58."><p class="c-article-references__text" id="ref-CR58">Guo K, Li X, Hu X, Liu J, Fan T. Hahn-PCNN-CNN: an end-to-end multi-modal brain medical image fusion framework useful for clinical diagnosis. BMC Med Imaging. 2021;21(1):1–22.</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="noopener" data-track-label="10.1186/s12880-021-00642-z" data-track-item_id="10.1186/s12880-021-00642-z" data-track-value="article reference" data-track-action="article reference" href="https://link.springer.com/doi/10.1186/s12880-021-00642-z" aria-label="Article reference 58" data-doi="10.1186/s12880-021-00642-z">Article</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 58" href="http://scholar.google.com/scholar_lookup?&amp;title=Hahn-PCNN-CNN%3A%20an%20end-to-end%20multi-modal%20brain%20medical%20image%20fusion%20framework%20useful%20for%20clinical%20diagnosis&amp;journal=BMC%20Med%20Imaging&amp;doi=10.1186%2Fs12880-021-00642-z&amp;volume=21&amp;issue=1&amp;pages=1-22&amp;publication_year=2021&amp;author=Guo%2CK&amp;author=Li%2CX&amp;author=Hu%2CX&amp;author=Liu%2CJ&amp;author=Fan%2CT">
                    Google Scholar</a>&nbsp;
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="59."><p class="c-article-references__text" id="ref-CR59">Sun J, Li H, Wang B, Li J, Li M, Zhou Z, Peng Y. Application of a deep learning image reconstruction (DLIR) algorithm in head CT imaging for children to improve image quality and lesion detection. BMC Med Imaging. 2021;21(1):1–9.</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="noopener" data-track-label="10.1186/s12880-021-00637-w" data-track-item_id="10.1186/s12880-021-00637-w" data-track-value="article reference" data-track-action="article reference" href="https://link.springer.com/doi/10.1186/s12880-021-00637-w" aria-label="Article reference 59" data-doi="10.1186/s12880-021-00637-w">Article</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 59" href="http://scholar.google.com/scholar_lookup?&amp;title=Application%20of%20a%20deep%20learning%20image%20reconstruction%20%28DLIR%29%20algorithm%20in%20head%20CT%20imaging%20for%20children%20to%20improve%20image%20quality%20and%20lesion%20detection&amp;journal=BMC%20Med%20Imaging&amp;doi=10.1186%2Fs12880-021-00637-w&amp;volume=21&amp;issue=1&amp;pages=1-9&amp;publication_year=2021&amp;author=Sun%2CJ&amp;author=Li%2CH&amp;author=Wang%2CB&amp;author=Li%2CJ&amp;author=Li%2CM&amp;author=Zhou%2CZ&amp;author=Peng%2CY">
                    Google Scholar</a>&nbsp;
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="60."><p class="c-article-references__text" id="ref-CR60">The Catholic University of Korea Institutional Review Board. <a href="https://bit.ly/2YrlQPl" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="https://bit.ly/2YrlQPl">https://bit.ly/2YrlQPl</a>. Accessed: 2019-07-17.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="61."><p class="c-article-references__text" id="ref-CR61">Sysmex DI-60. <a href="https://bit.ly/313v6L3" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="https://bit.ly/313v6L3">https://bit.ly/313v6L3</a>. Accessed: 2019-07-17.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="62."><p class="c-article-references__text" id="ref-CR62">Srivastava N, Hinton G, Krizhevsky A, Sutskever I, Salakhutdinov R. Dropout: a simple way to prevent neural networks from overfitting. J Mach Learn Res. 2014;15(1):1929–58.</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 62" href="http://scholar.google.com/scholar_lookup?&amp;title=Dropout%3A%20a%20simple%20way%20to%20prevent%20neural%20networks%20from%20overfitting&amp;journal=J%20Mach%20Learn%20Res&amp;volume=15&amp;issue=1&amp;pages=1929-1958&amp;publication_year=2014&amp;author=Srivastava%2CN&amp;author=Hinton%2CG&amp;author=Krizhevsky%2CA&amp;author=Sutskever%2CI&amp;author=Salakhutdinov%2CR">
                    Google Scholar</a>&nbsp;
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="63."><p class="c-article-references__text" id="ref-CR63">Kingma DP, Ba J. Adam: a method for stochastic optimization. arXiv preprint <a href="http://arxiv.org/abs/1412.6980" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="http://arxiv.org/abs/1412.6980">arXiv:1412.6980</a> (2014).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="64."><p class="c-article-references__text" id="ref-CR64">Arlot S, Celisse A, et al. A survey of cross-validation procedures for model selection. Stat Surv. 2010;4:40–79.</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1214/09-SS054" data-track-item_id="10.1214/09-SS054" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1214%2F09-SS054" aria-label="Article reference 64" data-doi="10.1214/09-SS054">Article</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 64" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20survey%20of%20cross-validation%20procedures%20for%20model%20selection&amp;journal=Stat%20Surv&amp;doi=10.1214%2F09-SS054&amp;volume=4&amp;pages=40-79&amp;publication_year=2010&amp;author=Arlot%2CS&amp;author=Celisse%2CA">
                    Google Scholar</a>&nbsp;
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="65."><p class="c-article-references__text" id="ref-CR65">Rosasco L, Vito ED, Caponnetto A, Piana M, Verri A. Are loss functions all the same? Neural Comput. 2004;16(5):1063–76.</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1162/089976604773135104" data-track-item_id="10.1162/089976604773135104" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1162%2F089976604773135104" aria-label="Article reference 65" data-doi="10.1162/089976604773135104">Article</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=15070510" aria-label="PubMed reference 65">PubMed</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 65" href="http://scholar.google.com/scholar_lookup?&amp;title=Are%20loss%20functions%20all%20the%20same%3F&amp;journal=Neural%20Comput&amp;doi=10.1162%2F089976604773135104&amp;volume=16&amp;issue=5&amp;pages=1063-1076&amp;publication_year=2004&amp;author=Rosasco%2CL&amp;author=Vito%2CED&amp;author=Caponnetto%2CA&amp;author=Piana%2CM&amp;author=Verri%2CA">
                    Google Scholar</a>&nbsp;
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="66."><p class="c-article-references__text" id="ref-CR66">Sutskever I, Martens J, Dahl G, Hinton G. On the importance of initialization and momentum in deep learning. In: International conference on ML; 2013. p. 1139–47.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="67."><p class="c-article-references__text" id="ref-CR67">Jung, C. W-Net model and generated WBC images. 2022. <a href="https://bit.ly/2KAffwM" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="https://bit.ly/2KAffwM">https://bit.ly/2KAffwM</a>. Accessed: 2022-3-24.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="68."><p class="c-article-references__text" id="ref-CR68">Tiny ImageNet. <a href="https://bit.ly/36Qxvfp" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="https://bit.ly/36Qxvfp">https://bit.ly/36Qxvfp</a>. Accessed: 2020-01-13.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="69."><p class="c-article-references__text" id="ref-CR69">Gregor K, Danihelka I, Graves A, Rezende D, Wierstra D. Draw: a recurrent neural network for image generation. In: International conference on machine learning. PMLR (2015, June). p. 1462–71.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="70."><p class="c-article-references__text" id="ref-CR70">Radford A, Metz L, Chintala S. Unsupervised representation learning with deep convolutional generative adversarial networks. arXiv preprint <a href="http://arxiv.org/abs/1511.06434" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="http://arxiv.org/abs/1511.06434">arXiv:1511.06434</a> (2015).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="71."><p class="c-article-references__text" id="ref-CR71">Maas AL, Hannun AY, Ng AY. Rectifier nonlinearities improve neural network acoustic models. In: Proceedings of ICML, vol 30; 2013. p. 3.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="72."><p class="c-article-references__text" id="ref-CR72">Ioffe S, Szegedy C. Batch normalization: accelerating deep network training by reducing internal covariate shift. arXiv preprint <a href="http://arxiv.org/abs/1502.03167" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="http://arxiv.org/abs/1502.03167">arXiv:1502.03167</a> (2).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="73."><p class="c-article-references__text" id="ref-CR73">Kanbilim Dataset. <a href="http://kanbilim.com/" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="http://kanbilim.com/">http://kanbilim.com/</a>. Accessed: 2019-06-15.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="74."><p class="c-article-references__text" id="ref-CR74">Ghosh A, Singh S, Sheet D. Simultaneous localization and classification of acute lymphoblastic leukemic cells in peripheral blood smears using a deep convolutional network with average pooling layer. In: 2017 IEEE ICIIS. IEEE; 2017. p. 1–6.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="75."><p class="c-article-references__text" id="ref-CR75">WBC-classification. <a href="https://bit.ly/2zbz8oA" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="https://bit.ly/2zbz8oA">https://bit.ly/2zbz8oA</a>. Accessed: 2019-06-15.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="76."><p class="c-article-references__text" id="ref-CR76">Liang G, Hong H, Xie W, Zheng L. Combining convolutional neural network with recursive neural network for blood cell image classification. IEEE Access. 2018;6:36188–97.</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1109/ACCESS.2018.2846685" data-track-item_id="10.1109/ACCESS.2018.2846685" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1109%2FACCESS.2018.2846685" aria-label="Article reference 76" data-doi="10.1109/ACCESS.2018.2846685">Article</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 76" href="http://scholar.google.com/scholar_lookup?&amp;title=Combining%20convolutional%20neural%20network%20with%20recursive%20neural%20network%20for%20blood%20cell%20image%20classification&amp;journal=IEEE%20Access&amp;doi=10.1109%2FACCESS.2018.2846685&amp;volume=6&amp;pages=36188-36197&amp;publication_year=2018&amp;author=Liang%2CG&amp;author=Hong%2CH&amp;author=Xie%2CW&amp;author=Zheng%2CL">
                    Google Scholar</a>&nbsp;
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="77."><p class="c-article-references__text" id="ref-CR77">BCCD dataset. <a href="https://bit.ly/2X5vQOl" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="https://bit.ly/2X5vQOl">https://bit.ly/2X5vQOl</a>. Accessed: 2019-06-15.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="78."><p class="c-article-references__text" id="ref-CR78">An efficient technique for white blood cells nuclei automatic segmentation. <a href="https://bit.ly/2XN064Z" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="https://bit.ly/2XN064Z">https://bit.ly/2XN064Z</a>. Accessed: 2019-06-15.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="79."><p class="c-article-references__text" id="ref-CR79">Putzu L, Caocci G, Di Ruberto C. Leucocyte classification for leukaemia detection using image processing techniques. Artif Intell Med. 2014;62(3):179–91.</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1016/j.artmed.2014.09.002" data-track-item_id="10.1016/j.artmed.2014.09.002" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.artmed.2014.09.002" aria-label="Article reference 79" data-doi="10.1016/j.artmed.2014.09.002">Article</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=25241903" aria-label="PubMed reference 79">PubMed</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 79" href="http://scholar.google.com/scholar_lookup?&amp;title=Leucocyte%20classification%20for%20leukaemia%20detection%20using%20image%20processing%20techniques&amp;journal=Artif%20Intell%20Med&amp;doi=10.1016%2Fj.artmed.2014.09.002&amp;volume=62&amp;issue=3&amp;pages=179-191&amp;publication_year=2014&amp;author=Putzu%2CL&amp;author=Caocci%2CG&amp;author=Ruberto%2CC">
                    Google Scholar</a>&nbsp;
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="80."><p class="c-article-references__text" id="ref-CR80">CellaVision. <a href="https://www.cellavision.com/" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="https://www.cellavision.com/">https://www.cellavision.com/</a>. Accessed: 2019-06-15.</p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" rel="nofollow" href="https://citation-needed.springer.com/v2/references/10.1186/s12880-022-00818-1?format=refman&amp;flavour=references">Download references<svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-download-medium"></use></svg></a></p></div></div></div></section></div><section data-title="Acknowledgements"><div class="c-article-section" id="Ack1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Ack1">Acknowledgements</h2><div class="c-article-section__content" id="Ack1-content"><p>This research was supported by the National Research Foundation (NRF-2016K1A1A2912757), by Institute of Information &amp; communications Technology Planning &amp; Evaluation (IITP) grant funded by the Korea Government (MSIT) (No. 2021-0-02068, Artificial Intelligence Innovation Hub), and by the Technology Innovation Program (No: 10049771, Development of Highly-Specialized Platform for IVD Medical Devices, and No: 10059106, Development of a smart white blood cell image analyzer with 60t/h throughput and sub- m imaging device, based on Manual Review Center with less than 1% analysis error) funded by the Ministry of Trade, Industry &amp; Energy, Republic of Korea.</p></div></div></section><section data-title="Funding"><div class="c-article-section" id="Fun-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Fun">Funding</h2><div class="c-article-section__content" id="Fun-content"><p>This research was supported by the National Research Foundation (NRF-2016K1A1A2912757), by Institute of Information &amp; communications Technology Planning &amp; Evaluation (IITP) grant funded by the Korea Government&nbsp;(MSIT) (No. 2021–0-02068, Artificial Intelligence Innovation Hub), and by the Technology Innovation Program (No: 10049771, Development of Highly-Specialized Platform for IVD Medical Devices, and No: 10059106, Development of a smart white blood cell image analyzer with 60t/h throughput and sub- m imaging device, based on Manual Review Center with less than 1% analysis error) funded by the Ministry of Trade, Industry &amp; Energy, Republic of Korea.</p></div></div></section><section aria-labelledby="author-information" data-title="Author information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Authors and Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">Department of Cyber Security, Ewha Womans University, 52, Ewhayeodae-gil, Seodaemun-gu, Seoul, 03760, Republic of Korea</p><p class="c-article-author-affiliation__authors-list">Changhun Jung&nbsp;&amp;&nbsp;DaeHun Nyang</p></li><li id="Aff2"><p class="c-article-author-affiliation__address">Department of Computer Science, Loyola University Chicago, 1032 W Sheridan Rd, Chicago, 60660, USA</p><p class="c-article-author-affiliation__authors-list">Mohammed Abuhamad</p></li><li id="Aff3"><p class="c-article-author-affiliation__address">Department of Computer Science, University of Central Florida, 4000 Central Florida Blvd, Orlando, FL, 32816, USA</p><p class="c-article-author-affiliation__authors-list">David Mohaisen</p></li><li id="Aff4"><p class="c-article-author-affiliation__address">Department of Laboratory Medicine and College of Medicine, The Catholic University of Korea Seoul St. Mary’s Hospital, 222, Banpo-daero, Seocho-gu, Seoul, 06591, Republic of Korea</p><p class="c-article-author-affiliation__authors-list">Kyungja Han</p></li></ol><div class="u-js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Changhun-Jung-Aff1"><span class="c-article-authors-search__title u-h3 js-search-name">Changhun Jung</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?sortBy=newestFirst&amp;dc.creator=Changhun%20Jung" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" rel="nofollow">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text"><span class="c-article-authors-search__links-text">You can also search for this author in</span><span class="c-article-identifiers"><a class="c-article-identifiers__item" href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Changhun%20Jung" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span class="u-hide">&nbsp;</span><a class="c-article-identifiers__item" href="https://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Changhun%20Jung%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></div></li><li id="auth-Mohammed-Abuhamad-Aff2"><span class="c-article-authors-search__title u-h3 js-search-name">Mohammed Abuhamad</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?sortBy=newestFirst&amp;dc.creator=Mohammed%20Abuhamad" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" rel="nofollow">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text"><span class="c-article-authors-search__links-text">You can also search for this author in</span><span class="c-article-identifiers"><a class="c-article-identifiers__item" href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Mohammed%20Abuhamad" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span class="u-hide">&nbsp;</span><a class="c-article-identifiers__item" href="https://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Mohammed%20Abuhamad%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></div></li><li id="auth-David-Mohaisen-Aff3"><span class="c-article-authors-search__title u-h3 js-search-name">David Mohaisen</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?sortBy=newestFirst&amp;dc.creator=David%20Mohaisen" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" rel="nofollow">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text"><span class="c-article-authors-search__links-text">You can also search for this author in</span><span class="c-article-identifiers"><a class="c-article-identifiers__item" href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=David%20Mohaisen" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span class="u-hide">&nbsp;</span><a class="c-article-identifiers__item" href="https://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22David%20Mohaisen%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></div></li><li id="auth-Kyungja-Han-Aff4"><span class="c-article-authors-search__title u-h3 js-search-name">Kyungja Han</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?sortBy=newestFirst&amp;dc.creator=Kyungja%20Han" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" rel="nofollow">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text"><span class="c-article-authors-search__links-text">You can also search for this author in</span><span class="c-article-identifiers"><a class="c-article-identifiers__item" href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Kyungja%20Han" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span class="u-hide">&nbsp;</span><a class="c-article-identifiers__item" href="https://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Kyungja%20Han%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></div></li><li id="auth-DaeHun-Nyang-Aff1"><span class="c-article-authors-search__title u-h3 js-search-name">DaeHun Nyang</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?sortBy=newestFirst&amp;dc.creator=DaeHun%20Nyang" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" rel="nofollow">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text"><span class="c-article-authors-search__links-text">You can also search for this author in</span><span class="c-article-identifiers"><a class="c-article-identifiers__item" href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=DaeHun%20Nyang" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span class="u-hide">&nbsp;</span><a class="c-article-identifiers__item" href="https://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22DaeHun%20Nyang%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="contributions">Contributions</h3><p>Model implementation, CJ; experiment, CJ, MA; analysis of experiment results, CJ, MA, DM, DN; related work, MA; network design, DM, DN, dataset provision and analysis, KH; administrative support, DN; All authors read and approval the final manuscript.</p><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" href="mailto:nyang@ewha.ac.kr">DaeHun Nyang</a>.</p></div></div></section><section data-title="Ethics declarations"><div class="c-article-section" id="ethics-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="ethics">Ethics declarations</h2><div class="c-article-section__content" id="ethics-content">
              
              
                <h3 class="c-article__sub-heading" id="FPar1">Ethics approval and consent to participate</h3>
                <p>The dataset and was provided by The Catholic University of Korea (The CUK), and approved by the Institutional Review Board (IRB) of The CUK [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 60" title="The Catholic University of Korea Institutional Review Board. 
                  https://bit.ly/2YrlQPl
                  
                . Accessed: 2019-07-17." href="/article/10.1186/s12880-022-00818-1#ref-CR60" id="ref-link-section-d107567642e5072">60</a>]. The experimental protocols and informed consent were approved by the Institutional Review Board (IRB) of The CUK [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 60" title="The Catholic University of Korea Institutional Review Board. 
                  https://bit.ly/2YrlQPl
                  
                . Accessed: 2019-07-17." href="/article/10.1186/s12880-022-00818-1#ref-CR60" id="ref-link-section-d107567642e5075">60</a>].</p>
              
              
                <h3 class="c-article__sub-heading" id="FPar2">Consent for publication</h3>
                <p>Not applicable.</p>
              
              
                <h3 class="c-article__sub-heading" id="FPar3">Competing interests</h3>
                <p>The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.</p>
              
            </div></div></section><section data-title="Additional information"><div class="c-article-section" id="additional-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="additional-information">Additional information</h2><div class="c-article-section__content" id="additional-information-content"><h3 class="c-article__sub-heading">Publisher's Note</h3><p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p><p>An earlier version of this work has appeared in Association for the Advancement of Artificial Intelligence (AAAI) 2019 Fall Symposium on AI for Social Good [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1" title="Changhun J, Mohammed A, Jumabek A, Aziz M, Kyungja H, DaeHun N. W-Net: a CNN-based architecture for white blood cells image classification. In: AAAI 2019 fall symposium on AI for social good; 2019." href="/article/10.1186/s12880-022-00818-1#ref-CR1" id="ref-link-section-d107567642e464">1</a>]</p></div></div></section><section aria-labelledby="appendices"><div class="c-article-section" id="appendices-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="appendices">Appendix 1: The detailed results for all experiments</h2><div class="c-article-section__content" id="appendices-content"><h3 class="c-article__sub-heading u-visually-hidden" id="App1">Appendix 1: The detailed results for all experiments</h3><p>In this section, we show the detailed results of tenfold cross validation for W-Net (Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1186/s12880-022-00818-1#Tab10">10</a>), W-Net-SVM (Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1186/s12880-022-00818-1#Tab11">11</a>), AlexNet (Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1186/s12880-022-00818-1#Tab12">12</a>), VGGNet (Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1186/s12880-022-00818-1#Tab13">13</a>), ResNet (Tables <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1186/s12880-022-00818-1#Tab14">14</a>,  <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1186/s12880-022-00818-1#Tab15">15</a>), RNN (Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1186/s12880-022-00818-1#Tab16">16</a>) and further training (Tables <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1186/s12880-022-00818-1#Tab17">17</a>, <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1186/s12880-022-00818-1#Tab18">18</a>).</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-10"><figure><figcaption class="c-article-table__figcaption"><b id="Tab10" data-test="table-caption">Table 10 The result of tenfold cross-validation of W-Net for classification accuracy</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1186/s12880-022-00818-1/tables/10" aria-label="Full size table 10"><span>Full size table</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-11"><figure><figcaption class="c-article-table__figcaption"><b id="Tab11" data-test="table-caption">Table 11 The result of tenfold cross-validation of W-Net-SVM for classification accuracy</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1186/s12880-022-00818-1/tables/11" aria-label="Full size table 11"><span>Full size table</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-12"><figure><figcaption class="c-article-table__figcaption"><b id="Tab12" data-test="table-caption">Table 12 The result of tenfold cross-validation of AlexNet for classification accuracy</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1186/s12880-022-00818-1/tables/12" aria-label="Full size table 12"><span>Full size table</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-13"><figure><figcaption class="c-article-table__figcaption"><b id="Tab13" data-test="table-caption">Table 13 The result of tenfold cross-validation of VGGNet for classification accuracy</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1186/s12880-022-00818-1/tables/13" aria-label="Full size table 13"><span>Full size table</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-14"><figure><figcaption class="c-article-table__figcaption"><b id="Tab14" data-test="table-caption">Table 14 The result of ResNet50 for classification using tenfold cross-validation</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1186/s12880-022-00818-1/tables/14" aria-label="Full size table 14"><span>Full size table</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-15"><figure><figcaption class="c-article-table__figcaption"><b id="Tab15" data-test="table-caption">Table 15 The result of ResNet18 for classification using tenfold cross-validation</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1186/s12880-022-00818-1/tables/15" aria-label="Full size table 15"><span>Full size table</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-16"><figure><figcaption class="c-article-table__figcaption"><b id="Tab16" data-test="table-caption">Table 16 Tenfold evaluation of LSTM (RNN) model</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1186/s12880-022-00818-1/tables/16" aria-label="Full size table 16"><span>Full size table</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-17"><figure><figcaption class="c-article-table__figcaption"><b id="Tab17" data-test="table-caption">Table 17 The result of the first model trained using LISC public data from scratch</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1186/s12880-022-00818-1/tables/17" aria-label="Full size table 17"><span>Full size table</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-18"><figure><figcaption class="c-article-table__figcaption"><b id="Tab18" data-test="table-caption">Table 18 The result of the second model was initially trained using our dataset which is our W-Net model and then further trained using LISC public data</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1186/s12880-022-00818-1/tables/18" aria-label="Full size table 18"><span>Full size table</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div></div></div></section><section data-title="Rights and permissions"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content">
                <p><b>Open Access</b> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <a href="http://creativecommons.org/licenses/by/4.0/" rel="license">http://creativecommons.org/licenses/by/4.0/</a>. The Creative Commons Public Domain Dedication waiver (<a href="http://creativecommons.org/publicdomain/zero/1.0/" rel="license">http://creativecommons.org/publicdomain/zero/1.0/</a>) applies to the data made available in this article, unless otherwise stated in a credit line to the data.</p>
              <p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=WBC%20image%20classification%20and%20generative%20models%20based%20on%20convolutional%20neural%20network&amp;author=Changhun%20Jung%20et%20al&amp;contentID=10.1186%2Fs12880-022-00818-1&amp;copyright=The%20Author%28s%29&amp;publication=1471-2342&amp;publicationDate=2022-05-20&amp;publisherName=SpringerNature&amp;orderBeanReset=true&amp;oa=CC%20BY%20%2B%20CC0">Reprints and permissions</a></p></div></div></section><section aria-labelledby="article-info" data-title="About this article"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="u-hide-print c-bibliographic-information__column c-bibliographic-information__column--border"><a data-crossmark="10.1186/s12880-022-00818-1" target="_blank" rel="noopener" href="https://crossmark.crossref.org/dialog/?doi=10.1186/s12880-022-00818-1" data-track="click" data-track-action="Click Crossmark" data-track-label="link" data-test="crossmark"><img loading="lazy" width="57" height="81" alt="Check for updates. Verify currency and authenticity via CrossMark" src="data:image/svg+xml;base64,<svg height="81" width="57" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd"><path d="m17.35 35.45 21.3-14.2v-17.03h-21.3" fill="#989898"/><path d="m38.65 35.45-21.3-14.2v-17.03h21.3" fill="#747474"/><path d="m28 .5c-12.98 0-23.5 10.52-23.5 23.5s10.52 23.5 23.5 23.5 23.5-10.52 23.5-23.5c0-6.23-2.48-12.21-6.88-16.62-4.41-4.4-10.39-6.88-16.62-6.88zm0 41.25c-9.8 0-17.75-7.95-17.75-17.75s7.95-17.75 17.75-17.75 17.75 7.95 17.75 17.75c0 4.71-1.87 9.22-5.2 12.55s-7.84 5.2-12.55 5.2z" fill="#535353"/><path d="m41 36c-5.81 6.23-15.23 7.45-22.43 2.9-7.21-4.55-10.16-13.57-7.03-21.5l-4.92-3.11c-4.95 10.7-1.19 23.42 8.78 29.71 9.97 6.3 23.07 4.22 30.6-4.86z" fill="#9c9c9c"/><path d="m.2 58.45c0-.75.11-1.42.33-2.01s.52-1.09.91-1.5c.38-.41.83-.73 1.34-.94.51-.22 1.06-.32 1.65-.32.56 0 1.06.11 1.51.35.44.23.81.5 1.1.81l-.91 1.01c-.24-.24-.49-.42-.75-.56-.27-.13-.58-.2-.93-.2-.39 0-.73.08-1.05.23-.31.16-.58.37-.81.66-.23.28-.41.63-.53 1.04-.13.41-.19.88-.19 1.39 0 1.04.23 1.86.68 2.46.45.59 1.06.88 1.84.88.41 0 .77-.07 1.07-.23s.59-.39.85-.68l.91 1c-.38.43-.8.76-1.28.99-.47.22-1 .34-1.58.34-.59 0-1.13-.1-1.64-.31-.5-.2-.94-.51-1.31-.91-.38-.4-.67-.9-.88-1.48-.22-.59-.33-1.26-.33-2.02zm8.4-5.33h1.61v2.54l-.05 1.33c.29-.27.61-.51.96-.72s.76-.31 1.24-.31c.73 0 1.27.23 1.61.71.33.47.5 1.14.5 2.02v4.31h-1.61v-4.1c0-.57-.08-.97-.25-1.21-.17-.23-.45-.35-.83-.35-.3 0-.56.08-.79.22-.23.15-.49.36-.78.64v4.8h-1.61zm7.37 6.45c0-.56.09-1.06.26-1.51.18-.45.42-.83.71-1.14.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.36c.07.62.29 1.1.65 1.44.36.33.82.5 1.38.5.29 0 .57-.04.83-.13s.51-.21.76-.37l.55 1.01c-.33.21-.69.39-1.09.53-.41.14-.83.21-1.26.21-.48 0-.92-.08-1.34-.25-.41-.16-.76-.4-1.07-.7-.31-.31-.55-.69-.72-1.13-.18-.44-.26-.95-.26-1.52zm4.6-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.07.45-.31.29-.5.73-.58 1.3zm2.5.62c0-.57.09-1.08.28-1.53.18-.44.43-.82.75-1.13s.69-.54 1.1-.71c.42-.16.85-.24 1.31-.24.45 0 .84.08 1.17.23s.61.34.85.57l-.77 1.02c-.19-.16-.38-.28-.56-.37-.19-.09-.39-.14-.61-.14-.56 0-1.01.21-1.35.63-.35.41-.52.97-.52 1.67 0 .69.17 1.24.51 1.66.34.41.78.62 1.32.62.28 0 .54-.06.78-.17.24-.12.45-.26.64-.42l.67 1.03c-.33.29-.69.51-1.08.65-.39.15-.78.23-1.18.23-.46 0-.9-.08-1.31-.24-.4-.16-.75-.39-1.05-.7s-.53-.69-.7-1.13c-.17-.45-.25-.96-.25-1.53zm6.91-6.45h1.58v6.17h.05l2.54-3.16h1.77l-2.35 2.8 2.59 4.07h-1.75l-1.77-2.98-1.08 1.23v1.75h-1.58zm13.69 1.27c-.25-.11-.5-.17-.75-.17-.58 0-.87.39-.87 1.16v.75h1.34v1.27h-1.34v5.6h-1.61v-5.6h-.92v-1.2l.92-.07v-.72c0-.35.04-.68.13-.98.08-.31.21-.57.4-.79s.42-.39.71-.51c.28-.12.63-.18 1.04-.18.24 0 .48.02.69.07.22.05.41.1.57.17zm.48 5.18c0-.57.09-1.08.27-1.53.17-.44.41-.82.72-1.13.3-.31.65-.54 1.04-.71.39-.16.8-.24 1.23-.24s.84.08 1.24.24c.4.17.74.4 1.04.71s.54.69.72 1.13c.19.45.28.96.28 1.53s-.09 1.08-.28 1.53c-.18.44-.42.82-.72 1.13s-.64.54-1.04.7-.81.24-1.24.24-.84-.08-1.23-.24-.74-.39-1.04-.7c-.31-.31-.55-.69-.72-1.13-.18-.45-.27-.96-.27-1.53zm1.65 0c0 .69.14 1.24.43 1.66.28.41.68.62 1.18.62.51 0 .9-.21 1.19-.62.29-.42.44-.97.44-1.66 0-.7-.15-1.26-.44-1.67-.29-.42-.68-.63-1.19-.63-.5 0-.9.21-1.18.63-.29.41-.43.97-.43 1.67zm6.48-3.44h1.33l.12 1.21h.05c.24-.44.54-.79.88-1.02.35-.24.7-.36 1.07-.36.32 0 .59.05.78.14l-.28 1.4-.33-.09c-.11-.01-.23-.02-.38-.02-.27 0-.56.1-.86.31s-.55.58-.77 1.1v4.2h-1.61zm-47.87 15h1.61v4.1c0 .57.08.97.25 1.2.17.24.44.35.81.35.3 0 .57-.07.8-.22.22-.15.47-.39.73-.73v-4.7h1.61v6.87h-1.32l-.12-1.01h-.04c-.3.36-.63.64-.98.86-.35.21-.76.32-1.24.32-.73 0-1.27-.24-1.61-.71-.33-.47-.5-1.14-.5-2.02zm9.46 7.43v2.16h-1.61v-9.59h1.33l.12.72h.05c.29-.24.61-.45.97-.63.35-.17.72-.26 1.1-.26.43 0 .81.08 1.15.24.33.17.61.4.84.71.24.31.41.68.53 1.11.13.42.19.91.19 1.44 0 .59-.09 1.11-.25 1.57-.16.47-.38.85-.65 1.16-.27.32-.58.56-.94.73-.35.16-.72.25-1.1.25-.3 0-.6-.07-.9-.2s-.59-.31-.87-.56zm0-2.3c.26.22.5.37.73.45.24.09.46.13.66.13.46 0 .84-.2 1.15-.6.31-.39.46-.98.46-1.77 0-.69-.12-1.22-.35-1.61-.23-.38-.61-.57-1.13-.57-.49 0-.99.26-1.52.77zm5.87-1.69c0-.56.08-1.06.25-1.51.16-.45.37-.83.65-1.14.27-.3.58-.54.93-.71s.71-.25 1.08-.25c.39 0 .73.07 1 .2.27.14.54.32.81.55l-.06-1.1v-2.49h1.61v9.88h-1.33l-.11-.74h-.06c-.25.25-.54.46-.88.64-.33.18-.69.27-1.06.27-.87 0-1.56-.32-2.07-.95s-.76-1.51-.76-2.65zm1.67-.01c0 .74.13 1.31.4 1.7.26.38.65.58 1.15.58.51 0 .99-.26 1.44-.77v-3.21c-.24-.21-.48-.36-.7-.45-.23-.08-.46-.12-.7-.12-.45 0-.82.19-1.13.59-.31.39-.46.95-.46 1.68zm6.35 1.59c0-.73.32-1.3.97-1.71.64-.4 1.67-.68 3.08-.84 0-.17-.02-.34-.07-.51-.05-.16-.12-.3-.22-.43s-.22-.22-.38-.3c-.15-.06-.34-.1-.58-.1-.34 0-.68.07-1 .2s-.63.29-.93.47l-.59-1.08c.39-.24.81-.45 1.28-.63.47-.17.99-.26 1.54-.26.86 0 1.51.25 1.93.76s.63 1.25.63 2.21v4.07h-1.32l-.12-.76h-.05c-.3.27-.63.48-.98.66s-.73.27-1.14.27c-.61 0-1.1-.19-1.48-.56-.38-.36-.57-.85-.57-1.46zm1.57-.12c0 .3.09.53.27.67.19.14.42.21.71.21.28 0 .54-.07.77-.2s.48-.31.73-.56v-1.54c-.47.06-.86.13-1.18.23-.31.09-.57.19-.76.31s-.33.25-.41.4c-.09.15-.13.31-.13.48zm6.29-3.63h-.98v-1.2l1.06-.07.2-1.88h1.34v1.88h1.75v1.27h-1.75v3.28c0 .8.32 1.2.97 1.2.12 0 .24-.01.37-.04.12-.03.24-.07.34-.11l.28 1.19c-.19.06-.4.12-.64.17-.23.05-.49.08-.76.08-.4 0-.74-.06-1.02-.18-.27-.13-.49-.3-.67-.52-.17-.21-.3-.48-.37-.78-.08-.3-.12-.64-.12-1.01zm4.36 2.17c0-.56.09-1.06.27-1.51s.41-.83.71-1.14c.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.37c.08.62.29 1.1.65 1.44.36.33.82.5 1.38.5.3 0 .58-.04.84-.13.25-.09.51-.21.76-.37l.54 1.01c-.32.21-.69.39-1.09.53s-.82.21-1.26.21c-.47 0-.92-.08-1.33-.25-.41-.16-.77-.4-1.08-.7-.3-.31-.54-.69-.72-1.13-.17-.44-.26-.95-.26-1.52zm4.61-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.08.45-.31.29-.5.73-.57 1.3zm3.01 2.23c.31.24.61.43.92.57.3.13.63.2.98.2.38 0 .65-.08.83-.23s.27-.35.27-.6c0-.14-.05-.26-.13-.37-.08-.1-.2-.2-.34-.28-.14-.09-.29-.16-.47-.23l-.53-.22c-.23-.09-.46-.18-.69-.3-.23-.11-.44-.24-.62-.4s-.33-.35-.45-.55c-.12-.21-.18-.46-.18-.75 0-.61.23-1.1.68-1.49.44-.38 1.06-.57 1.83-.57.48 0 .91.08 1.29.25s.71.36.99.57l-.74.98c-.24-.17-.49-.32-.73-.42-.25-.11-.51-.16-.78-.16-.35 0-.6.07-.76.21-.17.15-.25.33-.25.54 0 .14.04.26.12.36s.18.18.31.26c.14.07.29.14.46.21l.54.19c.23.09.47.18.7.29s.44.24.64.4c.19.16.34.35.46.58.11.23.17.5.17.82 0 .3-.06.58-.17.83-.12.26-.29.48-.51.68-.23.19-.51.34-.84.45-.34.11-.72.17-1.15.17-.48 0-.95-.09-1.41-.27-.46-.19-.86-.41-1.2-.68z" fill="#535353"/></g></svg>"></a></div><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Jung, C., Abuhamad, M., Mohaisen, D. <i>et al.</i> WBC image classification and generative models based on convolutional neural network.
                    <i>BMC Med Imaging</i> <b>22</b>, 94 (2022). https://doi.org/10.1186/s12880-022-00818-1</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" data-track-external="" rel="nofollow" href="https://citation-needed.springer.com/v2/references/10.1186/s12880-022-00818-1?format=refman&amp;flavour=citation">Download citation<svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-download-medium"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2021-11-03">03 November 2021</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2022-05-06">06 May 2022</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2022-05-20">20 May 2022</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--full-width"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value">https://doi.org/10.1186/s12880-022-00818-1</span></p></li></ul><div data-component="share-box"><div class="c-article-share-box u-display-block"><h3 class="c-article__sub-heading">Share this article</h3><p class="c-article-share-box__description">Anyone you share the following link with will be able to read this content:</p><button class="js-get-share-url c-article-share-box__button" type="button" id="get-share-url" data-track="click" data-track-label="button" data-track-external="" data-track-action="get shareable link">Get shareable link</button><div class="js-no-share-url-container u-display-none" hidden=""><p class="js-c-article-share-box__no-sharelink-info c-article-share-box__no-sharelink-info">Sorry, a shareable link is not currently available for this article.</p></div><div class="js-share-url-container u-display-none" hidden=""><p class="js-share-url c-article-share-box__only-read-input" id="share-url" data-track="click" data-track-label="button" data-track-action="select share url"></p><button class="js-copy-share-url c-article-share-box__button--link-like" type="button" id="copy-share-url" data-track="click" data-track-label="button" data-track-action="copy share url" data-track-external="">Copy to clipboard</button></div><p class="js-c-article-share-box__additional-info c-article-share-box__additional-info">
                            Provided by the Springer Nature SharedIt content-sharing initiative
                        </p></div></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span><a href="/search?query=White%20blood%20cell&amp;facet-discipline=&quot;Medicine%20%26%20Public%20Health&quot;" data-track="click" data-track-action="view keyword" data-track-label="link">White blood cell</a></span></li><li class="c-article-subject-list__subject"><span><a href="/search?query=Classification&amp;facet-discipline=&quot;Medicine%20%26%20Public%20Health&quot;" data-track="click" data-track-action="view keyword" data-track-label="link">Classification</a></span></li><li class="c-article-subject-list__subject"><span><a href="/search?query=Medical%20image&amp;facet-discipline=&quot;Medicine%20%26%20Public%20Health&quot;" data-track="click" data-track-action="view keyword" data-track-label="link">Medical image</a></span></li><li class="c-article-subject-list__subject"><span><a href="/search?query=CNN&amp;facet-discipline=&quot;Medicine%20%26%20Public%20Health&quot;" data-track="click" data-track-action="view keyword" data-track-label="link">CNN</a></span></li><li class="c-article-subject-list__subject"><span><a href="/search?query=Deep%20learning&amp;facet-discipline=&quot;Medicine%20%26%20Public%20Health&quot;" data-track="click" data-track-action="view keyword" data-track-label="link">Deep learning</a></span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>

                    

                    
                </div>
            </main>

            <div class="c-article-sidebar u-text-sm u-hide-print l-with-sidebar__sidebar" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
                <aside aria-label="reading companion">
                    
                        
                    

                    
                        <div data-test="collections">
                            
    

                        </div>
                    

                    <div data-test="editorial-summary">
                        
                    </div>

                    <div class="c-reading-companion">
                        <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky" style="top: 40px;">
                            

                            
                                
                            

                            <ul class="c-reading-companion__tabs" role="tablist"><li role="presentation"><button data-tab-target="sections" role="tab" id="tab-sections" aria-controls="tabpanel-sections" aria-selected="true" class="c-reading-companion__tab c-reading-companion__tab--active" data-track="click" data-track-action="sections tab" data-track-label="tab">Sections</button></li><li role="presentation"><button data-tab-target="figures" role="tab" id="tab-figures" aria-controls="tabpanel-figures" aria-selected="false" tabindex="-1" class="c-reading-companion__tab" data-track="click" data-track-action="figures tab" data-track-label="tab">Figures</button></li><li role="presentation"><button data-tab-target="references" role="tab" id="tab-references" aria-controls="tabpanel-references" aria-selected="false" tabindex="-1" class="c-reading-companion__tab" data-track="click" data-track-action="references tab" data-track-label="tab">References</button></li></ul><div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections" aria-labelledby="tab-sections" role="tabpanel"><div class="c-reading-companion__scroll-pane" style="max-height: none;"><ul class="c-reading-companion__sections-list"><li id="rc-sec-Abs1" class="c-reading-companion__section-item"><a href="#Abs1" data-track="click" data-track-action="section anchor" data-track-label="link:Abstract">Abstract</a></li><li id="rc-sec-Sec1" class="c-reading-companion__section-item"><a href="#Sec1" data-track="click" data-track-action="section anchor" data-track-label="link:Background">Background</a></li><li id="rc-sec-Sec4" class="c-reading-companion__section-item"><a href="#Sec4" data-track="click" data-track-action="section anchor" data-track-label="link:Related works">Related works</a></li><li id="rc-sec-Sec7" class="c-reading-companion__section-item"><a href="#Sec7" data-track="click" data-track-action="section anchor" data-track-label="link:Methods">Methods</a></li><li id="rc-sec-Sec11" class="c-reading-companion__section-item"><a href="#Sec11" data-track="click" data-track-action="section anchor" data-track-label="link:Experiments">Experiments</a></li><li id="rc-sec-Sec20" class="c-reading-companion__section-item"><a href="#Sec20" data-track="click" data-track-action="section anchor" data-track-label="link:Design considerations for WNet">Design considerations for W-Net</a></li><li id="rc-sec-Sec24" class="c-reading-companion__section-item"><a href="#Sec24" data-track="click" data-track-action="section anchor" data-track-label="link:Dataset sharing">Dataset sharing</a></li><li id="rc-sec-Sec27" class="c-reading-companion__section-item"><a href="#Sec27" data-track="click" data-track-action="section anchor" data-track-label="link:Conclusion">Conclusion</a></li><li id="rc-sec-availability-of-data-and-materials" class="c-reading-companion__section-item"><a href="#availability-of-data-and-materials" data-track="click" data-track-action="section anchor" data-track-label="link:Availability of data and materials">Availability of data and materials</a></li><li id="rc-sec-Bib1" class="c-reading-companion__section-item"><a href="#Bib1" data-track="click" data-track-action="section anchor" data-track-label="link:References">References</a></li><li id="rc-sec-Ack1" class="c-reading-companion__section-item"><a href="#Ack1" data-track="click" data-track-action="section anchor" data-track-label="link:Acknowledgements">Acknowledgements</a></li><li id="rc-sec-Fun" class="c-reading-companion__section-item"><a href="#Fun" data-track="click" data-track-action="section anchor" data-track-label="link:Funding">Funding</a></li><li id="rc-sec-author-information" class="c-reading-companion__section-item"><a href="#author-information" data-track="click" data-track-action="section anchor" data-track-label="link:Author information">Author information</a></li><li id="rc-sec-ethics" class="c-reading-companion__section-item"><a href="#ethics" data-track="click" data-track-action="section anchor" data-track-label="link:Ethics declarations">Ethics declarations</a></li><li id="rc-sec-additional-information" class="c-reading-companion__section-item"><a href="#additional-information" data-track="click" data-track-action="section anchor" data-track-label="link:Additional information">Additional information</a></li><li id="rc-sec-appendices" class="c-reading-companion__section-item"><a href="#appendices" data-track="click" data-track-action="section anchor" data-track-label="link:Appendix 1 The detailed results for all experiments">Appendix 1: The detailed results for all experiments</a></li><li id="rc-sec-rightslink" class="c-reading-companion__section-item"><a href="#rightslink" data-track="click" data-track-action="section anchor" data-track-label="link:Rights and permissions">Rights and permissions</a></li><li id="rc-sec-article-info" class="c-reading-companion__section-item"><a href="#article-info" data-track="click" data-track-action="section anchor" data-track-label="link:About this article">About this article</a></li></ul></div>
                                <div class="u-lazy-ad-wrapper u-mt-16 u-show" data-component-mpu=""><div class="c-ad c-ad--300x250">
    <div class="c-ad__inner">
        <p class="c-ad__label">Advertisement</p>
        <div id="div-gpt-ad-MPU1" class="div-gpt-ad grade-c-hide" data-pa11y-ignore="" data-gpt="" data-gpt-unitpath="/270604982/springerlink/12880/article" data-gpt-sizes="300x250" data-test="MPU1-ad" data-gpt-targeting="pos=MPU1;articleid=s12880-022-00818-1;">
        </div>
    </div>
</div>

</div>
                            </div>
                            <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures" aria-labelledby="tab-figures" role="tabpanel"><div class="c-reading-companion__scroll-pane"><ul class="c-reading-companion__figures-list"><li class="c-reading-companion__figure-item"><figure><figcaption><b class="c-reading-companion__figure-title u-h4" id="rc-Fig1">Fig. 1</b></figcaption><picture><source data-srcset="https://media.springernature.com/lw685/springer-static/image/art%3A10.1186%2Fs12880-022-00818-1/MediaObjects/12880_2022_818_Fig1_HTML.png?"><img data-src="https://media.springernature.com/lw685/springer-static/image/art%3A10.1186%2Fs12880-022-00818-1/MediaObjects/12880_2022_818_Fig1_HTML.png" alt="figure 1" aria-describedby="rc-Fig1"></picture><p class="c-reading-companion__figure-links"><a href="#Fig1" data-track="click" data-track-action="figure anchor" data-track-label="link">View in article</a><a href="https://link.springer.com/article/10.1186/s12880-022-00818-1/figures/1" class="c-reading-companion__figure-full-link" data-track="click" data-track-action="view figure" data-track-label="link" rel="nofollow">Full size image<svg width="16" height="16" class="u-icon"><use href="#icon-chevron-right"></use></svg></a></p></figure></li><li class="c-reading-companion__figure-item"><figure><figcaption><b class="c-reading-companion__figure-title u-h4" id="rc-Fig2">Fig. 2</b></figcaption><picture><source data-srcset="https://media.springernature.com/lw685/springer-static/image/art%3A10.1186%2Fs12880-022-00818-1/MediaObjects/12880_2022_818_Fig2_HTML.png?"><img data-src="https://media.springernature.com/lw685/springer-static/image/art%3A10.1186%2Fs12880-022-00818-1/MediaObjects/12880_2022_818_Fig2_HTML.png" alt="figure 2" aria-describedby="rc-Fig2"></picture><p class="c-reading-companion__figure-links"><a href="#Fig2" data-track="click" data-track-action="figure anchor" data-track-label="link">View in article</a><a href="https://link.springer.com/article/10.1186/s12880-022-00818-1/figures/2" class="c-reading-companion__figure-full-link" data-track="click" data-track-action="view figure" data-track-label="link" rel="nofollow">Full size image<svg width="16" height="16" class="u-icon"><use href="#icon-chevron-right"></use></svg></a></p></figure></li><li class="c-reading-companion__figure-item"><figure><figcaption><b class="c-reading-companion__figure-title u-h4" id="rc-Fig3">Fig. 3</b></figcaption><picture><source data-srcset="https://media.springernature.com/lw685/springer-static/image/art%3A10.1186%2Fs12880-022-00818-1/MediaObjects/12880_2022_818_Fig3_HTML.png?"><img data-src="https://media.springernature.com/lw685/springer-static/image/art%3A10.1186%2Fs12880-022-00818-1/MediaObjects/12880_2022_818_Fig3_HTML.png" alt="figure 3" aria-describedby="rc-Fig3"></picture><p class="c-reading-companion__figure-links"><a href="#Fig3" data-track="click" data-track-action="figure anchor" data-track-label="link">View in article</a><a href="https://link.springer.com/article/10.1186/s12880-022-00818-1/figures/3" class="c-reading-companion__figure-full-link" data-track="click" data-track-action="view figure" data-track-label="link" rel="nofollow">Full size image<svg width="16" height="16" class="u-icon"><use href="#icon-chevron-right"></use></svg></a></p></figure></li><li class="c-reading-companion__figure-item"><figure><figcaption><b class="c-reading-companion__figure-title u-h4" id="rc-Fig4">Fig. 4</b></figcaption><picture><source data-srcset="https://media.springernature.com/lw685/springer-static/image/art%3A10.1186%2Fs12880-022-00818-1/MediaObjects/12880_2022_818_Fig4_HTML.png?"><img data-src="https://media.springernature.com/lw685/springer-static/image/art%3A10.1186%2Fs12880-022-00818-1/MediaObjects/12880_2022_818_Fig4_HTML.png" alt="figure 4" aria-describedby="rc-Fig4"></picture><p class="c-reading-companion__figure-links"><a href="#Fig4" data-track="click" data-track-action="figure anchor" data-track-label="link">View in article</a><a href="https://link.springer.com/article/10.1186/s12880-022-00818-1/figures/4" class="c-reading-companion__figure-full-link" data-track="click" data-track-action="view figure" data-track-label="link" rel="nofollow">Full size image<svg width="16" height="16" class="u-icon"><use href="#icon-chevron-right"></use></svg></a></p></figure></li></ul></div></div>
                            <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references" aria-labelledby="tab-references" role="tabpanel"><div class="c-reading-companion__scroll-pane"><ol class="c-reading-companion__references-list c-reading-companion__references-list--numeric" data-track-context="article references sidebar"><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR1">Changhun J, Mohammed A, Jumabek A, Aziz M, Kyungja H, DaeHun N. W-Net: a CNN-based architecture for white blood cells image classification. In: AAAI 2019 fall symposium on AI for social good; 2019.</p></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR2">Pillay J, et al. In vivo labeling with <sup>2</sup>H<sub>2</sub>O reveals a human neutrophil lifespan of 5.4 days. Blood. 2010;116(4):625–7.</p><p class="c-reading-companion__reference-links"><a href="https://doi.org/10.1182%2Fblood-2010-01-259028" data-track="click_references" data-track-action="article reference" data-track-value="article reference" data-track-label="10.1182/blood-2010-01-259028" data-track-item_id="10.1182/blood-2010-01-259028">Article</a>&nbsp;<a href="https://link.springer.com/articles/cas-redirect/1:CAS:528:DC%2BC3cXhtVClu7vJ" data-track="click_references" data-track-action="cas reference" data-track-value="cas reference" data-track-label="link" data-track-item_id="link">CAS</a>&nbsp;<a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=20410504" data-track="click_references" data-track-action="pubmed reference" data-track-value="pubmed reference" data-track-label="link" data-track-item_id="link">PubMed</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=In%20vivo%20labeling%20with%202H2O%20reveals%20a%20human%20neutrophil%20lifespan%20of%205.4%20days&amp;journal=Blood&amp;doi=10.1182%2Fblood-2010-01-259028&amp;volume=116&amp;issue=4&amp;pages=625-627&amp;publication_year=2010&amp;author=Pillay%2CJ" data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link">
                    Google Scholar</a>&nbsp;</p></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR3">Rothenberg ME, Hogan SP. The eosinophil. Annu Rev Immunol. 2006;24:147–74.</p><p class="c-reading-companion__reference-links"><a href="https://doi.org/10.1146%2Fannurev.immunol.24.021605.090720" data-track="click_references" data-track-action="article reference" data-track-value="article reference" data-track-label="10.1146/annurev.immunol.24.021605.090720" data-track-item_id="10.1146/annurev.immunol.24.021605.090720">Article</a>&nbsp;<a href="https://link.springer.com/articles/cas-redirect/1:CAS:528:DC%2BD28XkvFSqt70%3D" data-track="click_references" data-track-action="cas reference" data-track-value="cas reference" data-track-label="link" data-track-item_id="link">CAS</a>&nbsp;<a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=16551246" data-track="click_references" data-track-action="pubmed reference" data-track-value="pubmed reference" data-track-label="link" data-track-item_id="link">PubMed</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=The%20eosinophil&amp;journal=Annu%20Rev%20Immunol&amp;doi=10.1146%2Fannurev.immunol.24.021605.090720&amp;volume=24&amp;pages=147-174&amp;publication_year=2006&amp;author=Rothenberg%2CME&amp;author=Hogan%2CSP" data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link">
                    Google Scholar</a>&nbsp;</p></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR4">Falcone FH, Haas H, Gibbs BF. The human basophil: a new appreciation of its role in immune responses. Blood. 2000;13:4028–38.</p><p class="c-reading-companion__reference-links"><a href="https://doi.org/10.1182%2Fblood.V96.13.4028" data-track="click_references" data-track-action="article reference" data-track-value="article reference" data-track-label="10.1182/blood.V96.13.4028" data-track-item_id="10.1182/blood.V96.13.4028">Article</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=The%20human%20basophil%3A%20a%20new%20appreciation%20of%20its%20role%20in%20immune%20responses&amp;journal=Blood&amp;doi=10.1182%2Fblood.V96.13.4028&amp;volume=13&amp;pages=4028-4038&amp;publication_year=2000&amp;author=Falcone%2CFH&amp;author=Haas%2CH&amp;author=Gibbs%2CBF" data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link">
                    Google Scholar</a>&nbsp;</p></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR5">Butcher EC, Picker LJ. Lymphocyte homing and homeostasis. Science. 1996;272(5258):60–7.</p><p class="c-reading-companion__reference-links"><a href="https://doi.org/10.1126%2Fscience.272.5258.60" data-track="click_references" data-track-action="article reference" data-track-value="article reference" data-track-label="10.1126/science.272.5258.60" data-track-item_id="10.1126/science.272.5258.60">Article</a>&nbsp;<a href="https://link.springer.com/articles/cas-redirect/1:CAS:528:DyaK28XitVKkt70%3D" data-track="click_references" data-track-action="cas reference" data-track-value="cas reference" data-track-label="link" data-track-item_id="link">CAS</a>&nbsp;<a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=8600538" data-track="click_references" data-track-action="pubmed reference" data-track-value="pubmed reference" data-track-label="link" data-track-item_id="link">PubMed</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=Lymphocyte%20homing%20and%20homeostasis&amp;journal=Science&amp;doi=10.1126%2Fscience.272.5258.60&amp;volume=272&amp;issue=5258&amp;pages=60-67&amp;publication_year=1996&amp;author=Butcher%2CEC&amp;author=Picker%2CLJ" data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link">
                    Google Scholar</a>&nbsp;</p></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR6">Gordon S, Taylor PR. Monocyte and macrophage heterogeneity. Nat Rev Immunol. 2005;5:953–64.</p><p class="c-reading-companion__reference-links"><a href="https://doi.org/10.1038%2Fnri1733" data-track="click_references" data-track-action="article reference" data-track-value="article reference" data-track-label="10.1038/nri1733" data-track-item_id="10.1038/nri1733">Article</a>&nbsp;<a href="https://link.springer.com/articles/cas-redirect/1:CAS:528:DC%2BD2MXht1Kntr3N" data-track="click_references" data-track-action="cas reference" data-track-value="cas reference" data-track-label="link" data-track-item_id="link">CAS</a>&nbsp;<a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=16322748" data-track="click_references" data-track-action="pubmed reference" data-track-value="pubmed reference" data-track-label="link" data-track-item_id="link">PubMed</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=Monocyte%20and%20macrophage%20heterogeneity&amp;journal=Nat%20Rev%20Immunol&amp;doi=10.1038%2Fnri1733&amp;volume=5&amp;pages=953-964&amp;publication_year=2005&amp;author=Gordon%2CS&amp;author=Taylor%2CPR" data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link">
                    Google Scholar</a>&nbsp;</p></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR7">WBC (White Blood Cell) Count. <a href="https://bit.ly/3cDg58c" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="https://bit.ly/3cDg58c">https://bit.ly/3cDg58c</a>. Accessed: 2020-06-09.</p></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR8">Statistics. <a href="https://bit.ly/30esTwt" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="https://bit.ly/30esTwt">https://bit.ly/30esTwt</a>. Accessed: 2019-06-27.</p></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR9">Leukemia. <a href="https://bit.ly/32WFwOn" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="https://bit.ly/32WFwOn">https://bit.ly/32WFwOn</a>. Accessed: 2019-06-29.</p></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR10">Shin H, et al. Deep convolutional neural networks for computer-aided detection: CNN architectures, dataset characteristics and transfer learning. IEEE Trans Med Imaging. 2016;35(5):1285–98.</p><p class="c-reading-companion__reference-links"><a href="https://doi.org/10.1109%2FTMI.2016.2528162" data-track="click_references" data-track-action="article reference" data-track-value="article reference" data-track-label="10.1109/TMI.2016.2528162" data-track-item_id="10.1109/TMI.2016.2528162">Article</a>&nbsp;<a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=26886976" data-track="click_references" data-track-action="pubmed reference" data-track-value="pubmed reference" data-track-label="link" data-track-item_id="link">PubMed</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=Deep%20convolutional%20neural%20networks%20for%20computer-aided%20detection%3A%20CNN%20architectures%2C%20dataset%20characteristics%20and%20transfer%20learning&amp;journal=IEEE%20Trans%20Med%20Imaging&amp;doi=10.1109%2FTMI.2016.2528162&amp;volume=35&amp;issue=5&amp;pages=1285-1298&amp;publication_year=2016&amp;author=Shin%2CH" data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link">
                    Google Scholar</a>&nbsp;</p></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR11">Tajbakhsh N, et al. Convolutional neural networks for medical image analysis: full training or fine tuning? IEEE Trans Med Imaging. 2016;35(5):1299–312.</p><p class="c-reading-companion__reference-links"><a href="https://doi.org/10.1109%2FTMI.2016.2535302" data-track="click_references" data-track-action="article reference" data-track-value="article reference" data-track-label="10.1109/TMI.2016.2535302" data-track-item_id="10.1109/TMI.2016.2535302">Article</a>&nbsp;<a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=26978662" data-track="click_references" data-track-action="pubmed reference" data-track-value="pubmed reference" data-track-label="link" data-track-item_id="link">PubMed</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=Convolutional%20neural%20networks%20for%20medical%20image%20analysis%3A%20full%20training%20or%20fine%20tuning%3F&amp;journal=IEEE%20Trans%20Med%20Imaging&amp;doi=10.1109%2FTMI.2016.2535302&amp;volume=35&amp;issue=5&amp;pages=1299-1312&amp;publication_year=2016&amp;author=Tajbakhsh%2CN" data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link">
                    Google Scholar</a>&nbsp;</p></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR12">Gupta H, Jin KH, Nguyen HQ, McCann MT, Unser M. CNN-based projected gradient descent for consistent CT image reconstruction. IEEE Trans Med Imaging. 2018;37(6):1440–53.</p><p class="c-reading-companion__reference-links"><a href="https://doi.org/10.1109%2FTMI.2018.2832656" data-track="click_references" data-track-action="article reference" data-track-value="article reference" data-track-label="10.1109/TMI.2018.2832656" data-track-item_id="10.1109/TMI.2018.2832656">Article</a>&nbsp;<a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=29870372" data-track="click_references" data-track-action="pubmed reference" data-track-value="pubmed reference" data-track-label="link" data-track-item_id="link">PubMed</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=CNN-based%20projected%20gradient%20descent%20for%20consistent%20CT%20image%20reconstruction&amp;journal=IEEE%20Trans%20Med%20Imaging&amp;doi=10.1109%2FTMI.2018.2832656&amp;volume=37&amp;issue=6&amp;pages=1440-1453&amp;publication_year=2018&amp;author=Gupta%2CH&amp;author=Jin%2CKH&amp;author=Nguyen%2CHQ&amp;author=McCann%2CMT&amp;author=Unser%2CM" data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link">
                    Google Scholar</a>&nbsp;</p></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR13">Wolterink JM, Leiner T, Viergever MA, Isgum I. Generative adversarial networks for noise reduction in low-dose CT. IEEE Trans Med Imaging. 2017;36(12):2536–45.</p><p class="c-reading-companion__reference-links"><a href="https://doi.org/10.1109%2FTMI.2017.2708987" data-track="click_references" data-track-action="article reference" data-track-value="article reference" data-track-label="10.1109/TMI.2017.2708987" data-track-item_id="10.1109/TMI.2017.2708987">Article</a>&nbsp;<a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=28574346" data-track="click_references" data-track-action="pubmed reference" data-track-value="pubmed reference" data-track-label="link" data-track-item_id="link">PubMed</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=Generative%20adversarial%20networks%20for%20noise%20reduction%20in%20low-dose%20CT&amp;journal=IEEE%20Trans%20Med%20Imaging&amp;doi=10.1109%2FTMI.2017.2708987&amp;volume=36&amp;issue=12&amp;pages=2536-2545&amp;publication_year=2017&amp;author=Wolterink%2CJM&amp;author=Leiner%2CT&amp;author=Viergever%2CMA&amp;author=Isgum%2CI" data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link">
                    Google Scholar</a>&nbsp;</p></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR14">Krizhevsky A, Sutskever I, Hinton GE. Imagenet classification with deep convolutional neural networks. In: Advances in NIPS; 2012.</p></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR15">Simonyan K, Zisserman A. Very deep convolutional networks for large-scale image recognition. arXiv preprint <a href="http://arxiv.org/abs/1409.1556" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="http://arxiv.org/abs/1409.1556">arXiv:1409.1556</a> (2014).</p></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR16">He K, Zhang X, Ren S, Sun J. Deep residual learning for image recognition. In: IEEE CVPR; 2016. p. 770–8.</p></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR17">Rezatofighi SH, Soltanian-Zadeh H. Automatic recognition of five types of white blood cells in peripheral blood. Comput Med Imaging Graph. 2011;35(4):333–43.</p><p class="c-reading-companion__reference-links"><a href="https://doi.org/10.1016%2Fj.compmedimag.2011.01.003" data-track="click_references" data-track-action="article reference" data-track-value="article reference" data-track-label="10.1016/j.compmedimag.2011.01.003" data-track-item_id="10.1016/j.compmedimag.2011.01.003">Article</a>&nbsp;<a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=21300521" data-track="click_references" data-track-action="pubmed reference" data-track-value="pubmed reference" data-track-label="link" data-track-item_id="link">PubMed</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=Automatic%20recognition%20of%20five%20types%20of%20white%20blood%20cells%20in%20peripheral%20blood&amp;journal=Comput%20Med%20Imaging%20Graph&amp;doi=10.1016%2Fj.compmedimag.2011.01.003&amp;volume=35&amp;issue=4&amp;pages=333-343&amp;publication_year=2011&amp;author=Rezatofighi%2CSH&amp;author=Soltanian-Zadeh%2CH" data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link">
                    Google Scholar</a>&nbsp;</p></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR18">Goodfellow I, et al. Generative adversarial nets. In: Advances in NIPS; 2014. p. 2672–80.</p></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR19">Wang Q, Chang L, Zhou M, Li Q, Liu H, Guo F. A spectral and morphologic method for white blood cell classification. Opt Laser Technol. 2016;84:144–8.</p><p class="c-reading-companion__reference-links"><a href="https://doi.org/10.1016%2Fj.optlastec.2016.05.013" data-track="click_references" data-track-action="article reference" data-track-value="article reference" data-track-label="10.1016/j.optlastec.2016.05.013" data-track-item_id="10.1016/j.optlastec.2016.05.013">Article</a>&nbsp;<a href="https://link.springer.com/articles/cas-redirect/1:CAS:528:DC%2BC28Xosl2htLY%3D" data-track="click_references" data-track-action="cas reference" data-track-value="cas reference" data-track-label="link" data-track-item_id="link">CAS</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=A%20spectral%20and%20morphologic%20method%20for%20white%20blood%20cell%20classification&amp;journal=Opt%20Laser%20Technol&amp;doi=10.1016%2Fj.optlastec.2016.05.013&amp;volume=84&amp;pages=144-148&amp;publication_year=2016&amp;author=Wang%2CQ&amp;author=Chang%2CL&amp;author=Zhou%2CM&amp;author=Li%2CQ&amp;author=Liu%2CH&amp;author=Guo%2CF" data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link">
                    Google Scholar</a>&nbsp;</p></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR20">Dorini LB, Minetto R, Leite NJ. Semiautomatic white blood cell segmentation based on multiscale analysis. IEEE J Biomed Health Inform. 2012;17(1):250–6.</p><p class="c-reading-companion__reference-links"><a href="https://doi.org/10.1109%2FTITB.2012.2207398" data-track="click_references" data-track-action="article reference" data-track-value="article reference" data-track-label="10.1109/TITB.2012.2207398" data-track-item_id="10.1109/TITB.2012.2207398">Article</a>&nbsp;<a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=22855228" data-track="click_references" data-track-action="pubmed reference" data-track-value="pubmed reference" data-track-label="link" data-track-item_id="link">PubMed</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=Semiautomatic%20white%20blood%20cell%20segmentation%20based%20on%20multiscale%20analysis&amp;journal=IEEE%20J%20Biomed%20Health%20Inform&amp;doi=10.1109%2FTITB.2012.2207398&amp;volume=17&amp;issue=1&amp;pages=250-256&amp;publication_year=2012&amp;author=Dorini%2CLB&amp;author=Minetto%2CR&amp;author=Leite%2CNJ" data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link">
                    Google Scholar</a>&nbsp;</p></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR21">Prinyakupt J, Pluempitiwiriyawej C. Segmentation of white blood cells and comparison of cell morphology by linear and naïve bayes classifiers. Biomed Eng Online. 2015;14(1):63.</p><p class="c-reading-companion__reference-links"><a href="https://link.springer.com/doi/10.1186/s12938-015-0037-1" data-track="click_references" data-track-action="article reference" data-track-value="article reference" data-track-label="10.1186/s12938-015-0037-1" data-track-item_id="10.1186/s12938-015-0037-1">Article</a>&nbsp;<a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=26123131" data-track="click_references" data-track-action="pubmed reference" data-track-value="pubmed reference" data-track-label="link" data-track-item_id="link">PubMed</a>&nbsp;<a href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4485641" data-track="click_references" data-track-action="pubmed central reference" data-track-value="pubmed central reference" data-track-label="link" data-track-item_id="link">PubMed Central</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=Segmentation%20of%20white%20blood%20cells%20and%20comparison%20of%20cell%20morphology%20by%20linear%20and%20na%C3%AFve%20bayes%20classifiers&amp;journal=Biomed%20Eng%20Online&amp;doi=10.1186%2Fs12938-015-0037-1&amp;volume=14&amp;issue=1&amp;publication_year=2015&amp;author=Prinyakupt%2CJ&amp;author=Pluempitiwiriyawej%2CC" data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link">
                    Google Scholar</a>&nbsp;</p></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR22">Shitong W, Min W. A new detection algorithm (NDA) based on fuzzy cellular neural networks for white blood cell detection. IEEE Trans Inf Technol Biomed Publ Inf. 2006;10(1):5–10.</p><p class="c-reading-companion__reference-links"><a href="https://doi.org/10.1109%2FTITB.2005.855545" data-track="click_references" data-track-action="article reference" data-track-value="article reference" data-track-label="10.1109/TITB.2005.855545" data-track-item_id="10.1109/TITB.2005.855545">Article</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=A%20new%20detection%20algorithm%20%28NDA%29%20based%20on%20fuzzy%20cellular%20neural%20networks%20for%20white%20blood%20cell%20detection&amp;journal=IEEE%20Trans%20Inf%20Technol%20Biomed%20Publ%20Inf&amp;doi=10.1109%2FTITB.2005.855545&amp;volume=10&amp;issue=1&amp;pages=5-10&amp;publication_year=2006&amp;author=Shitong%2CW&amp;author=Min%2CW" data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link">
                    Google Scholar</a>&nbsp;</p></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR23">Andrade AR, Vogado LH, de Veras MS, Silva RRV, Araujo FH, Medeiros FN. Recent computational methods for white blood cell nuclei segmentation: a comparative study. Comput Methods Prog Biomed. 2019;173:1–14.</p><p class="c-reading-companion__reference-links"><a href="https://doi.org/10.1016%2Fj.cmpb.2019.03.001" data-track="click_references" data-track-action="article reference" data-track-value="article reference" data-track-label="10.1016/j.cmpb.2019.03.001" data-track-item_id="10.1016/j.cmpb.2019.03.001">Article</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=Recent%20computational%20methods%20for%20white%20blood%20cell%20nuclei%20segmentation%3A%20a%20comparative%20study&amp;journal=Comput%20Methods%20Prog%20Biomed&amp;doi=10.1016%2Fj.cmpb.2019.03.001&amp;volume=173&amp;pages=1-14&amp;publication_year=2019&amp;author=Andrade%2CAR&amp;author=Vogado%2CLH&amp;author=Veras%2CMS&amp;author=Silva%2CRRV&amp;author=Araujo%2CFH&amp;author=Medeiros%2CFN" data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link">
                    Google Scholar</a>&nbsp;</p></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR24">Viswanathan P. Fuzzy c means detection of leukemia based on morphological contour segmentation. Procedia Comput Sci. 2015;58:84–90.</p><p class="c-reading-companion__reference-links"><a href="https://doi.org/10.1016%2Fj.procs.2015.08.017" data-track="click_references" data-track-action="article reference" data-track-value="article reference" data-track-label="10.1016/j.procs.2015.08.017" data-track-item_id="10.1016/j.procs.2015.08.017">Article</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=Fuzzy%20c%20means%20detection%20of%20leukemia%20based%20on%20morphological%20contour%20segmentation&amp;journal=Procedia%20Comput%20Sci&amp;doi=10.1016%2Fj.procs.2015.08.017&amp;volume=58&amp;pages=84-90&amp;publication_year=2015&amp;author=Viswanathan%2CP" data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link">
                    Google Scholar</a>&nbsp;</p></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR25">Gautam A, Bhadauria H. White blood nucleus extraction using k-mean clustering and mathematical morphing. In: 2014 5th International conference-confluence the next generation information technology summit (confluence). IEEE; 2014. p. 549–554.</p></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR26">Mohapatra S, Samanta SS, Patra D, Satpathi S. Fuzzy based blood image segmentation for automated leukemia detection. In: 2011 ICDeCom. IEEE; 2011. p. 1–5.</p></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR27">Nazlibilek S, Karacor D, Ercan T, Sazli MH, Kalender O, Ege Y. Automatic segmentation, counting, size determination and classification of white blood cells. Measurement. 2014;55:58–65.</p><p class="c-reading-companion__reference-links"><a href="https://doi.org/10.1016%2Fj.measurement.2014.04.008" data-track="click_references" data-track-action="article reference" data-track-value="article reference" data-track-label="10.1016/j.measurement.2014.04.008" data-track-item_id="10.1016/j.measurement.2014.04.008">Article</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=Automatic%20segmentation%2C%20counting%2C%20size%20determination%20and%20classification%20of%20white%20blood%20cells&amp;journal=Measurement&amp;doi=10.1016%2Fj.measurement.2014.04.008&amp;volume=55&amp;pages=58-65&amp;publication_year=2014&amp;author=Nazlibilek%2CS&amp;author=Karacor%2CD&amp;author=Ercan%2CT&amp;author=Sazli%2CMH&amp;author=Kalender%2CO&amp;author=Ege%2CY" data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link">
                    Google Scholar</a>&nbsp;</p></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR28">Abdeldaim AM, Sahlol AT, Elhoseny M, Hassanien AE. Computer-aided acute lymphoblastic leukemia diagnosis system based on image analysis. Berlin: Springer; 2018. p. 131–47.</p><p class="c-reading-companion__reference-links"><a href="http://scholar.google.com/scholar_lookup?&amp;title=Computer-aided%20acute%20lymphoblastic%20leukemia%20diagnosis%20system%20based%20on%20image%20analysis&amp;pages=131-147&amp;publication_year=2018&amp;author=Abdeldaim%2CAM&amp;author=Sahlol%2CAT&amp;author=Elhoseny%2CM&amp;author=Hassanien%2CAE" data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link">
                    Google Scholar</a>&nbsp;</p></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR29">Tosta TAA, De Abreu AF, Travençolo BAN, do Nascimento MZ, Neves LA. Unsupervised segmentation of leukocytes images using thresholding neighborhood valley-emphasis. In: 2015 IEEE CBMS. IEEE; 2015. p. 93–94.</p></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR30">Cao H, Liu H, Song E. A novel algorithm for segmentation of leukocytes in peripheral blood. Biomed Signal Process Control. 2018;45:10–21.</p><p class="c-reading-companion__reference-links"><a href="https://doi.org/10.1016%2Fj.bspc.2018.05.010" data-track="click_references" data-track-action="article reference" data-track-value="article reference" data-track-label="10.1016/j.bspc.2018.05.010" data-track-item_id="10.1016/j.bspc.2018.05.010">Article</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=A%20novel%20algorithm%20for%20segmentation%20of%20leukocytes%20in%20peripheral%20blood&amp;journal=Biomed%20Signal%20Process%20Control&amp;doi=10.1016%2Fj.bspc.2018.05.010&amp;volume=45&amp;pages=10-21&amp;publication_year=2018&amp;author=Cao%2CH&amp;author=Liu%2CH&amp;author=Song%2CE" data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link">
                    Google Scholar</a>&nbsp;</p></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR31">Mohammed EA, Mohamed MM, Naugler C, Far BH. Chronic lymphocytic leukemia cell segmentation from microscopic blood images using watershed algorithm and optimal thresholding. In: 2013 26th IEEE CCECE. IEEE; 2013. p. 1–5.</p></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR32">Hegde RB, Prasad K, Hebbar H, Singh BMK. Development of a robust algorithm for detection of nuclei and classification of white blood cells in peripheral blood smear images. J Med Syst. 2018;42(6):110.</p><p class="c-reading-companion__reference-links"><a href="https://link.springer.com/doi/10.1007/s10916-018-0962-1" data-track="click_references" data-track-action="article reference" data-track-value="article reference" data-track-label="10.1007/s10916-018-0962-1" data-track-item_id="10.1007/s10916-018-0962-1">Article</a>&nbsp;<a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=29721616" data-track="click_references" data-track-action="pubmed reference" data-track-value="pubmed reference" data-track-label="link" data-track-item_id="link">PubMed</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=Development%20of%20a%20robust%20algorithm%20for%20detection%20of%20nuclei%20and%20classification%20of%20white%20blood%20cells%20in%20peripheral%20blood%20smear%20images&amp;journal=J%20Med%20Syst&amp;doi=10.1007%2Fs10916-018-0962-1&amp;volume=42&amp;issue=6&amp;publication_year=2018&amp;author=Hegde%2CRB&amp;author=Prasad%2CK&amp;author=Hebbar%2CH&amp;author=Singh%2CBMK" data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link">
                    Google Scholar</a>&nbsp;</p></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR33">Chan Y-K, Tsai M-H, Huang D-C, Zheng Z-H, Hung K-D. Leukocyte nucleus segmentation and nucleus lobe counting. BMC Bioinform. 2010;11(1):558.</p><p class="c-reading-companion__reference-links"><a href="https://link.springer.com/doi/10.1186/1471-2105-11-558" data-track="click_references" data-track-action="article reference" data-track-value="article reference" data-track-label="10.1186/1471-2105-11-558" data-track-item_id="10.1186/1471-2105-11-558">Article</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=Leukocyte%20nucleus%20segmentation%20and%20nucleus%20lobe%20counting&amp;journal=BMC%20Bioinform&amp;doi=10.1186%2F1471-2105-11-558&amp;volume=11&amp;issue=1&amp;publication_year=2010&amp;author=Chan%2CY-K&amp;author=Tsai%2CM-H&amp;author=Huang%2CD-C&amp;author=Zheng%2CZ-H&amp;author=Hung%2CK-D" data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link">
                    Google Scholar</a>&nbsp;</p></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR34">Mathur A, Tripathi AS, Kuse M. Scalable system for classification of white blood cells from Leishman stained blood stain images. J Pathol Inform. 2013;4(Suppl):15.</p><p class="c-reading-companion__reference-links"><a href="https://doi.org/10.4103%2F2153-3539.109883" data-track="click_references" data-track-action="article reference" data-track-value="article reference" data-track-label="10.4103/2153-3539.109883" data-track-item_id="10.4103/2153-3539.109883">Article</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=Scalable%20system%20for%20classification%20of%20white%20blood%20cells%20from%20Leishman%20stained%20blood%20stain%20images&amp;journal=J%20Pathol%20Inform&amp;doi=10.4103%2F2153-3539.109883&amp;volume=4&amp;issue=Suppl&amp;publication_year=2013&amp;author=Mathur%2CA&amp;author=Tripathi%2CAS&amp;author=Kuse%2CM" data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link">
                    Google Scholar</a>&nbsp;</p></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR35">Su M-C, Cheng C-Y, Wang P-C. A neural-network-based approach to WBC classification. Sci World J. 2014;2014:1–9.</p><p class="c-reading-companion__reference-links"><a href="http://scholar.google.com/scholar_lookup?&amp;title=A%20neural-network-based%20approach%20to%20WBC%20classification&amp;journal=Sci%20World%20J&amp;volume=2014&amp;pages=1-9&amp;publication_year=2014&amp;author=Su%2CM-C&amp;author=Cheng%2CC-Y&amp;author=Wang%2CP-C" data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link">
                    Google Scholar</a>&nbsp;</p></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR36">Ramesh N, Dangott B, Salama ME, Tasdizen T. Isolation and two-step classification of normal white blood cells in peripheral blood smears. J Pathol Inform. 2012;3:179–91.</p><p class="c-reading-companion__reference-links"><a href="https://doi.org/10.4103%2F2153-3539.93895" data-track="click_references" data-track-action="article reference" data-track-value="article reference" data-track-label="10.4103/2153-3539.93895" data-track-item_id="10.4103/2153-3539.93895">Article</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=Isolation%20and%20two-step%20classification%20of%20normal%20white%20blood%20cells%20in%20peripheral%20blood%20smears&amp;journal=J%20Pathol%20Inform&amp;doi=10.4103%2F2153-3539.93895&amp;volume=3&amp;pages=179-191&amp;publication_year=2012&amp;author=Ramesh%2CN&amp;author=Dangott%2CB&amp;author=Salama%2CME&amp;author=Tasdizen%2CT" data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link">
                    Google Scholar</a>&nbsp;</p></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR37">Ghosh P, Bhattacharjee D, Nasipuri M. Blood smear analyzer for white blood cell counting: a hybrid microscopic image analyzing technique. Appl Soft Comput. 2016;46:629–38.</p><p class="c-reading-companion__reference-links"><a href="https://doi.org/10.1016%2Fj.asoc.2015.12.038" data-track="click_references" data-track-action="article reference" data-track-value="article reference" data-track-label="10.1016/j.asoc.2015.12.038" data-track-item_id="10.1016/j.asoc.2015.12.038">Article</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=Blood%20smear%20analyzer%20for%20white%20blood%20cell%20counting%3A%20a%20hybrid%20microscopic%20image%20analyzing%20technique&amp;journal=Appl%20Soft%20Comput&amp;doi=10.1016%2Fj.asoc.2015.12.038&amp;volume=46&amp;pages=629-638&amp;publication_year=2016&amp;author=Ghosh%2CP&amp;author=Bhattacharjee%2CD&amp;author=Nasipuri%2CM" data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link">
                    Google Scholar</a>&nbsp;</p></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR38">Habibzadeh M, Jannesari M, Rezaei Z, Baharvand H, Totonchi M. Automatic white blood cell classification using pre-trained deep learning models: Resnet and inception. In: Tenth ICMV 2017, vol 10696. International Society for Optics and Photonics; 2018. p. 1069612.</p></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR39">Rawat J, Singh A, Bhadauria H, Virmani J, Devgun JS. Application of ensemble artificial neural network for the classification of white blood cells using microscopic blood images. Int J Comput Syst Eng. 2018;4(2–3):202–16.</p><p class="c-reading-companion__reference-links"><a href="https://doi.org/10.1504%2FIJCSYSE.2018.091407" data-track="click_references" data-track-action="article reference" data-track-value="article reference" data-track-label="10.1504/IJCSYSE.2018.091407" data-track-item_id="10.1504/IJCSYSE.2018.091407">Article</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=Application%20of%20ensemble%20artificial%20neural%20network%20for%20the%20classification%20of%20white%20blood%20cells%20using%20microscopic%20blood%20images&amp;journal=Int%20J%20Comput%20Syst%20Eng&amp;doi=10.1504%2FIJCSYSE.2018.091407&amp;volume=4&amp;issue=2%E2%80%933&amp;pages=202-216&amp;publication_year=2018&amp;author=Rawat%2CJ&amp;author=Singh%2CA&amp;author=Bhadauria%2CH&amp;author=Virmani%2CJ&amp;author=Devgun%2CJS" data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link">
                    Google Scholar</a>&nbsp;</p></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR40">Patil AM, Patil MD, Birajdar GK. White blood cells image classification using deep learning with canonical correlation analysis. IRBM. 2021;42(5):378–89.</p><p class="c-reading-companion__reference-links"><a href="https://doi.org/10.1016%2Fj.irbm.2020.08.005" data-track="click_references" data-track-action="article reference" data-track-value="article reference" data-track-label="10.1016/j.irbm.2020.08.005" data-track-item_id="10.1016/j.irbm.2020.08.005">Article</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=White%20blood%20cells%20image%20classification%20using%20deep%20learning%20with%20canonical%20correlation%20analysis&amp;journal=IRBM&amp;doi=10.1016%2Fj.irbm.2020.08.005&amp;volume=42&amp;issue=5&amp;pages=378-389&amp;publication_year=2021&amp;author=Patil%2CAM&amp;author=Patil%2CMD&amp;author=Birajdar%2CGK" data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link">
                    Google Scholar</a>&nbsp;</p></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR41">Toğaçar M, Ergen B, Cömert Z. Classification of white blood cells using deep features obtained from convolutional neural network models based on the combination of feature selection methods. Appl Soft Comput. 2020;97:106810.</p><p class="c-reading-companion__reference-links"><a href="https://doi.org/10.1016%2Fj.asoc.2020.106810" data-track="click_references" data-track-action="article reference" data-track-value="article reference" data-track-label="10.1016/j.asoc.2020.106810" data-track-item_id="10.1016/j.asoc.2020.106810">Article</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=Classification%20of%20white%20blood%20cells%20using%20deep%20features%20obtained%20from%20convolutional%20neural%20network%20models%20based%20on%20the%20combination%20of%20feature%20selection%20methods&amp;journal=Appl%20Soft%20Comput&amp;doi=10.1016%2Fj.asoc.2020.106810&amp;volume=97&amp;publication_year=2020&amp;author=To%C4%9Fa%C3%A7ar%2CM&amp;author=Ergen%2CB&amp;author=C%C3%B6mert%2CZ" data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link">
                    Google Scholar</a>&nbsp;</p></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR42">Khan S, Sajjad M, Hussain T, Ullah A, Imran AS. A review on traditional machine learning and deep learning models for WBCs classification in blood smear images. IEEE Access. 2020;9:10657–73.</p><p class="c-reading-companion__reference-links"><a href="https://doi.org/10.1109%2FACCESS.2020.3048172" data-track="click_references" data-track-action="article reference" data-track-value="article reference" data-track-label="10.1109/ACCESS.2020.3048172" data-track-item_id="10.1109/ACCESS.2020.3048172">Article</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=A%20review%20on%20traditional%20machine%20learning%20and%20deep%20learning%20models%20for%20WBCs%20classification%20in%20blood%20smear%20images&amp;journal=IEEE%20Access&amp;doi=10.1109%2FACCESS.2020.3048172&amp;volume=9&amp;pages=10657-10673&amp;publication_year=2020&amp;author=Khan%2CS&amp;author=Sajjad%2CM&amp;author=Hussain%2CT&amp;author=Ullah%2CA&amp;author=Imran%2CAS" data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link">
                    Google Scholar</a>&nbsp;</p></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR43">Mohamed EH, El-Behaidy WH, Khoriba G, Li J. Improved white blood cells classification based on pre-trained deep learning models. J Commun Softw Syst. 2020;16(1):37–45.</p><p class="c-reading-companion__reference-links"><a href="https://doi.org/10.24138%2Fjcomss.v16i1.818" data-track="click_references" data-track-action="article reference" data-track-value="article reference" data-track-label="10.24138/jcomss.v16i1.818" data-track-item_id="10.24138/jcomss.v16i1.818">Article</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=Improved%20white%20blood%20cells%20classification%20based%20on%20pre-trained%20deep%20learning%20models&amp;journal=J%20Commun%20Softw%20Syst&amp;doi=10.24138%2Fjcomss.v16i1.818&amp;volume=16&amp;issue=1&amp;pages=37-45&amp;publication_year=2020&amp;author=Mohamed%2CEH&amp;author=El-Behaidy%2CWH&amp;author=Khoriba%2CG&amp;author=Li%2CJ" data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link">
                    Google Scholar</a>&nbsp;</p></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR44">Banik PP, Saha R, Kim KD. An automatic nucleus segmentation and CNN model based classification method of white blood cell. Expert Syst Appl. 2020;149:113211.</p><p class="c-reading-companion__reference-links"><a href="https://doi.org/10.1016%2Fj.eswa.2020.113211" data-track="click_references" data-track-action="article reference" data-track-value="article reference" data-track-label="10.1016/j.eswa.2020.113211" data-track-item_id="10.1016/j.eswa.2020.113211">Article</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=An%20automatic%20nucleus%20segmentation%20and%20CNN%20model%20based%20classification%20method%20of%20white%20blood%20cell&amp;journal=Expert%20Syst%20Appl&amp;doi=10.1016%2Fj.eswa.2020.113211&amp;volume=149&amp;publication_year=2020&amp;author=Banik%2CPP&amp;author=Saha%2CR&amp;author=Kim%2CKD" data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link">
                    Google Scholar</a>&nbsp;</p></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR45">Karthikeyan MP, Venkatesan R. Interpolative Leishman-stained transformation invariant deep pattern classification for white blood cells. Soft Comput. 2020;24(16):12215–25.</p><p class="c-reading-companion__reference-links"><a href="https://link.springer.com/doi/10.1007/s00500-019-04662-4" data-track="click_references" data-track-action="article reference" data-track-value="article reference" data-track-label="10.1007/s00500-019-04662-4" data-track-item_id="10.1007/s00500-019-04662-4">Article</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=Interpolative%20Leishman-stained%20transformation%20invariant%20deep%20pattern%20classification%20for%20white%20blood%20cells&amp;journal=Soft%20Comput&amp;doi=10.1007%2Fs00500-019-04662-4&amp;volume=24&amp;issue=16&amp;pages=12215-12225&amp;publication_year=2020&amp;author=Karthikeyan%2CMP&amp;author=Venkatesan%2CR" data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link">
                    Google Scholar</a>&nbsp;</p></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR46">Kutlu H, Avci E, Özyurt F. White blood cells detection and classification based on regional convolutional neural networks. Med Hypotheses. 2020;135:109472.</p><p class="c-reading-companion__reference-links"><a href="https://doi.org/10.1016%2Fj.mehy.2019.109472" data-track="click_references" data-track-action="article reference" data-track-value="article reference" data-track-label="10.1016/j.mehy.2019.109472" data-track-item_id="10.1016/j.mehy.2019.109472">Article</a>&nbsp;<a href="https://link.springer.com/articles/cas-redirect/1:CAS:528:DC%2BC1MXit1Smu7zE" data-track="click_references" data-track-action="cas reference" data-track-value="cas reference" data-track-label="link" data-track-item_id="link">CAS</a>&nbsp;<a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=31760248" data-track="click_references" data-track-action="pubmed reference" data-track-value="pubmed reference" data-track-label="link" data-track-item_id="link">PubMed</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=White%20blood%20cells%20detection%20and%20classification%20based%20on%20regional%20convolutional%20neural%20networks&amp;journal=Med%20Hypotheses&amp;doi=10.1016%2Fj.mehy.2019.109472&amp;volume=135&amp;publication_year=2020&amp;author=Kutlu%2CH&amp;author=Avci%2CE&amp;author=%C3%96zyurt%2CF" data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link">
                    Google Scholar</a>&nbsp;</p></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR47">Rakhlin A, Shvets A, Iglovikov V, Kalinin AA. Deep convolutional neural networks for breast cancer histology image analysis. In: ICIAR. Springer; 2018. p. 737–44.</p></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR48">Kermany DS, et al. Identifying medical diagnoses and treatable diseases by image-based deep learning. Cell. 2018;172(5):1122–31.</p><p class="c-reading-companion__reference-links"><a href="https://doi.org/10.1016%2Fj.cell.2018.02.010" data-track="click_references" data-track-action="article reference" data-track-value="article reference" data-track-label="10.1016/j.cell.2018.02.010" data-track-item_id="10.1016/j.cell.2018.02.010">Article</a>&nbsp;<a href="https://link.springer.com/articles/cas-redirect/1:CAS:528:DC%2BC1cXjt12ltr0%3D" data-track="click_references" data-track-action="cas reference" data-track-value="cas reference" data-track-label="link" data-track-item_id="link">CAS</a>&nbsp;<a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=29474911" data-track="click_references" data-track-action="pubmed reference" data-track-value="pubmed reference" data-track-label="link" data-track-item_id="link">PubMed</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=Identifying%20medical%20diagnoses%20and%20treatable%20diseases%20by%20image-based%20deep%20learning&amp;journal=Cell&amp;doi=10.1016%2Fj.cell.2018.02.010&amp;volume=172&amp;issue=5&amp;pages=1122-1131&amp;publication_year=2018&amp;author=Kermany%2CDS" data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link">
                    Google Scholar</a>&nbsp;</p></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR49">Acharya UR, et al. Automated identification of shockable and non-shockable life-threatening ventricular arrhythmias using convolutional neural network. Futur Gener Comput Syst. 2018;79:952–9.</p><p class="c-reading-companion__reference-links"><a href="https://doi.org/10.1016%2Fj.future.2017.08.039" data-track="click_references" data-track-action="article reference" data-track-value="article reference" data-track-label="10.1016/j.future.2017.08.039" data-track-item_id="10.1016/j.future.2017.08.039">Article</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=Automated%20identification%20of%20shockable%20and%20non-shockable%20life-threatening%20ventricular%20arrhythmias%20using%20convolutional%20neural%20network&amp;journal=Futur%20Gener%20Comput%20Syst&amp;doi=10.1016%2Fj.future.2017.08.039&amp;volume=79&amp;pages=952-959&amp;publication_year=2018&amp;author=Acharya%2CUR" data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link">
                    Google Scholar</a>&nbsp;</p></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR50">Moran MB, et al. Identification of thyroid nodules in infrared images by convolutional neural networks. In: 2018 IJCNN. IEEE; 2018. p. 1–7.</p></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR51">Sohail A, Khan A, Nisar H, Tabassum S, Zameer A. Mitotic nuclei analysis in breast cancer histopathology images using deep ensemble classifier. Med Image Anal. 2021;72:102121.</p><p class="c-reading-companion__reference-links"><a href="https://doi.org/10.1016%2Fj.media.2021.102121" data-track="click_references" data-track-action="article reference" data-track-value="article reference" data-track-label="10.1016/j.media.2021.102121" data-track-item_id="10.1016/j.media.2021.102121">Article</a>&nbsp;<a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=34139665" data-track="click_references" data-track-action="pubmed reference" data-track-value="pubmed reference" data-track-label="link" data-track-item_id="link">PubMed</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=Mitotic%20nuclei%20analysis%20in%20breast%20cancer%20histopathology%20images%20using%20deep%20ensemble%20classifier&amp;journal=Med%20Image%20Anal&amp;doi=10.1016%2Fj.media.2021.102121&amp;volume=72&amp;publication_year=2021&amp;author=Sohail%2CA&amp;author=Khan%2CA&amp;author=Nisar%2CH&amp;author=Tabassum%2CS&amp;author=Zameer%2CA" data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link">
                    Google Scholar</a>&nbsp;</p></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR52">Wachinger C, Reuter M, Klein T. DeepNAT: deep convolutional neural network for segmenting neuroanatomy. Neuroimage. 2018;170:434–45.</p><p class="c-reading-companion__reference-links"><a href="https://doi.org/10.1016%2Fj.neuroimage.2017.02.035" data-track="click_references" data-track-action="article reference" data-track-value="article reference" data-track-label="10.1016/j.neuroimage.2017.02.035" data-track-item_id="10.1016/j.neuroimage.2017.02.035">Article</a>&nbsp;<a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=28223187" data-track="click_references" data-track-action="pubmed reference" data-track-value="pubmed reference" data-track-label="link" data-track-item_id="link">PubMed</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=DeepNAT%3A%20deep%20convolutional%20neural%20network%20for%20segmenting%20neuroanatomy&amp;journal=Neuroimage&amp;doi=10.1016%2Fj.neuroimage.2017.02.035&amp;volume=170&amp;pages=434-445&amp;publication_year=2018&amp;author=Wachinger%2CC&amp;author=Reuter%2CM&amp;author=Klein%2CT" data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link">
                    Google Scholar</a>&nbsp;</p></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR53">Zhang B, Zhou J. Multi-feature representation for burn depth classification via burn images. Artif Intell Med. 2021;118:102128.</p><p class="c-reading-companion__reference-links"><a href="https://doi.org/10.1016%2Fj.artmed.2021.102128" data-track="click_references" data-track-action="article reference" data-track-value="article reference" data-track-label="10.1016/j.artmed.2021.102128" data-track-item_id="10.1016/j.artmed.2021.102128">Article</a>&nbsp;<a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=34412845" data-track="click_references" data-track-action="pubmed reference" data-track-value="pubmed reference" data-track-label="link" data-track-item_id="link">PubMed</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=Multi-feature%20representation%20for%20burn%20depth%20classification%20via%20burn%20images&amp;journal=Artif%20Intell%20Med&amp;doi=10.1016%2Fj.artmed.2021.102128&amp;volume=118&amp;publication_year=2021&amp;author=Zhang%2CB&amp;author=Zhou%2CJ" data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link">
                    Google Scholar</a>&nbsp;</p></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR54">Karimi D, Warfield SK, Gholipour A. Transfer learning in medical image segmentation: new insights from analysis of the dynamics of model parameters and learned representations. Artif Intell Med. 2021;116:102078.</p><p class="c-reading-companion__reference-links"><a href="https://doi.org/10.1016%2Fj.artmed.2021.102078" data-track="click_references" data-track-action="article reference" data-track-value="article reference" data-track-label="10.1016/j.artmed.2021.102078" data-track-item_id="10.1016/j.artmed.2021.102078">Article</a>&nbsp;<a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=34020754" data-track="click_references" data-track-action="pubmed reference" data-track-value="pubmed reference" data-track-label="link" data-track-item_id="link">PubMed</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=Transfer%20learning%20in%20medical%20image%20segmentation%3A%20new%20insights%20from%20analysis%20of%20the%20dynamics%20of%20model%20parameters%20and%20learned%20representations&amp;journal=Artif%20Intell%20Med&amp;doi=10.1016%2Fj.artmed.2021.102078&amp;volume=116&amp;publication_year=2021&amp;author=Karimi%2CD&amp;author=Warfield%2CSK&amp;author=Gholipour%2CA" data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link">
                    Google Scholar</a>&nbsp;</p></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR55">Nateghi R, Danyali H, Helfroush MS. A deep learning approach for mitosis detection: application in tumor proliferation prediction from whole slide images. Artif Intell Med. 2021;114:102048.</p><p class="c-reading-companion__reference-links"><a href="https://doi.org/10.1016%2Fj.artmed.2021.102048" data-track="click_references" data-track-action="article reference" data-track-value="article reference" data-track-label="10.1016/j.artmed.2021.102048" data-track-item_id="10.1016/j.artmed.2021.102048">Article</a>&nbsp;<a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=33875159" data-track="click_references" data-track-action="pubmed reference" data-track-value="pubmed reference" data-track-label="link" data-track-item_id="link">PubMed</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=A%20deep%20learning%20approach%20for%20mitosis%20detection%3A%20application%20in%20tumor%20proliferation%20prediction%20from%20whole%20slide%20images&amp;journal=Artif%20Intell%20Med&amp;doi=10.1016%2Fj.artmed.2021.102048&amp;volume=114&amp;publication_year=2021&amp;author=Nateghi%2CR&amp;author=Danyali%2CH&amp;author=Helfroush%2CMS" data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link">
                    Google Scholar</a>&nbsp;</p></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR56">Yoon H, Kim J, Lim HJ, Lee M-J. Image quality assessment of pediatric chest and abdomen ct by deep learning reconstruction. BMC Med Imaging. 2021;21:1–11.</p><p class="c-reading-companion__reference-links"><a href="https://link.springer.com/doi/10.1186/s12880-021-00677-2" data-track="click_references" data-track-action="article reference" data-track-value="article reference" data-track-label="10.1186/s12880-021-00677-2" data-track-item_id="10.1186/s12880-021-00677-2">Article</a>&nbsp;<a href="https://link.springer.com/articles/cas-redirect/1:CAS:528:DC%2BB3MXosVemuw%3D%3D" data-track="click_references" data-track-action="cas reference" data-track-value="cas reference" data-track-label="link" data-track-item_id="link">CAS</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=Image%20quality%20assessment%20of%20pediatric%20chest%20and%20abdomen%20ct%20by%20deep%20learning%20reconstruction&amp;journal=BMC%20Med%20Imaging&amp;doi=10.1186%2Fs12880-021-00677-2&amp;volume=21&amp;pages=1-11&amp;publication_year=2021&amp;author=Yoon%2CH&amp;author=Kim%2CJ&amp;author=Lim%2CHJ&amp;author=Lee%2CM-J" data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link">
                    Google Scholar</a>&nbsp;</p></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR57">Wang T, Song N, Liu L, Zhu Z, Chen B, Yang W, Chen Z. Efficiency of a deep learning-based artificial intelligence diagnostic system in spontaneous intracerebral hemorrhage volume measurement. BMC Med Imaging. 2021;21(1):1–9.</p><p class="c-reading-companion__reference-links"><a href="https://link.springer.com/doi/10.1186/s12880-021-00657-6" data-track="click_references" data-track-action="article reference" data-track-value="article reference" data-track-label="10.1186/s12880-021-00657-6" data-track-item_id="10.1186/s12880-021-00657-6">Article</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=Efficiency%20of%20a%20deep%20learning-based%20artificial%20intelligence%20diagnostic%20system%20in%20spontaneous%20intracerebral%20hemorrhage%20volume%20measurement&amp;journal=BMC%20Med%20Imaging&amp;doi=10.1186%2Fs12880-021-00657-6&amp;volume=21&amp;issue=1&amp;pages=1-9&amp;publication_year=2021&amp;author=Wang%2CT&amp;author=Song%2CN&amp;author=Liu%2CL&amp;author=Zhu%2CZ&amp;author=Chen%2CB&amp;author=Yang%2CW&amp;author=Chen%2CZ" data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link">
                    Google Scholar</a>&nbsp;</p></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR58">Guo K, Li X, Hu X, Liu J, Fan T. Hahn-PCNN-CNN: an end-to-end multi-modal brain medical image fusion framework useful for clinical diagnosis. BMC Med Imaging. 2021;21(1):1–22.</p><p class="c-reading-companion__reference-links"><a href="https://link.springer.com/doi/10.1186/s12880-021-00642-z" data-track="click_references" data-track-action="article reference" data-track-value="article reference" data-track-label="10.1186/s12880-021-00642-z" data-track-item_id="10.1186/s12880-021-00642-z">Article</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=Hahn-PCNN-CNN%3A%20an%20end-to-end%20multi-modal%20brain%20medical%20image%20fusion%20framework%20useful%20for%20clinical%20diagnosis&amp;journal=BMC%20Med%20Imaging&amp;doi=10.1186%2Fs12880-021-00642-z&amp;volume=21&amp;issue=1&amp;pages=1-22&amp;publication_year=2021&amp;author=Guo%2CK&amp;author=Li%2CX&amp;author=Hu%2CX&amp;author=Liu%2CJ&amp;author=Fan%2CT" data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link">
                    Google Scholar</a>&nbsp;</p></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR59">Sun J, Li H, Wang B, Li J, Li M, Zhou Z, Peng Y. Application of a deep learning image reconstruction (DLIR) algorithm in head CT imaging for children to improve image quality and lesion detection. BMC Med Imaging. 2021;21(1):1–9.</p><p class="c-reading-companion__reference-links"><a href="https://link.springer.com/doi/10.1186/s12880-021-00637-w" data-track="click_references" data-track-action="article reference" data-track-value="article reference" data-track-label="10.1186/s12880-021-00637-w" data-track-item_id="10.1186/s12880-021-00637-w">Article</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=Application%20of%20a%20deep%20learning%20image%20reconstruction%20%28DLIR%29%20algorithm%20in%20head%20CT%20imaging%20for%20children%20to%20improve%20image%20quality%20and%20lesion%20detection&amp;journal=BMC%20Med%20Imaging&amp;doi=10.1186%2Fs12880-021-00637-w&amp;volume=21&amp;issue=1&amp;pages=1-9&amp;publication_year=2021&amp;author=Sun%2CJ&amp;author=Li%2CH&amp;author=Wang%2CB&amp;author=Li%2CJ&amp;author=Li%2CM&amp;author=Zhou%2CZ&amp;author=Peng%2CY" data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link">
                    Google Scholar</a>&nbsp;</p></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR60">The Catholic University of Korea Institutional Review Board. <a href="https://bit.ly/2YrlQPl" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="https://bit.ly/2YrlQPl">https://bit.ly/2YrlQPl</a>. Accessed: 2019-07-17.</p></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR61">Sysmex DI-60. <a href="https://bit.ly/313v6L3" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="https://bit.ly/313v6L3">https://bit.ly/313v6L3</a>. Accessed: 2019-07-17.</p></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR62">Srivastava N, Hinton G, Krizhevsky A, Sutskever I, Salakhutdinov R. Dropout: a simple way to prevent neural networks from overfitting. J Mach Learn Res. 2014;15(1):1929–58.</p><p class="c-reading-companion__reference-links"><a href="http://scholar.google.com/scholar_lookup?&amp;title=Dropout%3A%20a%20simple%20way%20to%20prevent%20neural%20networks%20from%20overfitting&amp;journal=J%20Mach%20Learn%20Res&amp;volume=15&amp;issue=1&amp;pages=1929-1958&amp;publication_year=2014&amp;author=Srivastava%2CN&amp;author=Hinton%2CG&amp;author=Krizhevsky%2CA&amp;author=Sutskever%2CI&amp;author=Salakhutdinov%2CR" data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link">
                    Google Scholar</a>&nbsp;</p></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR63">Kingma DP, Ba J. Adam: a method for stochastic optimization. arXiv preprint <a href="http://arxiv.org/abs/1412.6980" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="http://arxiv.org/abs/1412.6980">arXiv:1412.6980</a> (2014).</p></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR64">Arlot S, Celisse A, et al. A survey of cross-validation procedures for model selection. Stat Surv. 2010;4:40–79.</p><p class="c-reading-companion__reference-links"><a href="https://doi.org/10.1214%2F09-SS054" data-track="click_references" data-track-action="article reference" data-track-value="article reference" data-track-label="10.1214/09-SS054" data-track-item_id="10.1214/09-SS054">Article</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=A%20survey%20of%20cross-validation%20procedures%20for%20model%20selection&amp;journal=Stat%20Surv&amp;doi=10.1214%2F09-SS054&amp;volume=4&amp;pages=40-79&amp;publication_year=2010&amp;author=Arlot%2CS&amp;author=Celisse%2CA" data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link">
                    Google Scholar</a>&nbsp;</p></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR65">Rosasco L, Vito ED, Caponnetto A, Piana M, Verri A. Are loss functions all the same? Neural Comput. 2004;16(5):1063–76.</p><p class="c-reading-companion__reference-links"><a href="https://doi.org/10.1162%2F089976604773135104" data-track="click_references" data-track-action="article reference" data-track-value="article reference" data-track-label="10.1162/089976604773135104" data-track-item_id="10.1162/089976604773135104">Article</a>&nbsp;<a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=15070510" data-track="click_references" data-track-action="pubmed reference" data-track-value="pubmed reference" data-track-label="link" data-track-item_id="link">PubMed</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=Are%20loss%20functions%20all%20the%20same%3F&amp;journal=Neural%20Comput&amp;doi=10.1162%2F089976604773135104&amp;volume=16&amp;issue=5&amp;pages=1063-1076&amp;publication_year=2004&amp;author=Rosasco%2CL&amp;author=Vito%2CED&amp;author=Caponnetto%2CA&amp;author=Piana%2CM&amp;author=Verri%2CA" data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link">
                    Google Scholar</a>&nbsp;</p></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR66">Sutskever I, Martens J, Dahl G, Hinton G. On the importance of initialization and momentum in deep learning. In: International conference on ML; 2013. p. 1139–47.</p></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR67">Jung, C. W-Net model and generated WBC images. 2022. <a href="https://bit.ly/2KAffwM" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="https://bit.ly/2KAffwM">https://bit.ly/2KAffwM</a>. Accessed: 2022-3-24.</p></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR68">Tiny ImageNet. <a href="https://bit.ly/36Qxvfp" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="https://bit.ly/36Qxvfp">https://bit.ly/36Qxvfp</a>. Accessed: 2020-01-13.</p></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR69">Gregor K, Danihelka I, Graves A, Rezende D, Wierstra D. Draw: a recurrent neural network for image generation. In: International conference on machine learning. PMLR (2015, June). p. 1462–71.</p></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR70">Radford A, Metz L, Chintala S. Unsupervised representation learning with deep convolutional generative adversarial networks. arXiv preprint <a href="http://arxiv.org/abs/1511.06434" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="http://arxiv.org/abs/1511.06434">arXiv:1511.06434</a> (2015).</p></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR71">Maas AL, Hannun AY, Ng AY. Rectifier nonlinearities improve neural network acoustic models. In: Proceedings of ICML, vol 30; 2013. p. 3.</p></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR72">Ioffe S, Szegedy C. Batch normalization: accelerating deep network training by reducing internal covariate shift. arXiv preprint <a href="http://arxiv.org/abs/1502.03167" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="http://arxiv.org/abs/1502.03167">arXiv:1502.03167</a> (2).</p></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR73">Kanbilim Dataset. <a href="http://kanbilim.com/" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="http://kanbilim.com/">http://kanbilim.com/</a>. Accessed: 2019-06-15.</p></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR74">Ghosh A, Singh S, Sheet D. Simultaneous localization and classification of acute lymphoblastic leukemic cells in peripheral blood smears using a deep convolutional network with average pooling layer. In: 2017 IEEE ICIIS. IEEE; 2017. p. 1–6.</p></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR75">WBC-classification. <a href="https://bit.ly/2zbz8oA" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="https://bit.ly/2zbz8oA">https://bit.ly/2zbz8oA</a>. Accessed: 2019-06-15.</p></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR76">Liang G, Hong H, Xie W, Zheng L. Combining convolutional neural network with recursive neural network for blood cell image classification. IEEE Access. 2018;6:36188–97.</p><p class="c-reading-companion__reference-links"><a href="https://doi.org/10.1109%2FACCESS.2018.2846685" data-track="click_references" data-track-action="article reference" data-track-value="article reference" data-track-label="10.1109/ACCESS.2018.2846685" data-track-item_id="10.1109/ACCESS.2018.2846685">Article</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=Combining%20convolutional%20neural%20network%20with%20recursive%20neural%20network%20for%20blood%20cell%20image%20classification&amp;journal=IEEE%20Access&amp;doi=10.1109%2FACCESS.2018.2846685&amp;volume=6&amp;pages=36188-36197&amp;publication_year=2018&amp;author=Liang%2CG&amp;author=Hong%2CH&amp;author=Xie%2CW&amp;author=Zheng%2CL" data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link">
                    Google Scholar</a>&nbsp;</p></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR77">BCCD dataset. <a href="https://bit.ly/2X5vQOl" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="https://bit.ly/2X5vQOl">https://bit.ly/2X5vQOl</a>. Accessed: 2019-06-15.</p></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR78">An efficient technique for white blood cells nuclei automatic segmentation. <a href="https://bit.ly/2XN064Z" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="https://bit.ly/2XN064Z">https://bit.ly/2XN064Z</a>. Accessed: 2019-06-15.</p></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR79">Putzu L, Caocci G, Di Ruberto C. Leucocyte classification for leukaemia detection using image processing techniques. Artif Intell Med. 2014;62(3):179–91.</p><p class="c-reading-companion__reference-links"><a href="https://doi.org/10.1016%2Fj.artmed.2014.09.002" data-track="click_references" data-track-action="article reference" data-track-value="article reference" data-track-label="10.1016/j.artmed.2014.09.002" data-track-item_id="10.1016/j.artmed.2014.09.002">Article</a>&nbsp;<a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=25241903" data-track="click_references" data-track-action="pubmed reference" data-track-value="pubmed reference" data-track-label="link" data-track-item_id="link">PubMed</a>&nbsp;<a href="http://scholar.google.com/scholar_lookup?&amp;title=Leucocyte%20classification%20for%20leukaemia%20detection%20using%20image%20processing%20techniques&amp;journal=Artif%20Intell%20Med&amp;doi=10.1016%2Fj.artmed.2014.09.002&amp;volume=62&amp;issue=3&amp;pages=179-191&amp;publication_year=2014&amp;author=Putzu%2CL&amp;author=Caocci%2CG&amp;author=Ruberto%2CC" data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link">
                    Google Scholar</a>&nbsp;</p></li><li class="c-reading-companion__reference-item"><p class="c-reading-companion__reference-citation u-font-family-serif" id="rc-ref-CR80">CellaVision. <a href="https://www.cellavision.com/" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="https://www.cellavision.com/">https://www.cellavision.com/</a>. Accessed: 2019-06-15.</p></li></ol></div></div>
                        </div>
                    </div>
                </aside>
            </div>
        </div>
    </article>
    <div class="app-elements">
    <nav aria-label="expander navigation">



    
        
    



</nav>
    <footer>
	<div class="eds-c-footer">
		
			
				<div class="eds-c-footer__container">
		<div class="eds-c-footer__grid eds-c-footer__group--separator">
			
			<div class="eds-c-footer__group">
				<h3 class="eds-c-footer__heading">Discover content</h3>
				<ul class="eds-c-footer__list">
					
						<li class="eds-c-footer__item"><a class="eds-c-footer__link" href="https://link.springer.com/journals/a/1" data-track="nav_journals_a_z" data-track-action="journals a-z" data-track-context="unified footer" data-track-label="link">Journals A-Z</a></li>
					
						<li class="eds-c-footer__item"><a class="eds-c-footer__link" href="https://link.springer.com/books/a/1" data-track="nav_books_a_z" data-track-action="books a-z" data-track-context="unified footer" data-track-label="link">Books A-Z</a></li>
					
				</ul>
			</div>
			
			<div class="eds-c-footer__group">
				<h3 class="eds-c-footer__heading">Publish with us</h3>
				<ul class="eds-c-footer__list">
					
						<li class="eds-c-footer__item"><a class="eds-c-footer__link" href="https://link.springer.com/journals" data-track="nav_journal_finder" data-track-action="journal finder" data-track-context="unified footer" data-track-label="link">Journal finder</a></li>
					
						<li class="eds-c-footer__item"><a class="eds-c-footer__link" href="https://www.springernature.com/gp/authors" data-track="nav_publish_your_research" data-track-action="publish your research" data-track-context="unified footer" data-track-label="link">Publish your research</a></li>
					
						<li class="eds-c-footer__item"><a class="eds-c-footer__link" href="https://www.springernature.com/gp/open-research/about/the-fundamentals-of-open-access-and-open-research" data-track="nav_open_access_publishing" data-track-action="open access publishing" data-track-context="unified footer" data-track-label="link">Open access publishing</a></li>
					
				</ul>
			</div>
			
			<div class="eds-c-footer__group">
				<h3 class="eds-c-footer__heading">Products and services</h3>
				<ul class="eds-c-footer__list">
					
						<li class="eds-c-footer__item"><a class="eds-c-footer__link" href="https://www.springernature.com/gp/products" data-track="nav_our_products" data-track-action="our products" data-track-context="unified footer" data-track-label="link">Our products</a></li>
					
						<li class="eds-c-footer__item"><a class="eds-c-footer__link" href="https://www.springernature.com/gp/librarians" data-track="nav_librarians" data-track-action="librarians" data-track-context="unified footer" data-track-label="link">Librarians</a></li>
					
						<li class="eds-c-footer__item"><a class="eds-c-footer__link" href="https://www.springernature.com/gp/societies" data-track="nav_societies" data-track-action="societies" data-track-context="unified footer" data-track-label="link">Societies</a></li>
					
						<li class="eds-c-footer__item"><a class="eds-c-footer__link" href="https://www.springernature.com/gp/partners" data-track="nav_partners_and_advertisers" data-track-action="partners and advertisers" data-track-context="unified footer" data-track-label="link">Partners and advertisers</a></li>
					
				</ul>
			</div>
			
			<div class="eds-c-footer__group">
				<h3 class="eds-c-footer__heading">Our brands</h3>
				<ul class="eds-c-footer__list">
					
						<li class="eds-c-footer__item"><a class="eds-c-footer__link" href="https://www.springer.com/" data-track="nav_imprint_Springer" data-track-action="Springer" data-track-context="unified footer" data-track-label="link">Springer</a></li>
					
						<li class="eds-c-footer__item"><a class="eds-c-footer__link" href="https://www.nature.com/" data-track="nav_imprint_Nature_Portfolio" data-track-action="Nature Portfolio" data-track-context="unified footer" data-track-label="link">Nature Portfolio</a></li>
					
						<li class="eds-c-footer__item"><a class="eds-c-footer__link" href="https://www.biomedcentral.com/" data-track="nav_imprint_BMC" data-track-action="BMC" data-track-context="unified footer" data-track-label="link">BMC</a></li>
					
						<li class="eds-c-footer__item"><a class="eds-c-footer__link" href="https://www.palgrave.com/" data-track="nav_imprint_Palgrave_Macmillan" data-track-action="Palgrave Macmillan" data-track-context="unified footer" data-track-label="link">Palgrave Macmillan</a></li>
					
						<li class="eds-c-footer__item"><a class="eds-c-footer__link" href="https://www.apress.com/" data-track="nav_imprint_Apress" data-track-action="Apress" data-track-context="unified footer" data-track-label="link">Apress</a></li>
					
						<li class="eds-c-footer__item"><a class="eds-c-footer__link" href="https://link.springer.com/brands/discover" data-track="nav_imprint_Discover" data-track-action="Discover" data-track-context="unified footer" data-track-label="link">Discover</a></li>
					
				</ul>
			</div>
			
		</div>
	</div>

		
		
		<div class="eds-c-footer__container">
	
		<nav aria-label="footer navigation">
			<ul class="eds-c-footer__links">
				
					<li class="eds-c-footer__item">
						
						
							<button class="eds-c-footer__link" data-cc-action="preferences" data-track="dialog_manage_cookies" data-track-action="Manage cookies" data-track-context="unified footer" data-track-label="link"><span class="eds-c-footer__button-text">Your privacy choices/Manage cookies</span></button>
						
					</li>
				
					<li class="eds-c-footer__item">
						
							<a class="eds-c-footer__link" href="https://www.springernature.com/gp/legal/ccpa" data-track="nav_california_privacy_statement" data-track-action="california privacy statement" data-track-context="unified footer" data-track-label="link">Your US state privacy rights</a>
						
						
					</li>
				
					<li class="eds-c-footer__item">
						
							<a class="eds-c-footer__link" href="https://www.springernature.com/gp/info/accessibility" data-track="nav_accessibility_statement" data-track-action="accessibility statement" data-track-context="unified footer" data-track-label="link">Accessibility statement</a>
						
						
					</li>
				
					<li class="eds-c-footer__item">
						
							<a class="eds-c-footer__link" href="https://link.springer.com/termsandconditions" data-track="nav_terms_and_conditions" data-track-action="terms and conditions" data-track-context="unified footer" data-track-label="link">Terms and conditions</a>
						
						
					</li>
				
					<li class="eds-c-footer__item">
						
							<a class="eds-c-footer__link" href="https://link.springer.com/privacystatement" data-track="nav_privacy_policy" data-track-action="privacy policy" data-track-context="unified footer" data-track-label="link">Privacy policy</a>
						
						
					</li>
				
					<li class="eds-c-footer__item">
						
							<a class="eds-c-footer__link" href="https://support.springernature.com/en/support/home" data-track="nav_help_and_support" data-track-action="help and support" data-track-context="unified footer" data-track-label="link">Help and support</a>
						
						
					</li>
				
					<li class="eds-c-footer__item">
						
							<a class="eds-c-footer__link" href="https://link.springer.com/legal-notice" data-track="nav_legal_notice" data-track-action="legal notice" data-track-context="unified footer" data-track-label="link">Legal notice</a>
						
						
					</li>
				
					<li class="eds-c-footer__item">
						
							<a class="eds-c-footer__link" href="https://support.springernature.com/en/support/solutions/articles/6000255911-subscription-cancellations" data-track-action="cancel contracts here">Cancel contracts here</a>
						
						
					</li>
				
			</ul>
		</nav>
	
	
		
			<div class="eds-c-footer__user">
				<p class="eds-c-footer__user-info">
					
					<span data-test="footer-user-ip">106.51.90.99</span>
				</p>
				<p class="eds-c-footer__user-info" data-test="footer-business-partners">Not affiliated</p>
			</div>
		
	
	
		<a href="https://www.springernature.com/" class="eds-c-footer__link">
			<img src="/oscar-static/images/logo-springernature-white-19dd4ba190.svg" alt="Springer Nature" loading="lazy" width="200" height="20">
		</a>
	
	<p class="eds-c-footer__legal" data-test="copyright">© 2025 Springer Nature</p>
</div>

	</div>
</footer>
</div>


    



<script src="/oscar-static/js/global-article-es5-bundle-237659debf.js" nomodule="true"></script><script src="/oscar-static/js/global-article-es6-bundle-2c19ea9e42.js" type="module"></script><div data-cc-ghost="" style="height: 301px;"></div><script type="text/javascript" id="fetch-contextual-ads-campaign-data" charset="">(function(){var g=function(a){a=new CustomEvent("campaignDataLoaded",{detail:a});document.dispatchEvent(a)},b=google_tag_manager["rm"]["50443292"](55);if(b){var k=1500;b=google_tag_manager["rm"]["50443292"](56);b="link"===b?google_tag_manager["rm"]["50443292"](57):google_tag_manager["rm"]["50443292"](58);var m=function(a,c){var h=!1,l=setTimeout(function(){h=!0;c(null)},k),d=new XMLHttpRequest;d.onload=function(){var e=null,f=[];if(200===d.status){try{e=JSON.parse(d.responseText)}catch(p){e={}}f=e.campaigns||[]}h||(clearTimeout(l),c(f.length?f.join(","):null))};d.open("GET",
"/platform/contextual?doi\x3d"+a);d.send()},n=function(a,c){a&&-1===a.indexOf("not set")?m(a,c):setTimeout(c,1)};n(b,function(a){window.campaignsForContextualAds=a;g(a);window.dataLayer.push({content:{article:{campaignID:a}}})})}else window.campaignsForContextualAds="",g(null)})();</script>
<script type="text/javascript" id="gtm-setup-accessdetailsloaded-handler" charset="">document.addEventListener("accessdetailsloaded",function(a){a=a.detail||{};var b={event:"update-access-details"};a.institutional_business_partner_ids&&a.resolved_by||console.log("BPID data could not be retrieved from /exposed-details");a.logged_in_to_sn_profile||console.log("logged_in_to_sn_profile could not be retrieved from /exposed-details");var c=a.institutional_business_partner_ids&&a.institutional_business_partner_ids.join?a.institutional_business_partner_ids.join(";"):"",d=a.resolved_by&&a.resolved_by.join?
a.resolved_by.join(";"):"",e=a.personal_business_partner_id_found,f=a.logged_in_to_sn_profile,g=a.snid;b.user={};b.user.profile={};b.user.profile.profileInfo={resolvedBy:d||null,bpid:c||null,personal:e||null,logged_in_to_sn_profile:f,snid:g||null};b.session={};b.session.authentication={};b.session.authentication.token=a.token||null;b.session.authentication.legacy={};window.dataLayer.push(b);window.idpUserDataLoaded=!0},!1);window.dataLayer.push({event:"accessdetailsloaded-handler-added"});
window.accessDetailsLoadedHandlerAdded=!0;</script><script id="crossmark-script" text="" charset="" type="text/javascript" src="https://crossmark-cdn.crossref.org/widget/v2.0/widget.js"></script><script type="text/javascript" id="polyfill-matches" charset="">Element.prototype.matches||(Element.prototype.matches=Element.prototype.matchesSelector||Element.prototype.mozMatchesSelector||Element.prototype.msMatchesSelector||Element.prototype.oMatchesSelector||Element.prototype.webkitMatchesSelector||function(a){a=(this.document||this.ownerDocument).querySelectorAll(a);for(var b=a.length;0<=--b&&a.item(b)!==this;);return-1<b});</script><script type="text/javascript" id="fetch-idp-user-data" charset="">(function(a){if("function"===typeof window.CustomEvent)return!1;var c=function(d,b){b=b||{};var e=document.createEvent("CustomEvent");e.initCustomEvent(d,b.bubbles||!1,b.cancelable||!1,b.detail||a);return e};c.prototype=window.Event.prototype;window.CustomEvent=c})();var parse=function(a,c){try{return 200===a?JSON.parse(c):null}catch(d){return null}},dispatch=function(a){a=new CustomEvent("accessdetailsloaded",{detail:a});document.dispatchEvent(a)},site=google_tag_manager["rm"]["50443292"](80),idpUrl;
switch(site){case "nature":idpUrl=-1<window.location.hostname.indexOf("local-www")||-1<window.location.hostname.indexOf("test-www")?"https://staging-idp.nature.com/exposed-details":"https://idp.nature.com/exposed-details";break;case "link":idpUrl=-1<window.location.hostname.indexOf("link-qa")?"https://staging-idp.springer.com/exposed-details":"https://idp.springer.com/exposed-details";break;case "springer":idpUrl=-1<window.location.hostname.indexOf("local-www")||-1<window.location.hostname.indexOf("test-www")?
"https://staging-idp.springer.com/exposed-details":"https://idp.springer.com/exposed-details"}if(void 0!==idpUrl){var transport=new XMLHttpRequest;transport.open("GET",idpUrl,!0);transport.withCredentials=!0;transport.onreadystatechange=function(){4===transport.readyState&&dispatch(parse(transport.status,transport.responseText))};transport.send()}else dispatch(null);</script><script type="text/javascript" id="create-ad-slots" charset="">function createAndLoadAds(){function h(b){"nature"===a&&google_tag_manager["rm"]["50443292"](108)(b);"bmc"!==a&&"springeropen"!==a||google_tag_manager["rm"]["50443292"](120)(b);("link"===a&&"oscar"===google_tag_manager["rm"]["50443292"](121)||"springer"===a||"link"===a&&!0===google_tag_manager["rm"]["50443292"](122))&&google_tag_manager["rm"]["50443292"](147)(b);"link"===a&&"bunsen"===google_tag_manager["rm"]["50443292"](148)&&google_tag_manager["rm"]["50443292"](173)(b);"link"===a&&"Core"===google_tag_manager["rm"]["50443292"](174)&&google_tag_manager["rm"]["50443292"](186)(b)}window.googletag=window.googletag||{cmd:[]};var a=google_tag_manager["rm"]["50443292"](187),e=google_tag_manager["rm"]["50443292"](189);
if(-1===window.location.search.indexOf("hide_ads\x3dtrue"))if(window.adSlots&&0!==Object.keys(window.adSlots).length)console.log("Ads previously loaded. Will not update ad slots until next page load.");else{window.adSlots||(window.adSlots={});window.getAd=function(b,f){for(var c in window.adSlots)if(-1<c.indexOf(b)){if("object"===f)return adSlots[c];if("slot"===f)return adSlots[c].slot}};googletag.cmd.push(function(){googletag.pubads().setPrivacySettings({limitedAds:e});console.log("limitedAds is ",
e);googletag.pubads().setRequestNonPersonalizedAds(google_tag_manager["rm"]["50443292"](191));googletag.pubads().enableSingleRequest();googletag.pubads().disableInitialLoad()});for(var g=document.querySelectorAll("[data-gpt]"),d=0;g[d];++d)h(g[d]);googletag.cmd.push(function(){googletag.enableServices()})}}createAndLoadAds();</script>
</body></html>